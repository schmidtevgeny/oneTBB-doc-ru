# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, Intel Corporation
# This file is distributed under the same license as the oneTBB package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: oneTBB \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-23 13:51+0300\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../oneTBB/doc/main/intro/Benefits.rst:4
msgid "|short_name| Benefits"
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:7
msgid ""
"|full_name| is a library that helps you leverage multi-core performance "
"without having to be a threading expert. Typically you can improve "
"performance for multi-core processors by implementing the key points "
"explained in the early sections of the Developer Guide. As your expertise"
" grows, you may want to dive into more complex subjects that are covered "
"in advanced sections."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:15
msgid ""
"There are a variety of approaches to parallel programming, ranging from "
"using platform-dependent threading primitives to exotic new languages. "
"The advantage of oneTBB is that it works at a higher level than raw "
"threads, yet does not require exotic languages or compilers. You can use "
"it with any compiler supporting ISO C++. The library differs from typical"
" threading packages in the following ways:"
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:23
msgid ""
"**oneTBB enables you to specify logical paralleism instead of threads**. "
"Most threading packages require you to specify threads. Programming "
"directly in terms of threads can be tedious and lead to inefficient "
"programs, because threads are low-level, heavy constructs that are close "
"to the hardware. Direct programming with threads forces you to "
"efficiently map logical tasks onto threads. In contrast, the oneTBB run-"
"time library automatically maps logical parallelism onto threads in a way"
" that makes efficient use of processor resources."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:34
msgid ""
"**oneTBB targets threading for performance**. Most general-purpose "
"threading packages support many different kinds of threading, such as "
"threading for asynchronous events in graphical user interfaces. As a "
"result, general-purpose packages tend to be low-level tools that provide "
"a foundation, not a solution. Instead, oneTBB focuses on the particular "
"goal of parallelizing computationally intensive work, delivering higher-"
"level, simpler solutions."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:43
msgid ""
"**oneTBB is compatible with other threading packages.** Because the "
"library is not designed to address all threading problems, it can coexist"
" seamlessly with other threading packages."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:48
msgid ""
"**oneTBB emphasizes scalable, data parallel programming**. Breaking a "
"program up into separate functional blocks, and assigning a separate "
"thread to each block is a solution that typically does not scale well "
"since typically the number of functional blocks is fixed. In contrast, "
"oneTBB emphasizes *data-parallel* programming, enabling multiple threads "
"to work on different parts of a collection. Data-parallel programming "
"scales well to larger numbers of processors by dividing the collection "
"into smaller pieces. With data-parallel programming, program performance "
"increases as you add processors."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:59
msgid ""
"**oneTBB relies on generic programming**. Traditional libraries specify "
"interfaces in terms of specific types or base classes. Instead, oneAPI "
"Threading Building Blocks uses generic programming. The essence of "
"generic programming is writing the best possible algorithms with the "
"fewest constraints. The C++ Standard Template Library (STL) is a good "
"example of generic programming in which the interfaces are specified by "
"*requirements* on types. For example, C++ STL has a template function "
"``sort`` that sorts a sequence abstractly defined in terms of iterators "
"on the sequence. The requirements on the iterators are:"
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:71
msgid "Provide random access"
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:74
msgid ""
"The expression ``*i<*j`` is true if the item pointed to by iterator ``i``"
" should precede the item pointed to by iterator ``j``, and false "
"otherwise."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:79
msgid "The expression ``swap(*i,*j)`` swaps two elements."
msgstr ""

#: ../../oneTBB/doc/main/intro/Benefits.rst:82
msgid ""
"Specification in terms of requirements on types enables the template to "
"sort many different representations of sequences, such as vectors and "
"deques. Similarly, the oneTBB templates specify requirements on types, "
"not particular types, and thus adapt to different data representations. "
"Generic programming enables oneTBB to deliver high performance algorithms"
" with broad applicability."
msgstr ""

#: ../../oneTBB/doc/main/intro/help_support.rst:4
msgid "Getting Help and Support"
msgstr ""

#: ../../oneTBB/doc/main/intro/help_support.rst
msgid "Getting Technical Support"
msgstr ""

#: ../../oneTBB/doc/main/intro/help_support.rst:13
msgid ""
"For general information about oneTBB technical support, product updates, "
"user forums, FAQs, tips and tricks and other support questions, go to "
"`GitHub issues <https://github.com/oneapi-src/oneTBB/issues>`_."
msgstr ""

#: ../../oneTBB/doc/main/intro/intro_os.rst:4
#: ../../oneTBB/doc/main/intro/introducing_main_os.rst:4
msgid "Introduction"
msgstr ""

#: ../../oneTBB/doc/main/intro/intro_os.rst:7
#: ../../oneTBB/doc/main/intro/introducing_main_os.rst:7
msgid ""
"|full_name| is a library that supports scalable parallel programming "
"using standard ISO C++ code. It does not require special languages or "
"compilers. It is designed to promote scalable data parallel programming. "
"Additionally, it fully supports nested parallelism, so you can build "
"larger parallel components from smaller parallel components. To use the "
"library, you specify tasks, not threads, and let the library map tasks "
"onto threads in an efficient manner."
msgstr ""

#: ../../oneTBB/doc/main/intro/intro_os.rst:16
#: ../../oneTBB/doc/main/intro/introducing_main_os.rst:16
msgid ""
"Many of the library interfaces employ generic programming, in which "
"interfaces are defined by requirements on types and not specific types. "
"The C++ Standard Template Library (STL) is an example of generic "
"programming. Generic programming enables oneTBB to be flexible yet "
"efficient. The generic interfaces enable you to customize components to "
"your specific needs."
msgstr ""

#: ../../oneTBB/doc/main/intro/intro_os.rst:25
#: ../../oneTBB/doc/main/intro/introducing_main_os.rst:25
msgid "|full_name| requires C++11 standard compiler support."
msgstr ""

#: ../../oneTBB/doc/main/intro/intro_os.rst:28
#: ../../oneTBB/doc/main/intro/introducing_main_os.rst:28
msgid ""
"The net result is that oneTBB enables you to specify parallelism far more"
" conveniently than using raw threads, and at the same time can improve "
"performance."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:4
msgid "Notational Conventions"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:7
msgid "The following conventions may be used in this document."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:16
msgid "Convention"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:17
msgid "Explanation"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:18
#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:66
#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:35
#: ../../oneTBB/doc/main/reference/make_edges_function.rst:49
#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:55
#: ../../oneTBB/doc/main/tbb_userguide/Constraints.rst:13
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst
msgid "Example"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:19
msgid "\\ *Italic*"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:20
msgid ""
"Used for introducing new terms, denotation of terms,    placeholders, or "
"titles of manuals."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:21
msgid "The filename consists of the *basename* and the *extension*."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:22
msgid "\\ ``Monospace``"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:23
msgid ""
"Indicates directory paths and filenames, commands and    command line"
"        options, function names, methods,   classes, data structures in "
"body text, source code."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:24
msgid ""
"\\ ``ippsapi.h``        \\ ``\\alt\\include``           Use the "
"okCreateObjs() function to...          \\ ``printf(\"hello, "
"world\\n\");``"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:25
msgid "[ ]"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:26
msgid "Items enclosed in brackets are optional."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:27
msgid "Fa[c]        Indicates Fa or Fac."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:28
msgid "{ \\| }"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:29
msgid ""
"Braces and vertical bars indicate the choice of one item    from a "
"selection of two or more items."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:30
msgid "X{K \\| W \\| P}        Indicates XK, XW, or XP."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:31
msgid "\"[\" \"]\" \"{\"    | \" }\" \"|\""
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:32
msgid ""
"Writing a metacharacter in quotation marks negates the      syntactical "
"meaning stated above;   | the character is taken as a literal."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:33
msgid ""
"\"[\" X \"]\" [ Y ]        Denotes the letter X    enclosed in brackets, "
"optionally followed by the letter Y."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:34
msgid "..."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:35
msgid ""
"The ellipsis indicates that the previous item can be    repeated several "
"times."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:36
msgid ""
"\\ ``filename`` ...        Indicates that one or    more filenames can be"
" specified."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:37
msgid ",..."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:38
msgid ""
"The ellipsis preceded by a comma indicates that the      previous item "
"can be repeated several times,   | separated by commas."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:39
msgid ""
"\\ ``word`` ,...        Indicates that one or    more words can be "
"specified. If more than one word is specified, the   words are comma-"
"separated."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:47
msgid ""
"Class members are summarized by informal class declarations that describe"
" the class as it seems to clients, not how it is actually implemented. "
"For example, here is an informal declaration of class ``Foo``:"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:64
msgid "The actual implementation might look like:"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:90
msgid ""
"The example shows two cases where the actual implementation departs from "
"the informal declaration:"
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:94
msgid "``Foo`` is actually a typedef to ``Foo_v3``."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:97
msgid "Method ``x()`` is inherited from a protected base class."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:100
msgid "The destructor is an implicit method generated by the compiler."
msgstr ""

#: ../../oneTBB/doc/main/intro/notation.rst:103
msgid ""
"The informal declarations are intended to show you what you need to know "
"to use the class without the distraction of irrelevant clutter particular"
" to the implementation."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:4
msgid "concurrent_lru_cache"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:7
msgid ""
"To enable this feature, define the ``TBB_PREVIEW_CONCURRENT_LRU_CACHE`` "
"macro to 1."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:9
msgid "A Class Template for Least Recently Used cache with concurrent operations."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:16
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:14
#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:14
#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:14
#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:18
#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:16
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:14
#: ../../oneTBB/doc/main/reference/make_edges_function.rst:14
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:14
#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:11
#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:11
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:16
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:16
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:16
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:14
#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:14
#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:19
#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:37
#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:20
#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:23
msgid "Description"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:18
msgid ""
"A ``concurrent_lru_cache`` container maps keys to values with the ability"
" to limit the number of stored unused values. For each key, there is at "
"most one item stored in the container."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:22
msgid ""
"The container permits multiple threads to concurrently retrieve items "
"from it."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:24
msgid ""
"The container tracks which items are in use by returning a proxy "
"``concurrent_lru_cache::handle`` object that refers to an item instead of"
" its value. Once there are no ``handle`` objects holding reference to an "
"item, it is considered unused."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:28
msgid ""
"The container stores all the items that are currently in use plus a "
"limited number of unused items. Excessive unused items are erased "
"according to least recently used policy."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:32
msgid ""
"When no item is found for a given key, the container calls the user-"
"specified ``value_function_type`` object to construct a value for the "
"key, and stores that value. The ``value_function_type`` object must be "
"thread-safe."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:37
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:22
#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:22
#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:21
#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:35
#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:24
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:23
#: ../../oneTBB/doc/main/reference/make_edges_function.rst:26
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:20
#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:18
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:23
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:24
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:29
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:22
#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:21
msgid "API"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:40
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:25
#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:25
#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:24
#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:38
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:26
#: ../../oneTBB/doc/main/reference/make_edges_function.rst:29
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:23
#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:21
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:26
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:27
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:32
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:25
#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:24
msgid "Header"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:47
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:32
#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:31
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:33
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:34
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:39
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:32
msgid "Synopsis"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:87
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:58
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:55
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:93
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:61
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:61
msgid "Member Functions"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:91
msgid ""
"**Effects**: Constructs an empty cache that can keep up to "
"``number_of_lru_history_items`` unused values, with a function object "
"``f`` for constructing new values."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:98
msgid ""
"**Effects**: Destroys the ``concurrent_lru_cache``. Calls the destructors"
" of the stored elements and deallocates the used storage."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:101
msgid "The behavior is undefined in case of concurrent operations with ``*this``."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:107
msgid ""
"**Effects**: Searches the container for an item that corresponds to the "
"given key. If such an item is not found, the user-specified function "
"object is called to construct a value that is inserted into the "
"container."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:111
msgid "**Returns**: a ``handle`` object holding reference to the matching value."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:114
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:85
msgid "Member Objects"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:117
msgid "``handle`` class"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:119
msgid "**Member Functions**"
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:123
msgid ""
"**Effects**: Constructs a ``handle`` object that does not refer to any "
"value."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:129
msgid ""
"**Effects**: Transfers the reference to the value stored in "
"``concurrent_lru_cache`` from ``other`` to the newly constructed object. "
"Upon completion, ``other`` no longer refers to any value."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:137
msgid ""
"**Effects**: Releases the reference (if it exists) to a value stored in "
"``concurrent_lru_cache``."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:139
msgid "The behavior is undefined for concurrent operations with ``*this``."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:145
msgid ""
"**Effects**: Transfers the reference to a value stored in "
"``concurrent_lru_cache`` from ``other`` to ``*this``. If existed, the "
"previous reference held by ``*this`` is released. Upon completion "
"``other`` no longer refers to any value."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:149
msgid "**Returns**: a reference to ``*this``."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:155
msgid ""
"**Returns**: ``true`` if ``*this`` holds reference to a value, ``false`` "
"otherwise."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:161
msgid ""
"**Returns**: a reference to a ``value_type`` object stored in "
"``concurrent_lru_cache``."
msgstr ""

#: ../../oneTBB/doc/main/reference/concurrent_lru_cache_cls.rst:163
msgid "The behavior is undefined if ``*this`` does not refer to any value."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:4
msgid "task_arena::constraints extensions"
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:7
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:7
msgid ""
"To enable this feature, set the "
"``TBB_PREVIEW_TASK_ARENA_CONSTRAINTS_EXTENSION`` macro to 1."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:16
msgid ""
"These extensions allow to customize ``tbb::task_arena::constraints`` with"
" the following properties:"
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:18
msgid ""
"On machines with IntelÂ® Hybrid Technology set the preferred core type for"
" threads working within the task arena."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:19
msgid ""
"Limit the maximum number of threads that can be scheduled to one core "
"simultaneously."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:62
msgid "Sets the ``numa_id`` field to the ``id``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:64
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:70
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:76
#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:82
msgid "**Returns:** Reference to ``*this``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:68
msgid "Sets the ``max_concurrency`` field to the ``maximal_concurrency``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:74
msgid "Sets the ``core_type`` field to the ``id``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:80
msgid "Sets the ``max_threads_per_core`` field to the ``threads_number``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:89
msgid ""
"An integral logical index uniquely identifying a NUMA node. All threads "
"joining the ``task_arena`` are bound to this NUMA node."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:94
msgid "To obtain a valid NUMA node ID, call ``oneapi::tbb::info::numa_nodes()``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:98
msgid ""
"The maximum number of threads that can participate in work processing "
"within the ``task_arena`` at the same time."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:104
msgid ""
"An integral logical index uniquely identifying a core type. All threads "
"joining the ``task_arena`` are bound to this core type."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:109
msgid ""
"To obtain a valid core type node ID, call "
"``oneapi::tbb::info::core_types()``."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:113
msgid ""
"The maximum number of threads that can be scheduled to one core "
"simultaneously."
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:115
#: ../../oneTBB/doc/main/reference/info_namespace.rst:12
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:68
msgid "See also:"
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:117
#: ../../oneTBB/doc/main/reference/info_namespace.rst:16
msgid ""
":doc:`oneapi::tbb::info namespace preview extensions "
"<info_namespace_extensions>`"
msgstr ""

#: ../../oneTBB/doc/main/reference/constraints_extensions.rst:118
#: ../../oneTBB/doc/main/reference/info_namespace.rst:15
msgid ""
"`oneapi::tbb::task_arena specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/task_arena/task_arena_cls.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:4
msgid "Constructors for Flow Graph nodes"
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:7
#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:7
#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:7
#: ../../oneTBB/doc/main/reference/make_edges_function.rst:7
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:7
#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:7
msgid ""
"To enable this feature, define the ``TBB_PREVIEW_FLOW_GRAPH_FEATURES`` "
"macro to 1."
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:16
msgid ""
"The \"Helper Functions for Expressing Graphs\" feature adds a set of new "
"constructors that can be used to construct a node that ``follows`` or "
"``precedes`` a set of nodes."
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:19
msgid ""
"Where possible, the constructors support Class Template Argument "
"Deduction (since C++17)."
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:32
#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:45
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:33
#: ../../oneTBB/doc/main/reference/make_edges_function.rst:36
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:30
#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:28
#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:31
msgid "Syntax"
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:103
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:38
#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:67
msgid "See Also"
msgstr ""

#: ../../oneTBB/doc/main/reference/constructors_for_nodes.rst:104
#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:42
msgid ":ref:`follows_precedes`"
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:4
msgid "The customizing mutex type for ``concurrent_hash_map``"
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:7
msgid ""
"To enable this feature, define the "
"``TBB_PREVIEW_CONCURRENT_HASH_MAP_EXTENSIONS`` macro to 1."
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:16
msgid ""
"oneTBB ``concurrnent_hash_map`` class uses reader-writer mutex to provide"
" thread safety and avoid data races for insert, lookup, and erasure "
"operations. This feature adds an extra template parameter for "
"``concurrent_hash_map`` that allows to customize the type of the reader-"
"writer mutex."
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:50
msgid "Type requirements"
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:52
msgid ""
"The type of the mutex passed as a template argument for "
"``concurrent_hash_map`` should meet the requirements of "
"`ReaderWriterMutex "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/named_requirements/mutexes/rw_mutex.html>`_."
" It should also provide the following API:"
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:58
msgid ""
"**Returns**: ``true`` if the ``scoped_lock`` object acquires the mutex as"
" a writer, ``false`` otherwise."
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:60
msgid ""
"The behavior is undefined if the ``scoped_lock`` object does not acquire "
"the mutex."
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:62
msgid ""
"``oneapi::tbb::spin_rw_mutex``, "
"``oneapi::tbb::speculative_spin_rw_mutex``, "
"``oneapi::tbb::queuing_rw_mutex``, ``oneapi::tbb::null_rw_mutex``, and "
"``oneapi::tbb::rw_mutex`` meet the requirements above."
msgstr ""

#: ../../oneTBB/doc/main/reference/custom_mutex_chmap.rst:67
msgid ""
"The example below demonstrates how to wrap ``std::shared_mutex`` (C++17) "
"to meet the requirements of `ReaderWriterMutex` and how to customize "
"``concurrent_hash_map`` to use this mutex."
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:4
msgid "``follows`` and ``precedes`` function templates"
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:9
msgid ""
"The ``follows`` and ``precedes`` helper functions aid in expressing "
"dependencies between nodes when building oneTBB flow graphs. These helper"
" functions can only be used while constructing the node."
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:20
msgid ""
"The ``follows`` helper function specifies that the node being constructed"
" is the successor of the set of nodes passed as an argument."
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:23
msgid ""
"The ``precedes`` helper function specifies that the node being "
"constructed is the predecessor of the set of nodes passed as an argument."
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:26
msgid ""
"Functions ``follows`` and ``precedes`` are meant to replace the graph "
"argument, which is passed as the first argument to the constructor of the"
" node. The graph argument for the node being constructed is obtained "
"either from the specified node set or the sequence of nodes passed to "
"``follows`` or ``precedes``."
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:31
msgid ""
"If the nodes passed to ``follows`` or ``precedes`` belong to different "
"graphs, the behavior is undefined."
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:64
msgid "Input Parameters"
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:66
msgid ""
"Either a set or a sequence of nodes can be used as arguments for "
"``follows`` and ``precedes``. The following expressions are equivalent:"
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:69
msgid "A set of nodes as an input"
msgstr ""

#: ../../oneTBB/doc/main/reference/follows_and_precedes_functions.rst:75
msgid "A sequence of nodes as an input"
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:4
msgid "Helper Functions for Expressing Graphs"
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:9
msgid ""
"Helper functions are intended to make creation of the flow graphs less "
"verbose."
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:18
msgid ""
"This feature adds ``make_edges``, ``make_node_set``, ``follows`` and "
"``precedes`` functions to ``oneapi::tbb::flow`` namespace. These "
"functions simplify the process of building flow graphs by allowing to "
"gather nodes into sets and connect them to other nodes in the graph."
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:37
msgid "Consider the graph depicted below."
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:42
msgid ""
"In the examples below, C++17 Class Template Argument Deduction is used to"
" avoid template parameter specification where possible."
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:45
msgid "**Regular API**"
msgstr ""

#: ../../oneTBB/doc/main/reference/helpers_for_expressing_graphs.rst:85
msgid "**Preview API**"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace.rst:4
msgid "oneapi::tbb::info namespace"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace.rst:6
msgid ""
"The ``oneapi::tbb::info`` namespace satisfies `the corresponding oneTBB "
"specification section "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/info_namespace.html>`_."
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace.rst:9
msgid ""
"The |full_name| implementation requires `the hwloc library <https://www-"
"lb.open-mpi.org/projects/hwloc>`_ to query NUMA(version >= 1.11) and "
"Hybrid CPUs(version >= 2.4) topology information."
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace.rst:14
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:71
msgid ""
"`info namespace specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/info_namespace.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace.rst:17
#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:70
msgid ""
":doc:`task_arena::constraints class preview extensions "
"<constraints_extensions>`"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:4
msgid "oneapi::tbb::info namespace extensions"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:16
msgid "These extensions allow to query information about execution environment."
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:48
msgid "Types"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:50
msgid "``core_type_id`` - Represents core type identifier."
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:53
#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:44
msgid "Functions"
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:57
msgid ""
"Returns the vector of integral indexes that indicate available core "
"types. The indexes are sorted from the least performant to the most "
"performant core type."
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:61
msgid ""
"If error occurs during system topology parsing, returns vector containing"
" single element that equals to ``task_arena::automatic``."
msgstr ""

#: ../../oneTBB/doc/main/reference/info_namespace_extensions.rst:66
msgid "Returns concurrency level for the given constraints."
msgstr ""

#: ../../oneTBB/doc/main/reference/make_edges_function.rst:4
msgid "``make_edges`` function template"
msgstr ""

#: ../../oneTBB/doc/main/reference/make_edges_function.rst:16
msgid ""
"The ``make_edges`` function template creates edges between a single node "
"and each node in a set of nodes."
msgstr ""

#: ../../oneTBB/doc/main/reference/make_edges_function.rst:19
msgid ""
"There are two ways to connect nodes in a set and a single node using "
"``make_edges``:"
msgstr ""

#: ../../oneTBB/doc/main/reference/make_edges_function.rst:51
msgid "The example implements the graph structure in the picture below."
msgstr ""

#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:4
msgid "``make_node_set`` function template"
msgstr ""

#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:16
msgid ""
"The ``make_node_set`` function template creates a set of nodes that can "
"be passed as arguments to ``make_edges``, ``follows`` and ``precedes`` "
"functions."
msgstr ""

#: ../../oneTBB/doc/main/reference/make_node_set_function.rst:40
msgid ":ref:`make_edges`"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:4
msgid "parallel_for_each Body semantics and requirements"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:13
msgid ""
"This page clarifies `ParallelForEachBody "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/named_requirements/algorithms/par_for_each_body.html>`_"
" named requirements for ``tbb::parallel_for_each`` algorithm "
"specification."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:40
msgid "Terms"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:42
msgid ""
"``iterator`` determines the type of the iterator passed into "
"``parallel_for_each`` algorithm (which is ``InputIterator`` for overloads"
" `(1)` and `(2)` and ``decltype(std::begin(c))`` for overloads `(3) - "
"(6)`)"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:44
msgid ""
"``value_type`` - the type ``typename "
"std::iterator_traits<iterator>::value_type``"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:45
msgid ""
"``reference`` -  the type ``typename "
"std::iterator_traits<iterator>::reference``."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:48
msgid "Requirements for different iterator types"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:50
msgid ""
"If the ``iterator`` satisfies `Input iterator` named requirements from "
"[input.iterators] ISO C++ Standard section and do not satisfies `Forward "
"iterator` named requirements from [forward.iterators] ISO C++ Standard "
"section, ``tbb::parallel_for_each`` requires the execution of the "
"``body`` with an object of type ``const value_type&`` or ``value_type&&``"
" to be well-formed. If both forms are well-formed, an overload with "
"rvalue reference will be preferred."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:57
msgid ""
"If the ``Body`` only takes non-const lvalue reference to ``value_type``, "
"named requirements above are violated and the program can be ill-formed."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:59
msgid ""
"If the ``iterator`` satisfies `Forward iterator` named requirements from "
"[forward.iterators] ISO C++ Standard section, ``tbb::parallel_for_each`` "
"requires the execution of the ``body`` with an object of type "
"``reference`` to be well-formed."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:63
msgid "Requirements for ``Body`` with ``feeder`` argument"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_for_each_semantics.rst:65
msgid ""
"Additional elements submitted into ``tbb::parallel_for_each`` through the"
" ``feeder::add`` passes to the ``Body`` as rvalues and therefore the "
"corresponding execution of the ``Body`` is required to be well-formed."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:4
msgid "parallel_sort ranges interface extension"
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:13
msgid ""
"|full_name| implementation extends the `oneapi::tbb::parallel_sort "
"specification "
"<https://spec.oneapi.io/versions/latest/elements/oneTBB/source/algorithms/functions/parallel_sort_func.html>`_"
" with overloads that takes the container by forwarding reference."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:48
msgid ""
"Equivalent to ``parallel_sort( std::begin(c), std::end(c), comp )``, "
"where `comp` uses `operator<` to determine relative orderings."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:52
msgid "Equivalent to ``parallel_sort( std::begin(c), std::end(c), comp )``."
msgstr ""

#: ../../oneTBB/doc/main/reference/parallel_sort_ranges_extension.rst:57
msgid "This interface may be used for sorting rvalue or constant views:"
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:4
msgid "|short_name| API Reference"
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:6
msgid ""
"For oneTBB API Reference, refer to `oneAPI Specification "
"<https://spec.oneapi.com/>`_. The current supported version of oneAPI "
"Specification is 1.0."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:10
msgid "Specification extensions"
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:12
msgid ""
"|full_name| implements the `oneTBB specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/nested-"
"index.html>`_. This document provides additional details or restrictions "
"where necessary. It also describes features that are not included in the "
"oneTBB specification."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:24
msgid "Preview features"
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:26
msgid ""
"A preview feature is a component of oneTBB introduced to receive early "
"feedback from users."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:29
msgid "The key properties of a preview feature are:"
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:31
msgid "It is off by default and must be explicitly enabled."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:32
msgid "It is intended to have a high quality implementation."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:33
msgid "There is no guarantee of future existence or compatibility."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:34
msgid ""
"It may have limited or no support in tools such as correctness analyzers,"
" profilers and debuggers."
msgstr ""

#: ../../oneTBB/doc/main/reference/reference.rst:38
msgid ""
"A preview feature is subject to change in future. It might be removed or "
"significantly altered in future releases. Changes to a preview feature do"
" NOT require usual deprecation and removal process. Therefore, using "
"preview features in production code is strongly discouraged."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:4
msgid "Scalable Memory Pools"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:7
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:7
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:7
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:7
msgid "To enable this feature, set the ``TBB_PREVIEW_MEMORY_POOL`` macro to 1."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:9
msgid ""
"Memory pools allocate and free memory from a specified region or an "
"underlying allocator using thread-safe, scalable operations. The  "
"following table summarizes the Memory Pool named requirement. Here, ``P``"
" represents an instance of the memory pool class."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:18
msgid "Pseudo-Signature"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:19
msgid "Semantics"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:20
msgid "\\ ``~P() throw();``"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:21
msgid "Destructor. Frees all the allocated memory."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:22
msgid "\\ ``void P::recycle();``"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:23
msgid "Frees all the allocated memory."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:24
msgid "\\ ``void* P::malloc(size_t n);``"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:25
msgid "Returns a pointer to ``n`` bytes allocated from the memory pool."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:26
msgid "\\ ``void P::free(void* ptr);``"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:27
msgid "Frees the memory object specified via ``ptr`` pointer."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:28
msgid "\\ ``void* P::realloc(void* ptr, size_t n);``"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:29
msgid "Reallocates the memory object pointed by ``ptr`` to ``n`` bytes."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst
msgid "Model Types"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools.rst:36
msgid ""
"The ``memory_pool`` template class and the ``fixed_pool`` class meet the "
"Memory Pool named requirement."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:4
msgid "fixed_pool"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:9
msgid "A class for scalable memory allocation from a buffer of fixed size."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:18
msgid ""
"``fixed_pool`` allocates and frees memory in a way that scales with the "
"number of processors. All the memory available for the allocation is "
"initially passed through arguments of the constructor. ``fixed_pool`` "
"meet the :doc:`Memory Pool named requirement<../scalable_memory_pools>`."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:59
msgid ""
"**Effects**: Constructs a memory pool to manage the memory of size "
"``size`` pointed to by ``buffer``. Throws the ``bad_alloc`` exception if "
"the library fails to construct an instance of the class."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:63
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:106
#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:69
#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst
#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:41
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst
msgid "Examples"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/fixed_pool_cls.rst:65
msgid "The code below provides a simple example of allocation from a fixed pool."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:4
msgid "memory_pool_allocator"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:9
msgid ""
"A class template that provides a memory pool with a C++ allocator "
"interface."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:18
msgid ""
"``memory_pool_allocator`` meets the allocator requirements from the "
"[allocator.requirements] ISO C++ Standard section It also provides a "
"constructor to allocate and deallocate memory. This constructor is linked"
" with an instance of either the ``memory_pool`` or the ``fixed_pool`` "
"class. The class is mainly intended for enabling memory pools within STL "
"containers."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:97
msgid ""
"**Effects**: Constructs a memory pool allocator serviced by "
"``memory_pool`` instance pool."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:103
msgid ""
"**Effects**: Constructs a memory pool allocator serviced by "
"``fixed_pool`` instance pool."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_allocator_cls.rst:108
msgid ""
"The code below provides a simple example of container construction with "
"the use of a memory pool."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:4
msgid "memory_pool"
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:9
msgid ""
"A class template for scalable memory allocation from memory blocks "
"provided by an underlying allocator."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:18
msgid ""
"A ``memory_pool`` allocates and frees memory in a way that scales with "
"the number of processors. The memory is obtained as big chunks from an "
"underlying allocator specified by the template argument. The latter must "
"satisfy the subset of the allocator requirements from the "
"[allocator.requirements] ISO C++ Standard section. A ``memory_pool`` meet"
" the :doc:`Memory Pool named requirement<../scalable_memory_pools>`."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:25
msgid ""
"If the underlying allocator refers to another scalable memory pool, the "
"inner pool (or pools) must be destroyed before the outer pool is "
"destroyed or recycled."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:65
msgid ""
"**Effects**: Constructs a memory pool with an instance of underlying "
"memory allocator of type ``Alloc`` copied from ``src``. Throws the "
"``bad_alloc`` exception if runtime fails to construct an instance of the "
"class."
msgstr ""

#: ../../oneTBB/doc/main/reference/scalable_memory_pools/memory_pool_cls.rst:71
msgid ""
"The code below provides a simple example of allocation from an extensible"
" memory pool."
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:4
msgid "task_group extensions"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:7
msgid ""
"To enable these extensions, set the ``TBB_PREVIEW_TASK_GROUP_EXTENSIONS``"
" macro to 1."
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:16
msgid ""
"|full_name| implementation extends the `tbb::task_group specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/task_group/task_group_cls.html>`_"
" with the following members:"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:18
msgid "requirements for a user-provided function object"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:65
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:72
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:80
msgid ""
"As an optimization hint, ``F`` might return a ``task_handle``, which task"
" object can be executed next."
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:68
#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:75
msgid ""
"The ``task_handle`` returned by the function must be created using "
"``*this`` ``task_group``. That is, the one for which the run method is "
"called, otherwise it is undefined behavior."
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:83
msgid ""
"The ``task_handle`` returned by the function must be created with "
"``*this`` ``task_group``. It means, with the one for which run method is "
"called, otherwise it is an undefined behavior."
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:87
msgid "See also"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:88
msgid ""
"`oneapi::tbb::task_group specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/task_group/task_group_cls.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:89
msgid ""
"`oneapi::tbb::task_group_context specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/scheduling_controls/task_group_context_cls.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:90
msgid ""
"`oneapi::tbb::task_group_status specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/task_group/task_group_status_enum.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/reference/task_group_extensions.rst:91
msgid ":doc:`oneapi::tbb::task_handle class <task_group_extensions/task_handle>`"
msgstr ""

#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:4
msgid "Type-specified message keys for join_node"
msgstr ""

#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:16
msgid ""
"The extension allows a key matching ``join_node`` to obtain keys via "
"functions associated with its input types. The extension simplifies the "
"existing approach by removing the need to provide a function object for "
"each input port of ``join_node``."
msgstr ""

#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:33
msgid ""
"The extension adds a special constructor to the ``join_node`` interface "
"when the ``key_matching<typename K, class KHash=tbb_hash_compare>`` "
"policy is used. The constructor has the following signature:"
msgstr ""

#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:41
msgid ""
"When constructed this way, a ``join_node`` calls the ``key_from_message``"
" function for each incoming message to obtain the key associated with it."
" The default implementation of ``key_from_message`` is the following"
msgstr ""

#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:58
msgid ""
"``T`` is one of the user-provided types in ``OutputTuple`` and is used to"
" construct the ``join_node``, and ``K`` is the key type of the node. By "
"default, the ``key()`` method defined in the message class will be "
"called. Alternatively, the user can define its own ``key_from_message`` "
"function in the same namespace with the message type. This function will "
"be found via C++ argument-dependent lookup and used in place of the "
"default implementation."
msgstr ""

#: ../../oneTBB/doc/main/reference/type_specified_message_keys.rst:69
msgid ""
"`join_node Specification "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/flow_graph/join_node_cls.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Example.rst:4
msgid "Advanced Example"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Example.rst:7
msgid ""
"An example of a more advanced associative operation is to find the index "
"where ``Foo(i)`` is minimized. A serial version might look like this:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Example.rst:28
msgid ""
"The loop works by keeping track of the minimum value found so far, and "
"the index of this value. This is the only information carried between "
"loop iterations. To convert the loop to use ``parallel_reduce``, the "
"function object must keep track of the carried information, and how to "
"merge this information when iterations are spread across multiple "
"threads. Also, the function object must record a pointer to ``a`` to "
"provide context."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Example.rst:37
msgid "The following code shows the complete function object."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Example.rst:83
msgid ""
"Now ``SerialMinIndex`` can be rewritten using ``parallel_reduce`` as "
"shown below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:4
msgid "Advanced Topic: Other Kinds of Iteration Spaces"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:7
msgid ""
"The examples so far have used the class ``blocked_range<T>`` to specify "
"ranges. This class is useful in many situations, but it does not fit "
"every situation. You can use |full_name| to define your own iteration "
"space objects. The object must specify how it can be split into subspaces"
" by providing a basic splitting constructor, an optional proportional "
"splitting constructor (accompanied with a trait value that enables its "
"usage), and two predicate methods. If your class is called ``R``, the "
"methods and constructors should be as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:38
msgid ""
"The method ``empty`` should return true if the range is empty. The method"
" ``is_divisible`` should return true if the range can be split into two "
"non-empty subspaces, and such a split is worth the overhead. The basic "
"splitting constructor should take two arguments:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:44
msgid "The first of type ``R``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:47
msgid "The second of type oneapi::tbb::split"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:50
msgid ""
"The second argument is not used; it serves only to distinguish the "
"constructor from an ordinary copy constructor. The basic splitting "
"constructor should attempt to split ``r`` roughly into two halves, and "
"update ``r`` to be the first half, and set the constructed object as the "
"second half."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:57
msgid ""
"Unlike the basic splitting constructor, the proportional splitting "
"constructor is optional and takes the second argument of type "
"``oneapi::tbb::proportional_split``. The type has methods ``left`` and "
"``right`` that return the values of the proportion. These values should "
"be used to split ``r`` accordingly, so that the updated ``r`` corresponds"
" to the left part of the proportion, and the constructed object "
"corresponds to the right part. The proportional splitting constructor "
"will be used only if the static constant ``is_splittable_in_proportion`` "
"is defined in the class and assigned the value of ``true``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:68
msgid ""
"Both splitting constructors should guarantee that the updated ``r`` part "
"and the constructed object are not empty. The parallel algorithm "
"templates call the splitting constructors on ``r`` only if "
"``r.is_divisible`` is true."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Advanced_Topic_Other_Kinds_of_Iteration_Spaces.rst:74
msgid ""
"The iteration space does not have to be linear. Look at "
"``oneapi/tbb/blocked_range2d.h`` for an example of a range that is two-"
"dimensional. Its splitting constructor attempts to split the range along "
"its longest axis. When used with ``parallel_for``, it causes the loop to "
"be \"recursively blocked\" in a way that improves cache usage. This nice "
"cache behavior means that using ``parallel_for`` over a "
"``blocked_range2d<T>`` can make a loop run faster than the sequential "
"equivalent, even on a single processor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:4
msgid "Configuring the Memory Allocator"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:7
msgid ""
"The oneTBB memory allocator provides the following API functions and "
"environment variables to configure its behavior:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:11
msgid ""
"the ``scalable_allocation_command`` function instructs the allocator to "
"perform a certain action, such as cleaning up its internal memory "
"buffers."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:16
msgid ""
"the ``scalable_allocation_mode`` function allows an application to set "
"certain parameters for the memory allocator, such as an option to map "
"memory in huge pages or define a recommended heap size. These settings "
"take effect until modified by another call to "
"``scalable_allocation_mode``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:23
msgid ""
"Some of the memory allocator parameters can also be set via system "
"environment variables. It can be useful to adjust the behavior without "
"modifying application source code, to ensure that a setting takes effect "
"as early as possible, or to avoid explicit dependency on the oneTBB "
"allocator binaries. The following environment variables are recognized:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:30
msgid ""
"``TBB_MALLOC_USE_HUGE_PAGES`` controls usage of huge pages for memory "
"mapping."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:34
msgid ""
"``TBB_MALLOC_SET_HUGE_OBJECT_THRESHOLD`` defines the lower bound for the "
"size (bytes), that is interpreted as huge and not released during regular"
" cleanup operations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Allocator_Configuration.rst:39
msgid ""
"These variables only take effect at the time the memory manager is "
"initialized; later environment changes are ignored. A call to "
"``scalable_allocation_mode`` overrides the effect of the corresponding "
"environment variable."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automatic_Chunking.rst:4
msgid "Automatic Chunking"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automatic_Chunking.rst:7
msgid ""
"A parallel loop construct incurs overhead cost for every chunk of work "
"that it schedules. |full_name| chooses chunk sizes automatically, "
"depending upon load balancing needs. The heuristic attempts to limit "
"overheads while still providing ample opportunities for load balancing."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automatic_Chunking.rst:15
msgid ""
"Typically a loop needs to take at least a million clock cycles to make it"
" worth using ``parallel_for``. For example, a loop that takes at least "
"500 microseconds on a 2 GHz processor might benefit from "
"``parallel_for``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automatic_Chunking.rst:21
msgid ""
"The default automatic chunking is recommended for most uses. As with most"
" heuristics, however, there are situations where controlling the chunk "
"size more precisely might yield better performance."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automically_Replacing_malloc.rst:4
msgid ""
"Automatically Replacing ``malloc`` and Other C/C++ Functions for Dynamic "
"Memory Allocation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automically_Replacing_malloc.rst:7
msgid ""
"On Windows*, Linux\\* operating systems, it is possible to automatically "
"replace all calls to standard functions for dynamic memory allocation "
"(such as ``malloc``) with the |full_name| scalable equivalents. Doing so "
"can sometimes improve application performance."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Automically_Replacing_malloc.rst:13
msgid ""
"Replacements are provided by the proxy library (the library names can be "
"found in platform-specific sections below). A proxy library and a "
"scalable memory allocator library should be taken from the same release "
"of oneTBB, otherwise the libraries may be mutually incompatible."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:4
msgid "Bandwidth and Cache Affinity"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:7
msgid ""
"For a sufficiently simple function ``Foo``, the examples might not show "
"good speedup when written as parallel loops. The cause could be "
"insufficient system bandwidth between the processors and memory. In that "
"case, you may have to rethink your algorithm to take better advantage of "
"cache. Restructuring to better utilize the cache usually benefits the "
"parallel program as well as the serial program."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:15
msgid ""
"An alternative to restructuring that works in some cases is "
"``affinity_partitioner.`` It not only automatically chooses the "
"grainsize, but also optimizes for cache affinity and tries to distribute "
"the data uniformly among threads. Using ``affinity_partitioner`` can "
"significantly improve performance when:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:22
msgid "The computation does a few operations per data access."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:25
msgid "The data acted upon by the loop fits in cache."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:28
msgid "The loop, or a similar loop, is re-executed over the same data."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:31
msgid ""
"There are more than two hardware threads available (and especially if the"
" number of threads is not a power of two). If only two threads are "
"available, the default scheduling in |full_name| usually provides "
"sufficient cache affinity."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:37
msgid "The following code shows how to use ``affinity_partitioner``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:58
msgid ""
"In the example, the ``affinity_partitioner`` object ``ap`` lives between "
"loop iterations. It remembers where iterations of the loop ran, so that "
"each iteration can be handed to the same thread that executed it before. "
"The example code gets the lifetime of the partitioner right by declaring "
"the ``affinity_partitioner`` as a local static object. Another approach "
"would be to declare it at a scope outside the iterative loop in "
"``TimeStepFoo``, and hand it down the call chain to ``parallel_for``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:67
msgid ""
"If the data does not fit across the systemâs caches, there may be little "
"benefit. The following figure shows the situations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:75
msgid ""
"Benefit of Affinity Determined by Relative Size of Data Set and Cache "
"|image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:101
#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:133
#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:161
#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:197
#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:142
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:44
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:265
#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:56
#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:56
#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:98
#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:344
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:165
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:135
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:203
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:193
#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:183
msgid "image0"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:79
msgid ""
"The next figure shows how parallel speedup might vary with the size of a "
"data set. The computation for the example is ``A[i]+=B[i]`` for ``i`` in "
"the range [0,N). It was chosen for dramatic effect. You are unlikely to "
"see quite this much variation in your code. The graph shows not much "
"improvement at the extremes. For small N, parallel scheduling overhead "
"dominates, resulting in little speedup. For large N, the data set is too "
"large to be carried in cache between loop invocations. The peak in the "
"middle is the sweet spot for affinity. Hence ``affinity_partitioner`` "
"should be considered a tool, not a cure-all, when there is a low ratio of"
" computations to memory accesses."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:95
msgid "Improvement from Affinity Dependent on Array Size |image1|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:104
#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:164
#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:145
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:268
#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:59
#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:99
#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:347
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:168
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:196
msgid "image1"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Basic_Flow_Graph_concepts.rst:4
msgid "Basic Flow Graph Concepts"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_Without_An_Exception.rst:4
msgid "Cancellation Without An Exception"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_Without_An_Exception.rst:7
msgid ""
"To cancel an algorithm but not throw an exception, use the expression "
"``current_context()->cancel_group_execution()``. The part "
"``current_context()`` references the ``task_group_context*`` of the "
"currently executing task if any on the current thread. Calling "
"``cancel_group_execution()`` cancels all tasks in its "
"``task_group_context``, which is explained in more detail in "
":ref:`Cancellation_and_Nested_Parallelism`. The method returns ``true`` "
"if it actually causes cancellation, ``false`` if the "
"``task_group_context`` was already cancelled."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_Without_An_Exception.rst:12
msgid ""
"The example below shows how to use "
"``current_context()->cancel_group_execution()``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:4
msgid "Cancellation and Nested Parallelism"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:7
msgid ""
"The discussion so far was simplified by assuming non-nested parallelism "
"and skipping details of ``task_group_context``. This topic explains both."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:12
msgid ""
"An |full_name| algorithm executes by creating ``task`` objects that "
"execute the snippets of code that you supply to the algorithm template. "
"By default, these ``task`` objects are associated with a "
"``task_group_context`` created by the algorithm. Nested oneTBB algorithms"
" create a tree of these ``task_group_context`` objects. Cancelling a "
"``task_group_context`` cancels all of its child ``task_group_context`` "
"objects, and transitively all its descendants. Hence an algorithm and all"
" algorithms it called can be cancelled with a single request."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:23
msgid ""
"Exceptions propagate upwards. Cancellation propagates downwards. The "
"opposition interplays to cleanly stop a nested computation when an "
"exception occurs. For example, consider the tree in the following figure."
" Imagine that each node represents an algorithm and its "
"``task_group_context``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:34
msgid "Tree of task_group_context |image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:38
msgid ""
"Suppose that the algorithm in C throws an exception and no node catches "
"the exception. oneTBB propagates the exception upwards, cancelling "
"related subtrees downwards, as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:43
msgid "Handle exception in C:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:46
msgid "Capture exception in C."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:49
msgid "Cancel tasks in C."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:52
msgid "Throw exception from C to B."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:55
msgid "Handle exception in B:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:58
msgid "Capture exception in B."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:61
msgid "Cancel tasks in B and, by downwards propagation, in D."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:64
msgid "Throw an exception out of B to A."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:67
msgid "Handle exception in A:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:70
msgid "Capture exception in A."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:73
msgid "Cancel tasks in A and, by downwards propagation, in E, F, and G."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:76
msgid "Throw an exception upwards out of A."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:79
msgid ""
"If your code catches the exception at any level, then oneTBB does not "
"propagate it any further. For example, an exception that does not escape "
"outside the body of a ``parallel_for`` does not cause cancellation of "
"other iterations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:85
msgid ""
"To prevent downwards propagation of cancellation into an algorithm, "
"construct an 'isolated' ``task_group_context`` on the stack and pass it "
"to the algorithm explicitly. The example uses C++11 lambda expressions "
"for brevity."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:117
msgid ""
"The example performs two parallel loops: an outer loop over ``i`` and "
"inner loop over ``j``. The creation of the isolated "
"``task_group_context`` ``root`` protects the inner loop from downwards "
"propagation of cancellation from the ``i`` loop. When the exception "
"propagates to the outer loop, any pending ``outer`` iterations are "
"cancelled, but not inner iterations for an outer iteration that started. "
"Hence when the program completes, each row of ``Data`` may be different, "
"depending upon whether its iteration ``i`` ran at all, but within a row, "
"the elements will be homogeneously ``false`` or ``true``, not a mixture."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cancellation_and_Nested_Parallelism.rst:128
msgid ""
"Removing the blue text would permit cancellation to propagate down into "
"the inner loop. In that case, a row of ``Data`` might end up with both "
"``true`` and ``false`` values."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:4
msgid "Concurrent Queue Classes"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:7
msgid ""
"Template class ``concurrent_queue<T,Alloc>`` implements a concurrent "
"queue with values of type ``T``. Multiple threads may simultaneously push"
" and pop elements from the queue. The queue is unbounded and has no "
"blocking operations. The fundamental operations on it are ``push`` and "
"``try_pop``. The ``push`` operation works just like ``push`` for a "
"std::queue. The operation ``try_pop`` pops an item if it is available. "
"The check and popping have to be done in a single operation for sake of "
"thread safety."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:17
#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:21
msgid "For example, consider the following serial code:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:32
msgid ""
"Even if each std::queue method were implemented in a thread-safe manner, "
"the composition of those methods as shown in the example would not be "
"thread safe if there were other threads also popping from the same queue."
" For example, ``MySerialQueue.empty()`` might return true just before "
"another thread snatches the last item from ``MySerialQueue``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:39
msgid "The equivalent thread-safe |full_name| code is:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:52
msgid ""
"In a single-threaded program, a queue is a first-in first-out structure. "
"But if multiple threads are pushing and popping concurrently, the "
"definition of \"first\" is uncertain. Use of ``concurrent_queue`` "
"guarantees that if a thread pushes two values, and another thread pops "
"those two values, they will be popped in the same order that they were "
"pushed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:60
msgid ""
"Template class ``concurrent_queue`` is unbounded and has no methods that "
"wait. It is up to the user to provide synchronization to avoid overflow, "
"or to wait for the queue to become non-empty. Typically this is "
"appropriate when the synchronization has to be done at a higher level."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:66
msgid ""
"Template class ``concurrent_bounded_queue<T,Alloc>`` is a variant that "
"adds blocking operations and the ability to specify a capacity. The "
"methods of particular interest on it are:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:71
msgid "``pop(item)`` waits until it can succeed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:74
msgid ""
"``push(item)`` waits until it can succeed without exceeding the queue's "
"capacity."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:78
msgid ""
"``try_push(item)`` pushes ``item`` only if it would not exceed the "
"queue's capacity."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:82
msgid "size() returns a *signed* integer."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:85
msgid ""
"The value of concurrent_queue::size() is defined as the number of push "
"operations started minus the number of pop operations started. If pops "
"outnumber pushes, ``size()`` becomes negative. For example, if a "
"``concurrent_queue`` is empty, and there are ``n`` pending pop "
"operations, ``size()`` returns -\\ ``n``. This provides an easy way for "
"producers to know how many consumers are waiting on the queue. Method "
"``empty()`` is defined to be true if and only if ``size()`` is not "
"positive."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Concurrent_Queue_Classes.rst:95
msgid ""
"By default, a ``concurrent_bounded_queue`` is unbounded. It may hold any "
"number of values, until memory runs out. It can be bounded by setting the"
" queue capacity with method ``set_capacity``.Setting the capacity causes "
"``push`` to block until there is room in the queue. Bounded queues are "
"slower than unbounded queues, so if there is a constraint elsewhere in "
"your program that prevents the queue from becoming too large, it is "
"better not to set the capacity. If you do not need the bounds or the "
"blocking pop, consider using ``concurrent_queue`` instead."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Constraints.rst:4
msgid "Constrained APIs"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Constraints.rst:6
msgid ""
"Starting from C++20, most of |full_name| APIs are constrained to enforce "
"`named requirements "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/named_requirements.html>`_"
" on template arguments types."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Constraints.rst:10
msgid ""
"The violations of these requirements are detected at a compile time "
"during the template instantiation."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Constraints.rst:29
msgid ""
"The code that violates named requirements but compiles successfully until"
" C++20, may not compile in C++20 mode due to early and strict constraints"
" diagnostics."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:4
msgid "Containers"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:7
msgid ""
"|full_name| provides highly concurrent container classes. These "
"containers can be used with raw Windows\\* OS or Linux\\* OS threads, or "
"in conjunction with task-based programming."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:12
msgid ""
"A concurrent container allows multiple threads to concurrently access and"
" update items in the container. Typical C++ STL containers do not permit "
"concurrent update; attempts to modify them concurrently often result in "
"corrupting the container. STL containers can be wrapped in a mutex to "
"make them safe for concurrent access, by letting only one thread operate "
"on the container at a time, but that approach eliminates concurrency, "
"thus restricting parallel speedup."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:21
msgid ""
"Containers provided by oneTBB offer a much higher level of concurrency, "
"via one or both of the following methods:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:25
msgid ""
"**Fine-grained locking:** Multiple threads operate on the container by "
"locking only those portions they really need to lock. As long as "
"different threads access different portions, they can proceed "
"concurrently."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:31
msgid ""
"**Lock-free techniques:** Different threads account and correct for the "
"effects of other interfering threads."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:35
msgid ""
"Notice that highly-concurrent containers come at a cost. They typically "
"have higher overheads than regular STL containers. Operations on highly-"
"concurrent containers may take longer than for STL containers. Therefore,"
" use highly-concurrent containers when the speedup from the additional "
"concurrency that they enable outweighs their slower sequential "
"performance."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Containers.rst:44
msgid ""
"As with most objects in C++, the constructor or destructor of a container"
" object must not be invoked concurrently with another operation on the "
"same object. Otherwise the resulting race may cause the operation to be "
"executed on an undefined object."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:4
msgid "Controlling Chunking"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:7
msgid ""
"Chunking is controlled by a *partitioner* and a *grainsize.*\\  To gain "
"the most control over chunking, you specify both."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:11
msgid ""
"Specify ``simple_partitioner()`` as the third argument to "
"``parallel_for``. Doing so turns off automatic chunking."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:15
msgid ""
"Specify the grainsize when constructing the range. The thread argument "
"form of the constructor is ``blocked_range<T>(begin,end,grainsize)``. The"
" default value of ``grainsize`` is 1. It is in units of loop iterations "
"per chunk."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:21
msgid ""
"If the chunks are too small, the overhead may exceed the performance "
"advantage."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:25
msgid ""
"The following code is the last example from parallel_for, modified to use"
" an explicit grainsize ``G``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:41
msgid ""
"The grainsize sets a minimum threshold for parallelization. The "
"``parallel_for`` in the example invokes ``ApplyFoo::operator()`` on "
"chunks, possibly of different sizes. Let *chunksize* be the number of "
"iterations in a chunk. Using ``simple_partitioner`` guarantees that [G/2]"
" <= *chunksize* <= G."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:48
msgid ""
"There is also an intermediate level of control where you specify the "
"grainsize for the range, but use an ``auto_partitioner`` and "
"``affinity_partitioner``. An ``auto_partitioner`` is the default "
"partitioner. Both partitioners implement the automatic grainsize "
"heuristic described in :ref:`Automatic_Chunking`. An "
"``affinity_partitioner`` implies an additional hint, as explained later "
"in Section :ref:`Bandwidth_and_Cache_Affinity`. Though these partitioners"
" may cause chunks to have more than G iterations, they never generate "
"chunks with less than [G/2] iterations. Specifying a range with an "
"explicit grainsize may occasionally be useful to prevent these "
"partitioners from generating wastefully small chunks if their heuristics "
"fail."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:62
msgid ""
"Because of the impact of grainsize on parallel loops, it is worth reading"
" the following material even if you rely on ``auto_partitioner`` and "
"``affinity_partitioner`` to choose the grainsize automatically."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:73
#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:34
#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:25
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:41
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:106
#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:29
#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:16
#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:43
#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:43
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:73
msgid "|image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:74
#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:57
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:139
#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:39
#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:70
#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:45
msgid "|image1|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:75
msgid "Case A"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:76
msgid "Case B"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:81
msgid ""
"The above figure illustrates the impact of grainsize by showing the "
"useful work as the gray area inside a brown border that represents "
"overhead. Both Case A and Case B have the same total gray area. Case A "
"shows how too small a grainsize leads to a relatively high proportion of "
"overhead. Case B shows how a large grainsize reduces this proportion, at "
"the cost of reducing potential parallelism. The overhead as a fraction of"
" useful work depends upon the grainsize, not on the number of grains. "
"Consider this relationship and not the total number of iterations or "
"number of processors when setting a grainsize."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:92
msgid ""
"A rule of thumb is that ``grainsize`` iterations of ``operator()`` should"
" take at least 100,000 clock cycles to execute. For example, if a single "
"iteration takes 100 clocks, then the ``grainsize`` needs to be at least "
"1000 iterations. When in doubt, do the following experiment:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:98
msgid ""
"Set the ``grainsize`` parameter higher than necessary. The grainsize is "
"specified in units of loop iterations. If you have no idea of how many "
"clock cycles an iteration might take, start with ``grainsize``\\ "
"=100,000. The rationale is that each iteration normally requires at least"
" one clock per iteration. In most cases, step 3 will guide you to a much "
"smaller value."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:106
msgid "Run your algorithm."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:109
msgid ""
"Iteratively halve the ``grainsize`` parameter and see how much the "
"algorithm slows down or speeds up as the value decreases."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:113
msgid ""
"A drawback of setting a grainsize too high is that it can reduce "
"parallelism. For example, if the grainsize is 1000 and the loop has 2000 "
"iterations, the ``parallel_for`` distributes the loop across only two "
"processors, even if more are available. However, if you are unsure, err "
"on the side of being a little too high instead of a little too low, "
"because too low a value hurts serial performance, which in turns hurts "
"parallel performance if there is other parallelism available higher up in"
" the call tree."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:124
msgid "You do not have to set the grainsize too precisely."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:127
msgid ""
"The next figure shows the typical \"bathtub curve\" for execution time "
"versus grainsize, based on the floating point ``a[i]=b[i]*c`` computation"
" over a million indices. There is little work per iteration. The times "
"were collected on a four-socket machine with eight hardware threads."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:138
msgid "Wall Clock Time Versus Grainsize |image2|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:167
#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:146
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:271
msgid "image2"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:142
msgid ""
"The scale is logarithmic. The downward slope on the left side indicates "
"that with a grainsize of one, most of the overhead is parallel scheduling"
" overhead, not useful work. An increase in grainsize brings a "
"proportional decrease in parallel overhead. Then the curve flattens out "
"because the parallel overhead becomes insignificant for a sufficiently "
"large grainsize. At the end on the right, the curve turns up because the "
"chunks are so large that there are fewer chunks than available hardware "
"threads. Notice that a grainsize over the wide range 100-100,000 works "
"quite well."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Controlling_Chunking_os.rst:154
msgid ""
"A general rule of thumb for parallelizing loop nests is to parallelize "
"the outermost one possible. The reason is that each iteration of an outer"
" loop is likely to provide a bigger grain of work than an iteration of an"
" inner loop."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:4
msgid "Cook Until Done: parallel_for_each"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:7
msgid ""
"For some loops, the end of the iteration space is not known in advance, "
"or the loop body may add more iterations to do before the loop exits. You"
" can deal with both situations using the template class "
"``oneapi::tbb::parallel_for_each``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:12
msgid ""
"A linked list is an example of an iteration space that is not known in "
"advance. In parallel programming, it is usually better to use dynamic "
"arrays instead of linked lists, because accessing items in a linked list "
"is inherently serial. But if you are limited to linked lists, the items "
"can be safely processed in parallel, and processing each item takes at "
"least a few thousand instructions, you can use ``parallel_for_each`` to "
"gain some parallelism."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:33
msgid ""
"If ``Foo`` takes at least a few thousand instructions to run, you can get"
" parallel speedup by converting the loop to use ``parallel_for_each``. To"
" do so, define an object with a ``const`` qualified ``operator()``. This "
"is similar to a C++ function object from the C++ standard header "
"``<functional>``, except that ``operator()`` must be ``const``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:52
msgid "The parallel form of ``SerialApplyFooToList`` is as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:63
msgid ""
"An invocation of ``parallel_for_each`` never causes two threads to act on"
" an input iterator concurrently. Thus typical definitions of input "
"iterators for sequential programs work correctly. This convenience makes "
"``parallel_for_each`` unscalable, because the fetching of work is serial."
" But in many situations, you still get useful speedup over doing things "
"sequentially."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:71
msgid "There are two ways that ``parallel_for_each`` can acquire work scalably."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:74
msgid "The iterators can be random-access iterators."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Cook_Until_Done_parallel_do.rst:77
msgid ""
"The body argument to ``parallel_for_each``, if it takes a second argument"
" *feeder* of type ``parallel_for_each<Item>&``, can add more work by "
"calling ``feeder.add(item)``. For example, suppose processing a node in a"
" tree is a prerequisite to processing its descendants. With "
"``parallel_for_each``, after processing a node, you could use "
"``feeder.add`` to add the descendant nodes. The instance of "
"``parallel_for_each`` does not terminate until all items have been "
"processed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:4
msgid "Data Flow Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:7
msgid ""
"In a data flow graph, nodes are computations that send and receive data "
"messages. Some nodes may only send messages, others may only receive "
"messages, and others may send messages in response to messages that they "
"receive."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:13
msgid ""
"In the following data flow graph, the left-most node generates the "
"integer values from 1 to 10 and passes them to two successor nodes. One "
"of the successors squares each value it receives and passes the result "
"downstream. The second successor cubes each value it receives and passes "
"the result downstream. The right-most node receives values from both of "
"the middle nodes. As it receives each value, it adds it to a running sum "
"of values. When the application is run to completion, the value of sum "
"will be equal to the sum of the sequence of squares and cubes from 1 to "
"10."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:28
msgid "Simple Data Flow Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:37
msgid ""
"The following code snippet shows an implementation of the **Simple Data "
"Flow Graph** shown above:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:69
msgid "In the implementation above, the following function_nodes are created:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:72
msgid "one to square values"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:73
msgid "one to cube values"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:74
msgid "one to add values to the global sum"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:77
msgid ""
"Since the squarer and cuber nodes are side-effect free, they are created "
"with an unlimited concurrency. The summer node updates the sum through a "
"reference to a global variable and therefore is not safe to execute in "
"parallel. It is therefore created with a concurrency limit of 1. The node"
" F from **Simple Data Flow Graph** above is implemented as a loop that "
"puts messages to both the squarer and cuber node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:85
msgid ""
"A slight improvement over the first implementation is to introduce an "
"additional node type, a ``broadcast_node``. A ``broadcast_node`` "
"broadcasts any message it receives to all of its successors."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:90
msgid ""
"This enables replacing the two ``try_put``'s in the loop with a single "
"``try_put``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:106
msgid ""
"An even better option, which will make the implementation even more like "
"the **Simple Data Flow Graph** above, is to introduce an ``input_node``. "
"An ``input_node``, as the name implies only sends messages and does not "
"receive messages. Its constructor takes two arguments:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:117
msgid ""
"The body is a function object, or lambda expression, that contains a "
"function operator:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:127
msgid "You can replace the loop in the example with an ``input_node``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:140
msgid ""
"The runtime library will repeatedly invoke the function operator in "
"``src_body`` until ``fc.stop()`` is invoked inside the body. You "
"therefore need to create body that will act like the body of the loop in "
"the **Simple Data Flow Graph** above. The final implementation after all "
"of these changes is shown below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Data_Flow_Graph.rst:189
msgid ""
"This final implementation has all of the nodes and edges from the "
"**Simple Data Flow Graph** above. In this simple example, there is not "
"much advantage in using an ``input_node`` over an explicit loop. But, "
"because an ``input_node`` is able to react to the behavior of downstream "
"nodes, it can limit memory use in more complex graphs. For more "
"information, see:ref:`create_token_based_system` ."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:4
msgid "Debug Versus Release Libraries"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:7
msgid ""
"The following table details the |full_name| dynamic shared libraries that"
" come in debug and release versions."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:18
msgid "Library"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:20
msgid "When to Use"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbb_debug``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbbmalloc_debug``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbbmalloc_proxy_debug``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbbbind_debug``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:25
msgid ""
"These versions have extensive internal checking for correct use of the "
"library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:26
msgid "Use with code that is compiled with the macro ``TBB_USE_DEBUG`` set to 1."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbb``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbbmalloc``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbbmalloc_proxy``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst
msgid "``tbbbind``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:31
msgid ""
"These versions deliver top performance. They eliminate  most checking for"
" correct use of the library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:32
msgid "Use with code compiled with ``TBB_USE_DEBUG`` undefined or set to zero."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:35
msgid ""
"Test your programs with the debug versions of the libraries first, to "
"assure that you are using the library correctly.Â  With the release "
"versions, incorrect usage may result in unpredictable program behavior."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:41
msgid ""
"oneTBB supports IntelÂ® Inspector, IntelÂ® VTuneâ¢ Profiler and IntelÂ® "
"Advisor. Full support of these tools requires compiling with macro "
"``TBB_USE_PROFILING_TOOLS=1``. That symbol defaults to 1 in the following"
" conditions:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:45
msgid "When ``TBB_USE_DEBUG=1``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:46
msgid "On the Microsoft Windows\\* operating system, when ``_DEBUG=1``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:48
msgid "The :ref:`reference` section explains the default values in more detail."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Debug_Versus_Release_Libraries.rst:52
msgid ""
"The instrumentation support for IntelÂ® Inspector becomes live after the "
"first initialization of the task library. If the library components are "
"used before this initialization occurs, IntelÂ® Inspector may falsely "
"report race conditions that are not really races."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:4
msgid "Dependence Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:7
msgid ""
"In a dependence graph, the nodes invoke body objects to perform "
"computations and the edges create a partial ordering of these "
"computations. At runtime, the library spawns and schedules tasks to "
"execute the body objects when it is legal to do so according to the "
"specified partial ordering. The following figure shows an example of an "
"application that could be expressed using a dependence graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:19
msgid "Dependence Graph for Making a Sandwich"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:28
msgid ""
"Dependence graphs are a special case of data flow graphs, where the data "
"passed between nodes are of type oneapi::tbb::flow::continue_msg. Unlike "
"a general data flow graph, nodes in a dependence graph do not spawn a "
"task for each message they receive. Instead, they are aware of the number"
" of predecessors they have, count the messages they receive and only "
"spawn a task to execute their body when this count is equal to the total "
"number of their predecessors."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:37
msgid ""
"The following figure shows another example of a dependence graph. It has "
"the same topology as the figure above, but with simple functions "
"replacing the sandwich making steps. In this partial ordering, function A"
" must complete executing before any other computation starts executing. "
"Function B must complete before C and D start executing; and E must "
"complete before D and F start executing. This is a partial ordering "
"because, for example, there is no explicit ordering requirement between B"
" and E or C and F."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:51
msgid "Simple Dependence Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:60
msgid ""
"To implement this as a flow graph, continue_node objects are used for the"
" nodes and continue_msg objects as the messages. A continue_node "
"constructor takes two arguments:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:71
msgid ""
"The first argument is the graph it belongs to and the second is a "
"function object or lambda expression. Unlike a function_node, a "
"continue_node is always assumed to have unlimited concurrency and will "
"immediately spawn a task whenever its dependencies are met."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:77
msgid ""
"The following code snippet is an implementation of the example in this "
"figure."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:108
msgid ""
"One possible execution of this graph is shown below. The execution of D "
"does not start until both B and E are finished. While a task is waiting "
"in the wait_for_all, its thread can participate in executing other tasks "
"from the oneTBB work pool."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:117
msgid "Execution Timeline for a Dependence Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:123
#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:161
msgid "|image2|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:126
msgid ""
"Again, it is important to note that all execution in the flow graph "
"happens asynchronously. The call to A.try_put returns control to the "
"calling thread quickly, after incrementing the counter and spawning a "
"task to execute the body of A. Likewise, the body tasks execute the "
"lambda expressions and then put a continue_msg to all successor nodes, if"
" any. Only the call to wait_for_all blocks, as it should, and even in "
"this case the calling thread may be used to execute tasks from the oneTBB"
" work pool while it is waiting."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Dependence_Graph.rst:136
msgid ""
"The above timeline shows the sequence when there are enough threads to "
"execute all of the tasks that can be executed concurrently in parallel. "
"If there are fewer threads, then some tasks that are spawned will need to"
" wait until a thread is available to execute them."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Edges.rst:4
msgid "Flow Graph Basics: Edges"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Edges.rst:7
msgid ""
"Most applications contain multiple nodes with edges connecting them to "
"each other. In the flow graph interface, edges are directed channels over"
" which messages are passed. They are created by calling the function "
"``make_edge( p, s )`` with two arguments: ``p``, the predecessor, and "
"``s``, the successor. You can modify the example used in the **Nodes** "
"topic to include a second node that squares the value it receives before "
"printing it and then connect that to the first node with an edge."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Edges.rst:40
msgid ""
"Now there are two ``function_node``s, ``n`` and ``m``. The call to "
"``make_edge`` creates an edge from ``n`` to ``m``. The node n is created "
"with unlimited concurrency, while ``m`` has a concurrency limit of 1. The"
" invocations of ``n`` can all proceed in parallel, while the invocations "
"of ``m`` will be serialized. Because there is an edge from ``n`` to "
"``m``, each value ``v``, returned by ``n``, will be automatically passed "
"to node ``m`` by the runtime library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:4
msgid "Exceptions and Cancellation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:7
msgid ""
"|full_name| supports exceptions and cancellation. When code inside an "
"oneTBB algorithm throws an exception, the following steps generally "
"occur:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:12
msgid ""
"The exception is captured. Any further exceptions inside the algorithm "
"are ignored."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:16
msgid ""
"The algorithm is cancelled. Pending iterations are not executed. If there"
" is oneTBB parallelism nested inside, the nested parallelism may also be "
"cancelled as explained in :ref:`Cancellation_and_Nested_Parallelism`."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:21
msgid ""
"Once all parts of the algorithm stop, an exception is thrown on the "
"thread that invoked the algorithm."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:25
msgid ""
"The exception thrown in step 3 might be the original exception, or might "
"merely be a summary of type ``captured_exception``. The latter usually "
"occurs on current systems because propagating exceptions between threads "
"requires support for the C++ ``std::exception_ptr`` functionality. As "
"compilers evolve to support this functionality, future versions of oneTBB"
" might throw the original exception. So be sure your code can catch "
"either type of exception. The following example demonstrates exception "
"handling."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Exceptions_and_Cancellation.rst:69
msgid ""
"The ``parallel_for`` attempts to iterate over 2000 elements of a vector "
"with only 1000 elements. Hence the expression ``Data.at(i)`` sometimes "
"throws an exception ``std::out_of_range`` during execution of the "
"algorithm. When the exception happens, the algorithm is cancelled and an "
"exception thrown at the call site to ``parallel_for``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph.rst:4
msgid "Parallelizing Data Flow and Dependence Graphs"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:4
msgid "Flow Graph Basics: Buffering and Forwarding"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:7
msgid ""
"|full_name| flow graph nodes use messages to communicate data and to "
"enforce dependencies. If a node passes a message successfully to any "
"successor, no further action is taken with the message by that node. As "
"noted in the section on Single-push vs. Broadcast-push, a message may be "
"passed to one or to multiple successors, depending on the type of the "
"node, how many successors are connected to the node, and whether the "
"message is pushed or pulled."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:16
msgid ""
"There are times when a node cannot successfully push a message to any "
"successor. In this case what happens to the message depends on the type "
"of the node. The two possibilities are:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:21
msgid "The node stores the message to be forwarded later."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:22
msgid "The node discards the message."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:25
msgid ""
"If a node discards messages that are not forwarded, and this behavior is "
"not desired, the node should be connected to a buffering node that does "
"store messages that cannot be pushed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:30
msgid ""
"If a message has been stored by a node, there are two ways it can be "
"passed to another node:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:34
msgid ""
"A successor to the node can pull the message using ``try_get()`` or "
"``try_reserve()``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:36
msgid "A successor can be connected using ``make_edge()``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Buffering_in_Nodes.rst:39
msgid ""
"If a ``try_get()`` successfully forwards a message, it is removed from "
"the node that stored it. If a node is connected using ``make_edge`` the "
"node will attempt to push a stored message to the new successor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:4
msgid "Flow Graph Basics: Message Passing Protocol"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:7
msgid ""
"|full_name| flow graph operates by passing messages between nodes. A node"
" may not be able to receive and process a message from its predecessor. "
"For a graph to operate most-efficiently, if this occurs the state of the "
"edge between the nodes can change its state to pull so when the successor"
" is able to handle a message it can query its predecessor to see if a "
"message is available. If the edge did not reverse from push to pull, the "
"predecessor node would have to repeatedly attempt to forward its message "
"until the successor accepts it. This would consume resources needlessly."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:18
msgid ""
"Once the edge is in pull mode, when the successor is not busy, it will "
"try to pull a message from a predecessor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:22
msgid ""
"If a predecessor has a message, the successor will process it and the "
"edge will remain in pull mode."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:24
msgid ""
"If the predecessor has no message, the edge between the nodes will switch"
" from pull to push mode."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:28
msgid "The state diagram of this Push-Pull protocol is:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Message_Passing_Protocol.rst:35
msgid "**The dynamic push / pull protocol.**"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:4
msgid "Flow Graph Basics: Reservation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:7
msgid ""
"|full_name| flow graph ``join_node`` has four possible policies: "
"``queueing``, ``reserving``, ``key_matching`` and ``tag_matching``. "
"``join_nodes`` need messages at every input before they can create an "
"output message. The reserving ``join_node`` does not have internal "
"buffering, and it does not pull messages from its inputs until it has a "
"message at each input. To create an output message it temporarily "
"reserves a message at each input port, and only if all input ports "
"succeed reserving messages will an output message be created. If any "
"input port fails to reserve a message, no message will be pulled by the "
"``join_node``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:19
msgid ""
"To support the reserving ``join_node`` some nodes support **reservation**"
" of their outputs. The way reservation works is:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:23
msgid ""
"When a node connected to a reserving ``join_node`` in push state tries to"
" push a message, the ``join_node`` always rejects the push and the edge "
"connecting the nodes is switched to pull mode."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:26
msgid ""
"The reserving input port calls ``try_reserve`` on each edge in pull "
"state. This may fail; if so, the reserving input port switches that edge "
"to push state, and tries to reserve the next node connected by an edge in"
" pull state. While the input port's predecessor is in reserved state, no "
"other node can retrieve the reserved value."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:31
msgid ""
"If each input port successfully reserves an edge in pull state, the "
"reserving ``join_node`` will create a message using the reserved messages"
" and try to push the resulting message to any nodes connected to it."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:35
msgid ""
"If the message is successfully pushed to a successor, the predecessors "
"that were reserved are signaled that the messages were used (by calling "
"``try_consume()``.) Those messages will be discarded by the predecessor "
"nodes, because they have been successfully pushed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:39
msgid ""
"If the message was not successfully pushed to any successor, the "
"predecessors that were reserved are signaled that the messages were not "
"used (by calling ``try_release()``.) At this point, the messages may be "
"pushed to or pulled by other nodes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:45
msgid ""
"Because the reserving ``join_node`` will only attempt to push when each "
"input port has at least one edge in a pull state, and will only attempt "
"to create and push a message if all input ports succeed reserving "
"messages, at least one of the predecessors to each of the reserving "
"``join_node`` input ports must be reservable."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:52
msgid ""
"The following example demonstrates a reserving ``join_node``'s behavior. "
"``buffer_nodes`` buffer their output, so they accept a switch of their "
"output edge from push to pull mode. ``broadcast_nodes`` do not buffer "
"messages and do not support ``try_get()`` or ``try_reserve()``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:93
msgid ""
"In the example above, port 0 of the reserving ``join_node`` ``jn`` has "
"two predecessors: a ``buffer_node`` ``buf1`` and a ``broadcast_node`` "
"``bn``. Port 1 of the ``join_node`` has one predecessor, ``buffer_node`` "
"``buf2``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:109
msgid ""
"We will discuss one possible execution sequence (the scheduling of tasks "
"may differ slightly, but the end result will be the same.)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:119
msgid ""
"``bn`` attempts to forward 2 to ``jn``. ``jn`` does not accept the value "
"and the arc from ``bn`` to ``jn`` reverses. Because neither bn nor jn "
"buffer messages, the message is dropped. Because not all the inputs to "
"``jn`` have available predecessors, ``jn`` does nothing further."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:126
msgid ""
"Any node which does not support reservation will not work correctly when "
"attached to a reserving ``join_node``. This program demonstrates why this"
" occurs; connecting non-reserving nodes to nodes requiring support for "
"reservation is **not** recommended practice."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:148
msgid ""
"``buf1`` attempts to forward 3 to ``jn``. ``jn`` does not accept the "
"value and the arc from ``buf1`` to ``jn`` reverses. Because not all the "
"inputs to ``jn`` have available predecessors, ``jn`` does nothing "
"further."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:170
msgid ""
"``buf2`` attempts to forward 4 to ``jn``. ``jn`` does not accept the "
"value and the arc from ``buf2`` to ``jn`` reverses. Now both inputs of "
"``jn`` have predecessors, a task to build and forward a message from "
"``jn`` will be spawned. We assume that task is not yet executing."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:183
msgid "|image3|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:274
msgid "image3"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:192
msgid ""
"``buf2`` has no successor (because the arc to ``jn`` is reversed,) so it "
"stores the value 7."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:203
msgid "|image4|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:277
msgid "image4"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:206
msgid "Now the task spawned to run ``jn`` runs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:209
msgid ""
"``jn`` tries to reserve ``bn``, which fails. The arc to ``bn`` switches "
"back to the forward direction."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:211
msgid ""
"``jn`` tries to reserve ``buf1``, which succeeds (reserved nodes are "
"colored grey.) ``jn`` receives the value 3 from ``buf1``, but it remains "
"in ``buf1`` (in case the attempt to forward a message from ``jn`` fails.)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:215
msgid ""
"``jn`` tries to reserve ``buf2``, which succeeds. ``jn`` receives the "
"value 4 from ``buf2``, but it remains in ``buf2``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:217
msgid "``jn`` constructs the output message ``tuple<3,4>``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:227
msgid "|image5|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:280
msgid "image5"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:230
msgid ""
"Now ``jn`` pushes its message to ``buf_out``, which accepts it. Because "
"the push succeeded, ``jn`` signals ``buf1`` and ``buf2`` that the "
"reserved values were used, and the buffers discard those values. Now "
"``jn`` attempts to reserve again."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:236
msgid ""
"No attempt to pull from ``bn`` is made, because the edge from ``bn`` to "
"``jn`` is in push state."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:238
msgid ""
"``jn`` tries to reserve ``buf1``, which fails. The arc to ``buf1`` "
"switches back to the forward direction."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:240
msgid "``jn`` does not try any further actions."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:250
msgid "|image6|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:283
msgid "image6"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Reservation.rst:253
msgid ""
"No further activity occurs in the graph, and the ``wait_for_all()`` will "
"complete. The output of this code is"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:4
msgid "Flow Graph Basics: Single-push vs. Broadcast-push"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:7
msgid ""
"Nodes in the |full_name| flow graph communicate by pushing and pulling "
"messages. Two policies for pushing messages are used, depending on the "
"type of the node:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:12
msgid ""
"**single-push**: No matter how many successors to the node exist and are "
"able to accept a message, each message will be only sent to one "
"successor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:15
msgid ""
"**broadcast-push**: A message will be pushed to every successor which is "
"connected to the node by an edge in push mode, and which accepts the "
"message."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:20
msgid "The following code demonstrates this difference:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:83
msgid "The output of this code is"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:93
msgid ""
"The single-push test uses a ``buffer_node``, which has a \"single-push\" "
"policy for forwarding messages. Putting three messages to the "
"``buffer_node`` results in three messages being pushed. Notice also only "
"the first ``function_node`` is sent to; in general there is no policy for"
" which node is pushed to if more than one successor can accept."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:100
msgid ""
"The broadcast-push test uses a ``broadcast_node``, which will push any "
"message it receives to all accepting successors. Putting three messages "
"to the ``broadcast_node`` results in a total of nine messages pushed to "
"the ``function_nodes``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:106
msgid ""
"Only nodes designed to buffer (hold and forward received messages) have a"
" \"single-push\" policy; all other nodes have a \"broadcast-push\" "
"policy."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Single_Vs_Broadcast.rst:109
msgid ""
"Please see the :ref:`broadcast_or_send` section of "
":ref:`Flow_Graph_Tips`, and :ref:`Flow_Graph_Buffering_in_Nodes` for more"
" information."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_Tips.rst:4
msgid "Flow Graph Tips and Tricks"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_exception_tips.rst:4
msgid "Flow Graph Tips for Exception Handling and Cancellation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_exception_tips.rst:7
msgid ""
"The execution of a flow graph can be canceled directly or as a result of "
"an exception that propagates beyond a node's body. You can then "
"optionally reset the graph so that it can be re-executed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_making_edges_tips.rst:4
msgid "Flow Graph Tips on Making Edges"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_nested_parallelism_tips.rst:4
msgid "Flow Graph Tips on Nested Parallelism"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_resource_tips.rst:4
msgid "Flow Graph Tips for Limiting Resource Consumption"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_resource_tips.rst:7
msgid ""
"You may want to control the number of messages allowed to enter parts of "
"your graph, or control the maximum number of tasks in the work pool. "
"There are several mechanisms available for limiting resource consumption "
"in a flow graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Flow_Graph_waiting_tips.rst:4
msgid "Flow Graph Tips for Waiting for and Destroying a Flow Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Main_Categories.rst:4
msgid "Graph Application Categories"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Main_Categories.rst:7
msgid "Most flow graphs fall into one of two categories:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Main_Categories.rst:10
msgid ""
"**Data flow graphs.** In this type of graph, data is passed along the "
"graph's edges. The nodes receive, transform and then pass along the data "
"messages."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Main_Categories.rst:13
msgid ""
"**Dependence graphs.** In this type of graph, the data operated on by the"
" nodes is obtained through shared memory directly and is not passed along"
" the edges."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Object.rst:4
msgid "Flow Graph Basics: Graph Object"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Object.rst:7
msgid ""
"Conceptually a flow graph is a collection of nodes and edges. Each node "
"belongs to exactly one graph and edges are made only between nodes in the"
" same graph. In the flow graph interface, a graph object represents this "
"collection of nodes and edges, and is used for invoking whole graph "
"operations such as waiting for all tasks related to the graph to "
"complete, resetting the state of all nodes in the graph, and canceling "
"the execution of all nodes in the graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Graph_Object.rst:16
msgid ""
"The code below creates a graph object and then waits for all tasks "
"spawned by the graph to complete. The call to ``wait_for_all`` in this "
"example returns immediately since this is a trivial graph with no nodes "
"or edges, and therefore no tasks are spawned."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:4
msgid "Guiding Task Scheduler Execution"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:6
msgid ""
"By default, the task scheduler tries to use all available computing "
"resources. In some cases, you may want to configure the task scheduler to"
" use only some of them."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:11
msgid ""
"Guiding the execution of the task scheduler may cause composability "
"issues."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:15
msgid ""
"|full_name| provides the ``task_arena`` interface to guide tasks "
"execution within the arena by:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:14
msgid "setting the preferred computation units;"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:15
msgid "restricting part of computation units."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:17
msgid ""
"Such customizations are encapsulated within the "
"``task_arena::constraints`` structure. To set the limitation, you have to"
" customize the ``task_arena::constraints`` and then pass it to the "
"``task_arena`` instance during the construction or initialization."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:21
msgid ""
"The structure ``task_arena::constraints`` allows to specify the following"
" restrictions:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:23
msgid "Preferred NUMA node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:24
msgid "Preferred core type"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:25
msgid ""
"The maximum number of logical threads scheduled per single core "
"simultaneously"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:26
msgid "The level of ``task_arena`` concurrency"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:28
msgid ""
"You may use the interfaces from ``tbb::info`` namespace to construct the "
"``tbb::task_arena::constraints`` instance. Interfaces from ``tbb::info`` "
"namespace respect the process affinity mask. For instance, if the process"
" affinity mask excludes execution on some of the NUMA nodes, then these "
"NUMA nodes are not returned by ``tbb::info::numa_nodes()`` interface."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:33
msgid "The following examples show how to use these interfaces:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:36
msgid "Setting the preferred NUMA node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:37
msgid ""
"The execution on systems with non-uniform memory access (`NUMA "
"<https://en.wikipedia.org/wiki/Non-uniform_memory_access>`_ systems) may "
"cause a performance penalty if threads from one NUMA node access the "
"memory allocated on a different NUMA node. To reduce this overhead, the "
"work may be divided among several ``task_arena`` instances, whose "
"execution preference is set to different NUMA nodes. To set execution "
"preference, assign a NUMA node identifier to the "
"``task_arena::constraints::numa_id`` field."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:61
msgid "Setting the preferred core type"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:62
msgid ""
"The processors with `IntelÂ® Hybrid Technology "
"<https://www.intel.com/content/www/us/en/products/docs/processors/core"
"/core-processors-with-hybrid-technology-brief.html>`_ contain several "
"core types, each is suited for different purposes. For example, some "
"applications may improve their performance by preferring execution on the"
" most performant cores. To set execution preference, assign specific core"
" type identifier to the ``task_arena::constraints::core_type`` field."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:67
msgid ""
"The example shows how to set the most performant core type as preferable "
"for work execution:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:81
msgid ""
"Limiting the maximum number of threads simultaneously scheduled to one "
"core"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:82
msgid ""
"The processors with `IntelÂ® Hyper-Threading Technology "
"<https://www.intel.com/content/www/us/en/architecture-and-technology"
"/hyper-threading/hyper-threading-technology.html>`_ allow more than one "
"thread to run on each core simultaneously. However, there might be "
"situations when there is need to lower the number of simultaneously "
"running threads per core. In such cases, assign the desired value to the "
"``task_arena::constraints::max_threads_per_core`` field."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:87
msgid ""
"The example shows how to allow only one thread to run on each core at a "
"time:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:96
msgid ""
"A more composable way to limit the number of threads executing on cores "
"is by setting the maximal concurrency of the ``tbb::task_arena``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Guiding_Task_Scheduler_Execution.rst:109
msgid ""
"Similarly to the previous example, the number of threads inside the arena"
" is equal to the number of available cores. However, this one results in "
"fewer overheads and better composability by imposing a less constrained "
"execution."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Initializing_and_Terminating_the_Library.rst:4
msgid "Initializing and Terminating the Library"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Initializing_and_Terminating_the_Library.rst:6
msgid ""
"|full_name| automatically initializes the task scheduler. The "
"initialization process is involved when a thread uses task scheduling "
"services the first time, for example any parallel algorithm, flow graph "
"or task group. The termination happens when the last such thread exits."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Initializing_and_Terminating_the_Library.rst:12
msgid "Explicit Library Finalization"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Initializing_and_Terminating_the_Library.rst:14
msgid ""
"oneTBB supports an explicit library termination as a preview feature. The"
" ``oneapi::tbb::finalize`` function called with an instance of class "
"``oneapi::tbb::task_scheduler_handle`` blocks the calling thread until "
"all worker threads implicitly created by the library have completed. If "
"waiting for thread completion is not safe, e.g. may result in a deadlock "
"or called inside a task, a parallel algorithm, or a flow graph node, the "
"method fails."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Initializing_and_Terminating_the_Library.rst:20
msgid ""
"If you know how many active ``oneapi::tbb::task_scheduler_handle`` "
"instances exist in the program, it is recommended to call "
"``oneapi::tbb::release`` function on all but the last one, then call "
"``oneapi::tbb::finalize`` for the last instance."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Iterating_Over_a_Concurrent_Queue_for_Debugging.rst:4
msgid "Iterating Over a Concurrent Queue for Debugging"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Iterating_Over_a_Concurrent_Queue_for_Debugging.rst:7
msgid ""
"The template classes ``concurrent_queue`` and "
"``concurrent_bounded_queue`` support STL-style iteration. This support is"
" intended only for debugging, when you need to dump a queue. The "
"iterators go forwards only, and are too slow to be very useful in "
"production code. If a queue is modified, all iterators pointing to it "
"become invalid and unsafe to use. The following snippet dumps a queue. "
"The ``operator<<`` is defined for a ``Foo``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Iterating_Over_a_Concurrent_Queue_for_Debugging.rst:27
msgid ""
"The prefix ``unsafe_`` on the methods is a reminder that they are not "
"concurrency safe."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:4
msgid "Lambda Expressions"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:7
msgid ""
"C++11 lambda expressions make the |full_name| ``parallel_for`` much "
"easier to use. A lambda expression lets the compiler do the tedious work "
"of creating a function object."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:12
msgid ""
"Below is the example from the previous section, rewritten with a lambda "
"expression. The lambda expression, replaces both the declaration and "
"construction of function object ``ApplyFoo`` in the example of the "
"previous section."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:36
msgid ""
"The [=] introduces the lambda expression. The expression creates a "
"function object very similar to ``ApplyFoo``. When local variables like "
"``a`` and ``n`` are declared outside the lambda expression, but used "
"inside it, they are \"captured\" as fields inside the function object. "
"The [=] specifies that capture is by value. Writing [&] instead would "
"capture the values by reference. After the [=] is the parameter list and "
"definition for the ``operator()`` of the generated function object. The "
"compiler documentation says more about lambda expressions and other "
"implemented C++11 features. It is worth reading more complete "
"descriptions of lambda expressions than can fit here, because lambda "
"expressions are a powerful feature for using template libraries in "
"general."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:50
msgid ""
"C++11 support is off by default in the compiler. The following table "
"shows the option for turning it on."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:60
msgid "Environment"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:61
msgid "IntelÂ® C++ Compiler Classic"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:62
msgid "IntelÂ® oneAPI DPC++/C++ Compiler"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:63
msgid "Windows\\* OS systems"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:64
msgid "\\ ``icl /Qstd=c++11 foo.cpp``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:65
msgid "\\ ``icx /Qstd=c++11 foo.cpp``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:66
msgid "Linux\\* OS systems"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:67
msgid "\\ ``icc -std=c++11 foo.cpp``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:68
msgid "\\ ``icx -std=c++11 foo.cpp``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:73
msgid ""
"For further compactness, oneTBB has a form of ``parallel_for`` expressly "
"for parallel looping over a consecutive range of integers. The expression"
" ``parallel_for(first,last,step,f)`` is like writing ``for(auto i=first;"
"         i<last;       i+=step)f(i)`` except that each f(i) can be "
"evaluated in parallel if resources permit. The ``step`` parameter is "
"optional. Here is the previous example rewritten in the compact form:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lambda_Expressions.rst:99
msgid ""
"The compact form supports only unidimensional iteration spaces of "
"integers and the automatic chunking feature detailed on the following "
"section."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:4
msgid "Linux\\* OS C/C++ Dynamic Memory Interface Replacement"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:7
msgid ""
"Release version of the proxy library is ``libtbbmalloc_proxy.so``, debug "
"version is ``libtbbmalloc_proxy_debug.so``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:11
#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:11
msgid "The following dynamic memory functions are replaced:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:14
msgid ""
"Standard C library functions: ``malloc``, ``calloc``, ``realloc``, "
"``free``, (added in C11) ``aligned_alloc``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:18
msgid "Standard POSIX\\* function: ``posix_memalign``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:21
msgid "Obsolete functions: ``valloc``, ``memalign``, ``pvalloc``, ``mallopt``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:25
#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:18
msgid "Replaceable global C++ operators ``new`` and ``delete``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:28
msgid ""
"GNU C library (glibc) specific functions: ``malloc_usable_size``, "
"``__libc_malloc``, ``__libc_calloc``, ``__libc_memalign``, "
"``__libc_free``, ``__libc_realloc``, ``__libc_pvalloc``, "
"``__libc_valloc``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:34
msgid ""
"You can do the replacement either by loading the proxy library at program"
" load time using the ``LD_PRELOAD`` environment variable (without "
"changing the executable file), or by linking the main executable file "
"with the proxy library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:40
msgid ""
"The OS program loader must be able to find the proxy library and the "
"scalable memory allocator library at program load time. For that you may "
"include the directory containing the libraries in the ``LD_LIBRARY_PATH``"
" environment variable or add it to ``/etc/ld.so.conf``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:47
msgid "There are limitations for dynamic memory replacement:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:50
msgid ""
"glibc memory allocation hooks, such as ``__malloc_hook``, are not "
"supported."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:54
msgid "Mono is not supported."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:63
msgid ""
"These examples show how to set ``LD_PRELOAD`` and how to link a program "
"to use the memory allocation replacements."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_C_Dynamic_Memory_Interface_Replacement.rst:76
msgid ""
"To use the debug version of the library, replace *tbbmalloc_proxy* with "
"*tbbmalloc_proxy_debug* in the above examples."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:4
msgid "Linux\\*"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:7
msgid ""
"This section uses *<tbb_install_dir>* to indicate the top-level "
"installation directory. The following table describes the subdirectory "
"structure for Linux\\*, relative to *<tbb_install_dir>*"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:16
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:14
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:16
msgid "Item"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:17
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:15
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:17
msgid "Location"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:18
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:16
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:18
msgid "Environment Variable"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:19
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:17
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:19
msgid "Header files"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst
msgid "``include/oneapi/tbb.h``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst
msgid "``include/oneapi/tbb/*.h``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:22
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:20
msgid "``CPATH``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:23
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:21
msgid "Shared libraries"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:24
msgid "``lib/<arch>/<lib><variant>.so.<version>``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst
msgid "``LIBRARY_PATH``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst
msgid "``LD_LIBRARY_PATH``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:28
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:26
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:33
msgid "where"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:30
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:35
msgid "``<arch>`` - ``ia32`` or ``intel64``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:32
msgid ""
"``<lib>`` - ``libtbb``, ``libtbbmalloc``, ``libtbbmalloc_proxy`` or "
"``libtbbbind``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:34
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:30
#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:49
msgid "``<variant>`` - ``_debug`` or empty"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Linux_OS.rst:36
#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:32
msgid "``<version>`` - binary version in a form of ``<major>.<minor>``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:4
msgid "Lock Pathologies"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:7
msgid ""
"Locks can introduce performance and correctness problems. If you are new "
"to locking, here are some of the problems to avoid:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst
msgid "Deadlock"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:17
msgid ""
"Deadlock happens when threads are trying to acquire more than one lock, "
"and each holds some of the locks the other threads need to proceed. More "
"precisely, deadlock happens when:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:22
msgid "There is a cycle of threads"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:25
msgid ""
"Each thread holds at least one lock on a mutex, and is waiting on a mutex"
" for which the *next* thread in the cycle already has a lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:30
msgid "No thread is willing to give up its lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:33
msgid ""
"Think of classic gridlock at an intersection â each car has \"acquired\" "
"part of the road, but needs to \"acquire\" the road under another car to "
"get through. Two common ways to avoid deadlock are:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:38
msgid ""
"Avoid needing to hold two locks at the same time. Break your program into"
" small actions in which each can be accomplished while holding a single "
"lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:43
msgid ""
"Always acquire locks in the same order. For example, if you have \"outer "
"container\" and \"inner container\" mutexes, and need to acquire a lock "
"on one of each, you could always acquire the \"outer sanctum\" one first."
" Another example is \"acquire locks in alphabetical order\" in a "
"situation where the locks have names. Or if the locks are unnamed, "
"acquire locks in order of the mutexâs numerical addresses."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:52
msgid "Use atomic operations instead of locks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst
msgid "Convoying"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:61
msgid ""
"Another common problem with locks is *convoying*. Convoying occurs when "
"the operating system interrupts a thread that is holding a lock. All "
"other threads must wait until the interrupted thread resumes and releases"
" the lock. Fair mutexes can make the situation even worse, because if a "
"waiting thread is interrupted, all the threads behind it must wait for it"
" to resume."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:69
msgid ""
"To minimize convoying, try to hold the lock as briefly as possible. "
"Precompute whatever you can before acquiring the lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Lock_Pathologies.rst:73
msgid "To avoid convoying, use atomic operations instead of locks where possible."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:4
msgid "macOS\\*"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:6
msgid ""
"This section uses *<install_dir>* to indicate the top-level installation "
"directory. The following table describes the subdirectory structure for "
"macOS\\*, relative to *<install_dir>*."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:22
msgid "``lib/<lib><variant>.<version>.dylib``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst
msgid "``DYLD_LIBRARY_PATH``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mac_OS.rst:28
msgid "``<lib>`` - ``libtbb``, ``libtbbmalloc`` or ``libtbbmalloc_proxy``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:4
msgid "Flow Graph Basics: Mapping Nodes to Tasks"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:7
msgid ""
"The following figure shows the timeline for one possible execution of the"
" two node graph example in the previous section. The bodies of n and m "
"will be referred to as Î»\\ :sub:`n` and Î»\\ :sub:`m`, respectively. The "
"three calls to try_put spawn three tasks; each one applies the lambda "
"expression, Î»\\ :sub:`n`, on one of the three input messages. Because n "
"has unlimited concurrency, these tasks can execute concurrently if there "
"are enough threads available. The call to ``g.wait_for_all()`` blocks "
"until there are no tasks executing in the graph. As with other "
"``wait_for_all`` functions in oneTBB, the thread that calls "
"``wait_for_all`` is not spinning idly during this time, but instead can "
"join in executing other tasks from the work pool."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:23
msgid "**Execution Timeline of a Two Node Graph**"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:32
msgid ""
"As each task from n finishes, it puts its output to m, since m is a "
"successor of n. Unlike node n, m has been constructed with a concurrency "
"limit of 1 and therefore does not spawn all tasks immediately. Instead, "
"it sequentially spawns tasks to execute its body, Î»\\ :sub:`m`, on the "
"messages in the order that they arrive. When all tasks are complete, the "
"call to ``wait_for_all`` returns."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:41
msgid ""
"All execution in the flow graph happens asynchronously. The calls to "
"try_put return control to the calling thread quickly, after either "
"immediately spawning a task or buffering the message being passed. "
"Likewise, the body tasks execute the lambda expressions and then put the "
"result to any successor nodes. Only the call to ``wait_for_all`` blocks, "
"as it should, and even in this case the calling thread may be used to "
"execute tasks from the oneTBB work pool while it is waiting."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mapping_Nodes2Tasks.rst:50
msgid ""
"The above timeline shows the sequence when there are enough threads to "
"execute all of the tasks that can be executed in parallel. If there are "
"fewer threads, some spawned tasks will need to wait until a thread is "
"available to execute them."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:4
msgid "Memory Allocation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:7
msgid ""
"|full_name| provides several memory allocator templates that are similar "
"to the STL template class std::allocator. Two templates, "
"``scalable_allocator<T>`` and ``cache_aligned_allocator<T>``, address "
"critical issues in parallel programming as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:14
msgid ""
"**Scalability**. Problems of scalability arise when using memory "
"allocators originally designed for serial programs, on threads that might"
" have to compete for a single shared pool in a way that allows only one "
"thread to allocate at a time."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:20
msgid ""
"Use the ``scalable_allocator<T>`` template to avoid scalability "
"bottlenecks. This template can improve the performance of programs that "
"rapidly allocate and free memory."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:25
msgid ""
"**False sharing**. Problems of sharing arise when two threads access "
"different words that share the same cache line. The problem is that a "
"cache line is the unit of information interchange between processor "
"caches. If one processor modifies a cache line and another processor "
"reads the same cache line, the line must be moved from one processor to "
"the other, even if the two processors are dealing with different words "
"within the line. False sharing can hurt performance because cache lines "
"can take hundreds of clocks to move."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:35
msgid ""
"Use the ``cache_aligned_allocator<T>`` template to always allocate on a "
"separate cache line. Two objects allocated by ``cache_aligned_allocator``"
" are guaranteed to not have false sharing. However, if an object is "
"allocated by ``cache_aligned_allocator`` and another object is allocated "
"some other way, there is no guarantee."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:42
msgid ""
"You can use these allocator templates as the *allocator* argument to STL "
"template classes.The following code shows how to declare an STL vector "
"that uses ``cache_aligned_allocator``\\ for allocation:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:54
msgid ""
"The functionality of ``cache_aligned_allocator<T>`` comes at some cost in"
" space, because it must allocate at least one cache lineâs worth of "
"memory, even for a small object. So use ``cache_aligned_allocator<T>`` "
"only if false sharing is likely to be a real problem."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Memory_Allocation.rst:61
msgid ""
"The scalable memory allocator also provides a set of functions equivalent"
" to the C standard library memory management routines but has the "
"``scalable_`` prefix in their names, as well as the way to easily "
"redirect the standard routines to these functions."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide.rst:4
msgid "Migrating from Threading Building Blocks (TBB)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide.rst:6
msgid ""
"While oneTBB is mostly source compatible with TBB, some interfaces were "
"deprecated in TBB and redesigned or removed in oneTBB. This section "
"considers the most difficult use cases for migrating TBB to oneTBB."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Mixing_Two_Runtimes.rst:4
msgid "Mixing two runtimes"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Mixing_Two_Runtimes.rst:6
msgid ""
"Threading Building Blocks (TBB) and oneAPI Threading Building Blocks "
"(oneTBB) can be safely used in the same application. TBB and oneTBB "
"runtimes are named differently and can be loaded safely within the same "
"process. In addition, the ABI versioning is completely different that "
"prevents symbols conflicts."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Mixing_Two_Runtimes.rst:11
msgid ""
"However, if both runtimes are loaded into the same process it can lead to"
" oversubscription because each runtime will use its own pool of threads. "
"It might lead to a performance penalty due to increased number of context"
" switches. To check if both TBB and oneTBB are loaded to the application,"
" export ``TBB_VERSION=1`` before the application run. If both runtimes "
"are loaded there will be two blocks of output, for example:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Mixing_Two_Runtimes.rst:17
msgid "oneTBB possible output:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Mixing_Two_Runtimes.rst:28
msgid "TBB possible output:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:4
msgid "Migrating from low-level task API"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:6
msgid ""
"The low-level task API of Intel(R) Threading Building Blocks (TBB) was "
"considered complex and hence error-prone, which was the primary reason it"
" had been removed from oneAPI Threading Building Blocks (oneTBB). This "
"guide helps with the migration from TBB to oneTBB for the use cases where"
" low-level task API is used."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:12
msgid "Spawning of individual tasks"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:13
msgid ""
"For most use cases, the spawning of individual tasks can be replaced with"
" the use of either ``oneapi::tbb::task_group`` or "
"``oneapi::tbb::parallel_invoke``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:16
msgid ""
"For example, ``RootTask``, ``ChildTask1``, and ``ChildTask2`` are the "
"user-side functors that inherit ``tbb::task`` and implement its "
"interface. Then spawning of ``ChildTask1`` and ``ChildTask2`` tasks that "
"can execute in parallel with each other and waiting on the ``RootTask`` "
"is implemented as:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:42
msgid "Using ``oneapi::tbb::task_group``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:43
msgid "The code above can be rewritten using ``oneapi::tbb::task_group``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:57
msgid ""
"The code looks more concise now. It also enables lambda functions and "
"does not require you to implement ``tbb::task`` interface that overrides "
"the ``tbb::task* tbb::task::execute()`` virtual method. With this new "
"approach, you work with functors in a C++-standard way by implementing "
"``void operator() const``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:72
msgid "Using ``oneapi::tbb::parallel_invoke``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:73
msgid ""
"It is also possible to use ``oneapi::tbb::parallel_invoke`` to rewrite "
"the original code and make it even more concise:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:90
msgid "Adding more work during task execution"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:91
msgid ""
"``oneapi::tbb::parallel_invoke`` follows a blocking style of programming,"
" which means that it completes only when all functors passed to the "
"parallel pattern complete their execution."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:94
msgid ""
"In TBB, cases when the amount of work is not known in advance and the "
"work needs to be added during the execution of a parallel algorithm were "
"mostly covered by ``tbb::parallel_do`` high-level parallel pattern. The "
"``tbb::parallel_do`` algorithm logic may be implemented using the task "
"API as:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:140
msgid ""
"In oneTBB ``tbb::parallel_do`` interface was removed. Instead, the "
"functionality of adding new work was included into the "
"``oneapi::tbb::parallel_for_each`` interface."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:143
msgid "The previous use case can be rewritten in oneTBB as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:165
msgid ""
"Since both TBB and oneTBB support nested expressions, you can run "
"additional functors from within an already running functor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:168
msgid ""
"The previous use case can be rewritten using ``oneapi::tbb::task_group`` "
"as:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:196
msgid "Task recycling"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:197
msgid ""
"You can re-run the functor by passing ``*this`` to the "
"``oneapi::tbb::task_group::run()`` method. The functor will be copied in "
"this case. However, its state can be shared among instances:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:227
msgid ""
"Such patterns are particularly useful when the work within a functor is "
"not completed but there is a need for the task scheduler to react to "
"outer circumstances, such as cancellation of group execution. To avoid "
"issues with concurrent access, it is recommended to submit it for re-"
"execution as the last step:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:261
msgid "Recycling as child or continuation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:262
msgid ""
"In oneTBB this kind of recycling is done manually. You have to track when"
" it is time to run the task:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:331
msgid "Scheduler Bypass"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:333
msgid ""
"TBB ``task::execute()`` method can return a pointer to a task that can be"
" executed next by the current thread. This might reduce scheduling "
"overheads compared to direct ``spawn``. Similar to ``spawn``, the "
"returned task is not guaranteed to be executed next by the current "
"thread."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:367
#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:423
msgid "In oneTBB, this can be done using ``oneapi::tbb::task_group``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:387
msgid ""
"Here ``oneapi::tbb::task_group::defer`` adds a new task into the ``tg``. "
"However, the task is not put into a queue of tasks ready for execution "
"via ``oneapi::tbb::task_group::run``, but bypassed to the executing "
"thread directly via function return value."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:392
msgid "Deferred task creation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:393
msgid ""
"The TBB low-level task API separates the task creation from the actual "
"spawning. This separation allows to postpone the task spawning, while the"
" parent task and final result production are blocked from premature "
"leave. For example, ``RootTask``, ``ChildTask``, and ``CallBackTask`` are"
" the user-side functors that inherit ``tbb::task`` and implement its "
"interface. Then, blocking the ``RootTask`` from leaving prematurely and "
"waiting on it is implemented as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:448
msgid ""
"Here ``oneapi::tbb::task_group::defer`` adds a new task into the ``tg``. "
"However, the task is not spawned until "
"``oneapi::tbb::task_arena::enqueue`` is called."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_API.rst:452
msgid ""
"The call to ``oneapi::tbb::task_group::wait`` will not return control "
"until both ``ChildTask`` and ``CallBackTask`` are executed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:4
msgid "Migrating from tbb::task_scheduler_init"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:6
msgid ""
"``tbb::task_scheduler_init`` was a multipurpose functionality in the "
"previous versions of Threading Building Blocks (TBB). This section "
"considers different use cases and how they can be covered with oneTBB."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:11
msgid "Managing the number of threads"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:14
msgid "Querying the default number of threads"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:16
msgid ""
"`oneapi::tbb::info::default_concurrency() "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/info_namespace.html>`_"
" returns the maximum concurrency that will be created by *default* in "
"implicit or explicit ``task_arena``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:20
msgid ""
"`oneapi::tbb::this_task_arena::max_concurrency() "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/task_arena/this_task_arena_ns.html>`_"
" returns the maximum number of threads available for the parallel "
"algorithms within the current context (or *default* if an implicit arena "
"is not initialized)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:25
msgid ""
"`oneapi::tbb::global_control::active_value(tbb::global_control::max_allowed_parallelism)"
" "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/scheduling_controls/global_control_cls.html>`_"
" returns the current limit of the thread pool (or *default* if oneTBB "
"scheduler is not initialized)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:30
msgid "Setting the maximum concurrency"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:32
msgid ""
"`task_arena(/* max_concurrency */) "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/task_arena/this_task_arena_ns.html>`_"
" limits the maximum concurrency of the parallel algorithm running inside "
"``task_arena``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:36
msgid ""
"`tbb::global_control(tbb::global_control::max_allowed_parallelism, /* "
"max_concurrency */) "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/scheduling_controls/global_control_cls.html>`_"
" limits the total number of oneTBB worker threads"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:43
msgid "The default parallelism:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:74
msgid "The limited parallelism:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:117
msgid "Setting thread stack size"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:118
msgid ""
"Use "
"`oneapi::tbb::global_control(oneapi::tbb::global_control::thread_stack_size,"
" /* stack_size */) "
"<https://spec.oneapi.com/versions/latest/elements/oneTBB/source/task_scheduler/scheduling_controls/global_control_cls.html>`_"
" to set the stack size for oneTBB worker threads:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:142
msgid "Terminating oneTBB scheduler"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Migration_Guide/Task_Scheduler_Init.rst:143
msgid ""
":ref:`task_scheduler_handle_reference` allows waiting for oneTBB worker "
"threads completion:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:4
msgid "More on HashCompare"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:7
msgid ""
"There are several ways to make the ``HashCompare`` argument for "
"``concurrent_hash_map`` work for your own types."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:11
msgid "Specify the ``HashCompare`` argument explicitly"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:14
msgid ""
"Let the ``HashCompare`` default to ``tbb_hash_compare<Key>`` and do one "
"of the following:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:18
msgid "Define a specialization of template ``tbb_hash_compare<Key>``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:21
msgid ""
"For example, if you have keys of type ``Foo``, and ``operator==`` is "
"defined for ``Foo``, you just have to provide a definition of "
"``tbb_hasher`` as shown below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:35
msgid ""
"In general, the definition of ``tbb_hash_compare<Key>`` or "
"``HashCompare`` must provide two signatures:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:39
msgid "A method ``hash`` that maps a ``Key`` to a ``size_t``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:42
msgid "A method ``equal`` that determines if two keys are equal"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:45
msgid ""
"The signatures go together in a single class because *if two keys are "
"equal, then they must hash to the same value*, otherwise the hash table "
"might not work. You could trivially meet this requirement by always "
"hashing to ``0``, but that would cause tremendous inefficiency. Ideally, "
"each key should hash to a different value, or at least the probability of"
" two distinct keys hashing to the same value should be kept low."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/More_on_HashCompare.rst:53
msgid ""
"The methods of ``HashCompare`` should be ``static`` unless you need to "
"have them behave differently for different instances. If so, then you "
"should construct the ``concurrent_hash_map`` using the constructor that "
"takes a ``HashCompare`` as a parameter. The following example is a "
"variation on an earlier example with instance-dependent methods. The "
"instance performs both case-sensitive or case-insensitive hashing, and "
"comparison, depending upon an internal flag ``ignore_case``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:4
msgid "Mutex Flavors"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:7
msgid ""
"Connoisseurs of mutexes distinguish various attributes of mutexes. It "
"helps to know some of these, because they involve tradeoffs of generality"
" and efficiency. Picking the right one often helps performance. Mutexes "
"can be described by the following qualities, also summarized in the table"
" below."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:14
msgid ""
"**Scalable**. Some mutexes are called *scalable*. In a strict sense, this"
" is not an accurate name, because a mutex limits execution to one thread "
"at a time. A *scalable mutex* is one that does not do *worse* than this. "
"A mutex can do worse than serialize execution if the waiting threads "
"consume excessive processor cycles and memory bandwidth, reducing the "
"speed of threads trying to do real work. Scalable mutexes are often "
"slower than non-scalable mutexes under light contention, so a non-"
"scalable mutex may be better. When in doubt, use a scalable mutex."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:25
msgid ""
"**Fair**. Mutexes can be *fair* or *unfair*. A fair mutex lets threads "
"through in the order they arrived. Fair mutexes avoid starving threads. "
"Each thread gets its turn. However, unfair mutexes can be faster, because"
" they let threads that are running go through first, instead of the "
"thread that is next in line which may be sleeping on account of an "
"interrupt."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:33
msgid ""
"**Yield or Block**. This is an implementation detail that impacts "
"performance. On long waits, an |full_name| mutex either *yields* or "
"*blocks*. Here *yields* means to repeatedly poll whether progress can be "
"made, and if not, temporarily yield [#]_ the processor. To *block* means "
"to yield the processor until the mutex permits progress. Use the yielding"
" mutexes if waits are typically short and blocking mutexes if waits are "
"typically long."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:43
msgid "The following is a summary of mutex behaviors:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:46
msgid ""
"``spin_mutex`` is non-scalable, unfair, non-recursive, and spins in user "
"space. It would seem to be the worst of all possible worlds, except that "
"it is *very fast* in *lightly contended* situations. If you can design "
"your program so that contention is somehow spread out among many "
"``spin_mutex`` objects, you can improve performance over using other "
"kinds of mutexes. If a mutex is heavily contended, your algorithm will "
"not scale anyway. Consider redesigning the algorithm instead of looking "
"for a more efficient lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:56
msgid ""
"``mutex`` has behavior similar to the ``spin_mutex``. However, the "
"``mutex`` *blocks* on long waits that makes it resistant to high "
"contention."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:61
msgid ""
"``queuing_mutex`` is scalable, fair, non-recursive, and spins in user "
"space. Use it when scalability and fairness are important."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:65
msgid ""
"``spin_rw_mutex`` and ``queuing_rw_mutex`` are similar to ``spin_mutex`` "
"and ``queuing_mutex``, but additionally support *reader* locks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:70
msgid ""
"``rw_mutex`` is similar to ``mutex``, but additionally support *reader* "
"locks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:74
msgid ""
"``speculative_spin_mutex`` and ``speculative_spin_rw_mutex`` are similar "
"to ``spin_mutex`` and ``spin_rw_mutex``, but additionally provide "
"*speculative locking* on processors that support hardware transaction "
"memory. Speculative locking allows multiple threads acquire the same "
"lock, as long as there are no \"conflicts\" that may generate different "
"results than non-speculative locking. These mutexes are *scalable* when "
"work with low conflict rate, i.e. mostly in speculative locking mode."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:84
msgid ""
"``null_mutex`` and ``null_rw_mutex`` do nothing. They can be useful as "
"template arguments. For example, suppose you are defining a container "
"template and know that some instantiations will be shared by multiple "
"threads and need internal locking, but others will be private to a thread"
" and not need locking. You can define the template to take a Mutex type "
"parameter. The parameter can be one of the real mutex types when locking "
"is necessary, and ``null_mutex`` when locking is unnecessary."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:100
msgid "Mutex"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:101
msgid "Scalable"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:102
msgid "Fair"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:103
msgid "Recursive"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:104
msgid "Long Wait"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:105
msgid "Size"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:106
msgid "\\ ``spin_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:107
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:108
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:109
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:114
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:115
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:120
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:121
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:127
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:131
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:132
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:133
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:138
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:139
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:144
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:145
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:151
msgid "no"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:110
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:122
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:128
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:134
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:146
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:152
msgid "yields"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:111
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:117
msgid "1 byte"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:112
msgid "\\ ``mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:113
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:125
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:126
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:137
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:149
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:150
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:156
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:157
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:162
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:163
msgid "â"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:116
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:140
msgid "blocks"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:118
msgid "\\ ``speculative_spin_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:119
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:143
msgid "HW dependent"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:123
msgid "2 cache lines"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:124
msgid "\\ ``queuing_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:129
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:135
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:141
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:153
msgid "1 word"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:130
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:136
msgid "\\ ``spin_rw_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:142
msgid "\\ ``speculative_spin_rw_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:147
msgid "3 cache lines"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:148
msgid "\\ ``queuing_rw_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:154
msgid "\\ ``null_mutex`` [#]_"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:155
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:161
msgid "moot"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:158
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:164
msgid "never"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:159
#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:165
msgid "empty"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:160
msgid "\\ ``null_rw_mutex``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:170
msgid ""
"The yielding is implemented via ``SwitchToThread()`` on Microsoft "
"Windows\\* operating systems and by ``sched_yield()`` on other systems."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutex_Flavors.rst:174
msgid ""
"Null mutexes are considered fair by oneTBB because they cannot cause "
"starvation. They lack any non-static data members."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:4
msgid "Mutual Exclusion"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:7
msgid ""
"Mutual exclusion controls how many threads can simultaneously run a "
"region of code. In |full_name|, mutual exclusion is implemented by "
"*mutexes* and *locks.* A mutex is an object on which a thread can acquire"
" a lock. Only one thread at a time can have a lock on a mutex; other "
"threads have to wait their turn."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:14
msgid ""
"The simplest mutex is ``spin_mutex``. A thread trying to acquire a lock "
"on a ``spin_mutex`` busy waits until it can acquire the lock. A "
"``spin_mutex`` is appropriate when the lock is held for only a few "
"instructions. For example, the following code uses a mutex "
"``FreeListMutex`` to protect a shared variable ``FreeList``. It checks "
"that only a single thread has access to ``FreeList`` at a time."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:49
msgid ""
"The constructor for ``scoped_lock`` waits until there are no other locks "
"on ``FreeListMutex``. The destructor releases the lock. The braces inside"
" routine ``AllocateNode`` may look unusual. Their role is to keep the "
"lifetime of the lock as short as possible, so that other waiting threads "
"can get their chance as soon as possible."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:57
msgid ""
"Be sure to name the lock object, otherwise it will be destroyed too soon."
" For example, if the creation of the ``scoped_lock`` object in the "
"example is changed to"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:65
msgid ""
"then the ``scoped_lock`` is destroyed when execution reaches the "
"semicolon, which releases the lock *before* ``FreeList`` is accessed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:69
msgid "The following shows an alternative way to write ``AllocateNode``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:89
msgid ""
"Method ``acquire`` waits until it can acquire a lock on the mutex; method"
" ``release`` releases the lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:93
msgid ""
"It is recommended that you add extra braces where possible, to clarify to"
" maintainers which code is protected by the lock."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:97
msgid ""
"If you are familiar with C interfaces for locks, you may be wondering why"
" there are not simply acquire and release methods on the mutex object "
"itself. The reason is that the C interface would not be exception safe, "
"because if the protected region threw an exception, control would skip "
"over the release. With the object-oriented interface, destruction of the "
"``scoped_lock`` object causes the lock to be released, no matter whether "
"the protected region was exited by normal control flow or an exception. "
"This is true even for our version of ``AllocateNode`` that used methods "
"``acquire`` and ``release â`` the explicit release causes the lock to be "
"released earlier, and the destructor then sees that the lock was released"
" and does nothing."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:110
msgid ""
"All mutexes in oneTBB have a similar interface, which not only makes them"
" easier to learn, but enables generic programming. For example, all of "
"the mutexes have a nested ``scoped_lock`` type, so given a mutex of type "
"``M``, the corresponding lock type is ``M::scoped_lock``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Mutual_Exclusion.rst:117
msgid ""
"It is recommended that you always use a ``typedef`` for the mutex type, "
"as shown in the previous examples. That way, you can change the type of "
"the lock later without having to edit the rest of the code. In the "
"examples, you could replace the ``typedef`` with ``typedef queuing_mutex "
"FreeListMutexType``, and the code would still be correct."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:4
msgid "Flow Graph Basics: Nodes"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:7
msgid ""
"A node is a class that inherits from oneapi::tbb::flow::graph_node and "
"also typically inherits from oneapi::tbb::flow::sender<T> , "
"oneapi::tbb::flow::receiver<T> or both. A node performs some operation, "
"usually on an incoming message and may generate zero or more output "
"messages. Some nodes require more than one input message or generate more"
" than one output message."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:14
msgid ""
"While it is possible to define your own node types by inheriting from "
"graph_node, sender and receiver, it is more typical that predefined node "
"types are used to construct a graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:19
msgid ""
"A ``function_node`` is a predefined type available in ``flow_graph.h`` "
"and represents a simple function with one input and one output. The "
"constructor for a ``function_node`` takes three arguments:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:36
msgid "Parameter"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:38
msgid "Body"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:39
msgid "Type of the body object."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:40
msgid "g"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:41
msgid "The graph the node belongs to."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:42
msgid "concurrency"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:43
msgid ""
"The concurrency limit for the node. You can use the    concurrency limit "
"to control how many invocations of the node are   allowed to proceed "
"concurrently, from 1 (serial) to an unlimited   number."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:44
msgid "body"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:45
msgid ""
"User defined function object, or lambda expression, that    is applied to"
" the incoming message to generate the outgoing message."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:50
msgid ""
"Below is code for creating a simple graph that contains a single "
"function_node. In this example, a node n is constructed that belongs to "
"graph g, and has a second argument of 1, which allows at most 1 "
"invocation of the node to occur concurrently. The body is a lambda "
"expression that prints each value v that it receives, spins for v "
"seconds, prints the value again, and then returns v unmodified. The code "
"for the function spin_for is not provided."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:71
msgid ""
"After the node is constructed in the example above, you can pass messages"
" to it, either by connecting it to other nodes using edges or by invoking"
" its function try_put. Using edges is described in the next section."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:85
msgid ""
"You can then wait for the messages to be processed by calling "
"wait_for_all on the graph object:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:95
msgid ""
"In the above example code, the function_node n was created with a "
"concurrency limit of 1. When it receives the message sequence 1, 2 and 3,"
" the node n will spawn a task to apply the body to the first input, 1. "
"When that task is complete, it will then spawn another task to apply the "
"body to 2. And likewise, the node will wait for that task to complete "
"before spawning a third task to apply the body to 3. The calls to try_put"
" do not block until a task is spawned; if a node cannot immediately spawn"
" a task to process the message, the message will be buffered in the node."
" When it is legal, based on concurrency limits, a task will be spawned to"
" process the next buffered message."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:107
msgid ""
"In the above graph, each message is processed sequentially. If however, "
"you construct the node with a different concurrency limit, parallelism "
"can be achieved:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:123
msgid ""
"You can use unlimited as the concurrency limit to instruct the library to"
" spawn a task as soon as a message arrives, regardless of how many other "
"tasks have been spawned. You can also use any specific value, such as 4 "
"or 8, to limit concurrency to at most 4 or 8, respectively. It is "
"important to remember that spawning a task does not mean creating a "
"thread. So while a graph may spawn many tasks, only the number of threads"
" available in the library's thread pool will be used to execute these "
"tasks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:133
msgid ""
"Suppose you use unlimited in the function_node constructor instead and "
"call try_put on the node:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Nodes.rst:146
msgid ""
"The library spawns three tasks, each one applying n's lambda expression "
"to one of the messages. If you have a sufficient number of threads "
"available on your system, then all three invocations of the body will "
"occur in parallel. If however, you have only one thread in the system, "
"they execute sequentially."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:4
msgid "Non-Linear Pipelines"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:7
msgid ""
"Template function ``parallel_pipeline`` supports only linear pipelines. "
"It does not directly handle more baroque plumbing, such as in the diagram"
" below."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:19
msgid ""
"However, you can still use a pipeline for this. Just topologically sort "
"the filters into a linear order, like this:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:23
msgid ""
"The light gray arrows are the original arrows that are now implied by "
"transitive closure of the other arrows. It might seem that lot of "
"parallelism is lost by forcing a linear order on the filters, but in fact"
" the only loss is in the *latency* of the pipeline, not the throughput. "
"The latency is the time it takes a token to flow from the beginning to "
"the end of the pipeline. Given a sufficient number of processors, the "
"latency of the original non-linear pipeline is three filters. This is "
"because filters A and B could process the token concurrently, and "
"likewise filters D and E could process the token concurrently."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:42
msgid ""
"In the linear pipeline, the latency is five filters. The behavior of "
"filters A, B, D and E above may need to be modified in order to properly "
"handle objects that donât need to be acted upon by the filter other than "
"to be passed along to the next filter in the pipeline."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Non-Linear_Pipelines.rst:48
msgid ""
"The throughput remains the same, because regardless of the topology, the "
"throughput is still limited by the throughput of the slowest serial "
"filter. If ``parallel_pipeline`` supported non-linear pipelines, it would"
" add a lot of programming complexity, and not improve throughput. The "
"linear limitation of ``parallel_pipeline`` is a good tradeoff of gain "
"versus pain."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Package_Contents.rst:4
msgid "Package Contents"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Package_Contents.rst:7
msgid ""
"|full_name| includes dynamic library files and header files for "
"Windows\\*, Linux\\* and macOS\\* operating systems as described in this "
"section."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Complex_Loops.rst:4
msgid "Parallelizing Complex Loops"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Complex_Loops.rst:7
msgid ""
"You can successfully parallelize many applications using only the "
"constructs in the **Parallelizing Simple Loops** section. However, some "
"situations call for other parallel patterns. This section describes the "
"support for some of these alternate patterns."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:4
msgid "Parallelizing Data Flow and Dependency Graphs"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:7
msgid ""
"In addition to loop parallelism, the |full_name| library also supports "
"graph parallelism. It's possible to create graphs that are highly "
"scalable, but it is also possible to create graphs that are completely "
"sequential."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:12
msgid ""
"Using graph parallelism, computations are represented by nodes and the "
"communication channels between these computations are represented by "
"edges. When a node in the graph receives a message, a task is spawned to "
"execute its body object on the incoming message. Messages flow through "
"the graph across the edges that connect the nodes. The following sections"
" present two examples of applications that can be expressed as graphs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:21
msgid ""
"The following figure shows a *streaming* or *data flow* application where"
" a sequence of values is processed as each value passes through the nodes"
" in the graph. In this example, the sequence is created by a function F. "
"For each value in the sequence, G squares the value and H cubes the "
"value. J then takes each of the squared and cubed values and adds them to"
" a global sum. After all values in the sequence are completely processed,"
" sum is equal to the sum of the sequence of squares and cubes from 1 to "
"10. In a streaming or data flow graph, the values actually flow across "
"the edges; the output of one node becomes the input of its successor(s)."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:37
msgid "**Simple Data Flow Graph**"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:46
msgid ""
"The following graphic shows a different form of graph application. In "
"this example, a dependence graph is used to establish a partial ordering "
"among the steps for making a peanut butter and jelly sandwich. In this "
"partial ordering, you must first get the bread before spreading the "
"peanut butter or jelly on the bread. You must spread on the peanut butter"
" before you put away the peanut butter jar, and likewise spread on the "
"jelly before you put away the jelly jar. And, you need to spread on both "
"the peanut butter and jelly before putting the two slices of bread "
"together. This is a partial ordering because, for example, it doesn't "
"matter if you spread on the peanut butter first or the jelly first. It "
"also doesn't matter if you finish making the sandwich before putting away"
" the jars."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:64
msgid "**Dependence Graph for Making a Sandwich**"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:73
msgid ""
"While it can be inferred that resources, such as the bread, or the jelly "
"jar, are shared between ordered steps, it is not explicit in the graph. "
"Instead, only the required ordering of steps is explicit in a dependence "
"graph. For example, you must \"Put jelly on 1 slice\" **before** you "
"\"Put away jelly jar\"."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:80
msgid ""
"The flow graph interface in the oneTBB library allows you to express data"
" flow and dependence graphs such as these, as well as more complicated "
"graphs that include cycles, conditionals, buffering and more. If you "
"express your application using the flow graph interface, the runtime "
"library spawns tasks to exploit the parallelism that is present in the "
"graph. For example, in the first example above, perhaps two different "
"values might be squared in parallel, or the same value might be squared "
"and cubed in parallel. Likewise in the second example, the peanut butter "
"might be spread on one slice of bread in parallel with the jelly being "
"spread on the other slice. The interface expresses what is legal to "
"execute in parallel, but allows the runtime library to choose at runtime "
"what will be executed in parallel."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Flow_Graph.rst:94
msgid ""
"The support for graph parallelism is contained within the namespace "
"``oneapi::tbb::flow`` and is defined in the ``flow_graph.h`` header file."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:4
msgid "Parallelizing Simple Loops"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:7
msgid ""
"The simplest form of scalable parallelism is a loop of iterations that "
"can each run simultaneously without interfering with each other. The "
"following sections demonstrate how to parallelize simple loops."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:13
msgid ""
"|full_name| components are defined in namespace ``tbb``. For brevityâs "
"sake, the namespace is explicit in the first mention of a component, but "
"implicit afterwards."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:19
msgid ""
"When compiling oneTBB programs, be sure to link in the oneTBB shared "
"library, otherwise undefined references will occur. The following table "
"shows compilation commands that use the debug version of the library. "
"Remove the \"``_debug``\" portion to link against the production version "
"of the library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:32
msgid "Operating System"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:33
msgid "Command line"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:34
msgid "Windows\\* OS"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:35
msgid "``icl /MD example.cpp tbb_debug.lib``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:36
msgid "Linux\\* OS"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Parallelizing_Simple_Loops_os.rst:37
msgid "``icc example.cpp -ltbb_debug``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:4
msgid "Partitioner Summary"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:7
msgid ""
"The parallel loop templates ``parallel_for`` and ``parallel_reduce`` take"
" an optional *partitioner* argument, which specifies a strategy for "
"executing the loop. The following table summarizes partitioners and their"
" effect when used in conjunction with ``blocked_range``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:19
msgid "Partitioner"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:21
msgid "When Used with ``blocked_range(i,j,g)``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:22
msgid "``simple_partitioner``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:23
msgid "Chunksize bounded by grain size."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:24
msgid "``g/2 â¤ chunksize â¤ g``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:25
msgid "``auto_partitioner`` (default)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:26
msgid "Automatic chunk size."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:27
#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:30
msgid "``g/2 â¤ chunksize``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:28
msgid "``affinity_partitioner``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:29
msgid ""
"Automatic chunk size, cache affinity and uniform distribution of "
"iterations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:31
msgid "``static_partitioner``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:32
msgid ""
"Deterministic chunk size, cache affinity and uniform distribution of "
"iterations without load balancing."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:33
msgid "``max(g/3, problem_size/num_of_resources) â¤ chunksize``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:38
msgid ""
"An ``auto_partitioner`` is used when no partitioner is specified. In "
"general, the ``auto_partitioner`` or ``affinity_partitioner`` should be "
"used, because these tailor the number of chunks based on available "
"execution resources. ``affinity_partitioner`` and ``static_partitioner`` "
"may take advantage of ``Range`` ability to split in a given ratio (see "
"\"Advanced Topic: Other Kinds of Iteration Spaces\") for distributing "
"iterations in nearly equal chunks between computing resources."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:47
msgid "``simple_partitioner`` can be useful in the following situations:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:50
msgid ""
"The subrange size for ``operator()`` must not exceed a limit. That might "
"be advantageous, for example, if your ``operator()`` needs a temporary "
"array proportional to the size of the range. With a limited subrange "
"size, you can use an automatic variable for the array instead of having "
"to use dynamic memory allocation."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:57
msgid ""
"A large subrange might use cache inefficiently. For example, suppose the "
"processing of a subrange involves repeated sweeps over the same memory "
"locations. Keeping the subrange below a limit might enable the repeatedly"
" referenced memory locations to fit in cache."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Partitioner_Summary.rst:63
msgid "You want to tune to a specific machine."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:4
msgid "Predefined Node Types"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:7
msgid ""
"You can define your own node types by inheriting from class graph_node, "
"class sender and class receiver but it is likely that you can create your"
" graph with the predefined node types already available in flow_graph.h. "
"Below is a table that lists all of the predefined types with a basic "
"description. See the Developer Reference for a more detailed description "
"of each node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:22
msgid "Predefined Node Type"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:24
msgid "input_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:25
msgid ""
"A single-output node, with a generic output type. When activated, it "
"executes a user body to generate its output. Its body is invoked if "
"downstream nodes have accepted the previous generated output. Otherwise, "
"the previous output is temporarily buffered until it is accepted "
"downstream and then the body is again invoked."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:28
msgid "function_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:29
msgid ""
"A single-input single-output node that broadcasts its output to all "
"successors. Has generic input and output types. Executes a user body, and"
" has controllable concurrency level and buffering policy.   For each "
"input exactly one output is returned."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:30
msgid "continue_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:31
msgid ""
"A single-input, single-output node that broadcasts its output to    all "
"successors. It has a single input that requires 1 or more inputs   of "
"type continue_msg and has a generic output type. It executes a   user "
"body when it receives N continue_msg objects at its input. N is   equal "
"to the number of predecessors plus any additional offset   assigned at "
"construction time."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:32
msgid "multifunction_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:33
msgid ""
"A single-input multi-output node. It has a generic input type and    "
"several generic output types. It executes a user body, and has   "
"controllable concurrency level and buffering policy. The body can   "
"output zero or more messages on each output port."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:34
msgid "broadcast_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:35
msgid ""
"A single-input, single-output node that broadcasts each message    "
"received to all successors. Its input and output are of the same   "
"generic type. It does not buffer messages."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:36
msgid "buffer_node, queue_node, priority_queue_node, and sequencer_node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:37
msgid ""
"Single-input, single-output nodes that buffer messages and send    their "
"output to one successor. The order in which the messages are   sent are "
"node specific (see the Developer Reference). These nodes are   unique in "
"that they send to only a single successor and not all   successors."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:38
msgid "join_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:39
msgid ""
"A multi-input, single-output node. There are several generic    input "
"types and the output type is a tuple of these generic types.   The node "
"combines one message from each input port to create a tuple   that is "
"broadcast to all successors. The policy used to combine   messages is "
"selectable as queueing, reserving or tag-matching."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:40
msgid "split_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:41
msgid ""
"A single-input, multi-output node. The input type is a tuple of    "
"generic types and there is one output port for each of the types in   the"
" tuple. The node receives a tuple of values and outputs each   element of"
" the tuple on a corresponding output port."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:42
msgid "write_once_node, overwrite_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:43
msgid ""
"Single-input, single-output nodes that buffer a single message    and "
"broadcast their outputs to all successors. After broadcast, the   nodes "
"retain the last message received, so it is available to any   future "
"successor. A write_once_node will only accept the first   message it "
"receives, while the overwrite_node will accept all   messages, "
"broadcasting them to all successors, and replacing the old   value with "
"the new."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:44
msgid "limiter_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:45
msgid ""
"A multi-input, single output node that broadcasts its output to    all "
"successors. The main input type and output type are of the same   generic"
" type. The node increments an internal counter when it   broadcasts a "
"message. If the increment causes it to reach its   user-assigned "
"threshold, it will broadcast no more messages. A   special input port can"
" be used to adjust the internal count, allowing   further messages to be "
"broadcast. The node does not buffer messages."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:46
msgid "indexer_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:47
msgid ""
"A multi-input, single-output node that broadcasts its output    message "
"to all of its successors. The input type is a list of generic   types and"
" the output type is a tagged_msg. The message is of one of   the types "
"listed in the input and the tag identifies the port on   which the "
"message was received. Messages are broadcast individually   as they "
"arrive at the input ports."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:48
msgid "composite_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:49
msgid ""
"A node that might have 0, 1 or multiple ports for both input and    "
"output. The composite_node packages a group of other nodes together   and"
" maintains a tuple of references to ports that border it. This   allows "
"for the corresponding ports of the composite_node to be used   to make "
"edges which hitherto would have been made from the actual   nodes in the "
"composite_node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:50
msgid "async_node (preview feature)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Predefined_Node_Types.rst:51
msgid ""
"A node that allows a flow graph to communicate with an external    "
"activity managed by the user or another runtime. This node receives   "
"messages of generic type, invokes the user-provided body to submit a   "
"message to an external activity. The external activity can use a   "
"special interface to return a generic type and put it to all   successors"
" of async_node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Reader_Writer_Mutexes.rst:4
msgid "Reader Writer Mutexes"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Reader_Writer_Mutexes.rst:7
msgid ""
"Mutual exclusion is necessary when at least one thread *writes* to a "
"shared variable. But it does no harm to permit multiple readers into a "
"protected region. The reader-writer variants of the mutexes, denoted by "
"``_rw_`` in the class names, enable multiple readers by distinguishing "
"*reader locks* from *writer locks.* There can be more than one reader "
"lock on a given mutex."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Reader_Writer_Mutexes.rst:15
msgid ""
"Requests for a reader lock are distinguished from requests for a writer "
"lock via an extra boolean parameter in the constructor for "
"``scoped_lock``. The parameter is false to request a reader lock and true"
" to request a writer lock. It defaults to ``true`` so that when omitted, "
"a ``spin_rw_mutex`` or ``queuing_rw_mutex`` behaves like its non-``_rw_``"
" counterpart."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/References.rst:4
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst
msgid "References"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/References.rst:7
msgid ""
"**[1]**Â Â  \"Memory Consistency & .NET\", Arch D. Robison, Dr. Dobbâs "
"Journal, April 2003."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/References.rst:11
msgid ""
"**[2]**Â Â  A Formal Specification of IntelÂ® ItaniumÂ® Processor Family "
"Memory Ordering, Intel Corporation, October 2002."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/References.rst:15
msgid ""
"**[3]**Â Â  \"Cilk: An Efficient Multithreaded Runtime System\", Robert "
"Blumofe, Christopher Joerg, Bradley Kuszmaul, C. Leiserson, and Keith "
"Randall, Proceedings of the fifth ACM SIGPLAN symposium on Principles and"
" practice of parallel programming, 1995."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Scalable_Memory_Allocator.rst:4
msgid "Scalable Memory Allocator"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Scalable_Memory_Allocator.rst:7
msgid ""
"Both the debug and release versions of |full_name| consists of two "
"dynamic shared libraries, one with general support and the other with a "
"scalable memory allocator. The latter is distinguished by ``malloc`` in "
"its name. For example, the release versions for Windows\\* OS are "
"``tbb<version>.dll`` and ``tbbmalloc.dll`` respectively. Applications may"
" choose to use only the general library, or only the scalable memory "
"allocator, or both. See the links below for more information on memory "
"allocation."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Summary_of_Containers.rst:4
msgid "Summary of Containers"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Summary_of_Containers.rst:7
msgid ""
"The high-level containers in |full_name| enable common idioms for "
"concurrent access. They are suitable for scenarios where the alternative "
"would be a serial container with a lock around it."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Summary_of_Loops_and_Pipelines.rst:4
msgid "Summary of Loops and Pipelines"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Summary_of_Loops_and_Pipelines.rst:6
msgid ""
"The high-level loop and pipeline templates in |full_name| give you "
"efficient scalable ways to exploit the power of multi-core chips without "
"having to start from scratch. They let you design your software at a high"
" task-pattern level and not worry about low-level manipulation of "
"threads. Because they are generic, you can customize them to your "
"specific needs. Have fun using these templates to unlock the power of "
"multi-core."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:4
msgid "Task-Based Programming"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:7
msgid ""
"When striving for performance, programming in terms of threads can be a "
"poor way to do multithreaded programming. It is much better to formulate "
"your program in terms of *logical tasks*, not threads, for several "
"reasons."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:13
msgid "Matching parallelism to available resources"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:16
msgid "Faster task startup and shutdown"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:19
msgid "More efficient evaluation order"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:22
msgid "Improved load balancing"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:25
msgid "Higherâlevel thinking"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:28
msgid "The following paragraphs explain these points in detail."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:31
msgid ""
"The threads you create with a threading package are *logical* threads, "
"which map onto the *physical threads* of the hardware. For computations "
"that do not wait on external devices, highest efficiency usually occurs "
"when there is exactly one running logical thread per physical thread. "
"Otherwise, there can be inefficiencies from the mismatch\\ *. "
"Undersubscription* occurs when there are not enough running logical "
"threads to keep the physical threads working. *Oversubscription* occurs "
"when there are more running logical threads than physical threads. "
"Oversubscription usually leads to *time sliced* execution of logical "
"threads, which incurs overheads as discussed in Appendix A, *Costs of "
"Time Slicing*. The scheduler tries to avoid oversubscription, by having "
"one logical thread per physical thread, and mapping tasks to logical "
"threads, in a way that tolerates interference by other threads from the "
"same or other processes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:47
msgid ""
"The key advantage of tasks versus logical threads is that tasks are much "
"*lighter weight* than logical threads. On Linux systems, starting and "
"terminating a task is about 18 times faster than starting and terminating"
" a thread. On Windows systems, the ratio is more than 100. This is "
"because a thread has its own copy of a lot of resources, such as register"
" state and a stack. On Linux, a thread even has its own process id. A "
"task in |full_name|, in contrast, is typically a small routine, and also,"
" cannot be preempted at the task level (though its logical thread can be "
"preempted)."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:58
msgid ""
"Tasks in oneTBB are efficient too because *the scheduler is unfair*. "
"Thread schedulers typically distribute time slices in a round-robin "
"fashion. This distribution is called \"fair\", because each logical "
"thread gets its fair share of time. Thread schedulers are typically fair "
"because it is the safest strategy to undertake without understanding the "
"higher-level organization of a program. In task-based programming, the "
"task scheduler does have some higher-level information, and so can "
"sacrifice fairness for efficiency. Indeed, it often delays starting a "
"task until it can make useful progress."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:69
msgid ""
"The scheduler does *load balancing*. In addition to using the right "
"number of threads, it is important to distribute work evenly across those"
" threads. As long as you break your program into enough small tasks, the "
"scheduler usually does a good job of assigning tasks to threads to "
"balance load. With thread-based programming, you are often stuck dealing "
"with load-balancing yourself, which can be tricky to get right."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:79
msgid ""
"Design your programs to try to create many more tasks than there are "
"threads, and let the task scheduler choose the mapping from tasks to "
"threads."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task-Based_Programming.rst:84
msgid ""
"Finally, the main advantage of using tasks instead of threads is that "
"they let you think at a higher, task-based, level. With thread-based "
"programming, you are forced to think at the low level of physical threads"
" to get good efficiency, because you have one logical thread per physical"
" thread to avoid undersubscription or oversubscription. You also have to "
"deal with the relatively coarse grain of threads. With tasks, you can "
"concentrate on the logical dependences between tasks, and leave the "
"efficient scheduling to the scheduler."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task_Scheduler_Summary.rst:4
msgid "Task Scheduler Summary"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Task_Scheduler_Summary.rst:7
msgid ""
"The task scheduler works most efficiently for fork-join parallelism with "
"lots of forks, so that the task-stealing can cause sufficient breadth-"
"first behavior to occupy threads, which then conduct themselves in a "
"depth-first manner until they need to steal more work."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/The_Task_Scheduler.rst:4
msgid "The Task Scheduler"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/The_Task_Scheduler.rst:7
msgid ""
"This section introduces the |full_name| *task scheduler*. The task "
"scheduler is the engine that powers the loop templates. When practical, "
"use the loop templates instead of the task scheduler, because the "
"templates hide the complexity of the scheduler. However, if you have an "
"algorithm that does not naturally map onto one of the high-level "
"templates, use the task scheduler."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Throughput_of_pipeline.rst:4
msgid "Throughput of pipeline"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Throughput_of_pipeline.rst:7
msgid ""
"The throughput of a pipeline is the rate at which tokens flow through it,"
" and is limited by two constraints. First, if a pipeline is run with "
"``N`` tokens, then obviously there cannot be more than ``N`` operations "
"running in parallel. Selecting the right value of ``N`` may involve some "
"experimentation. Too low a value limits parallelism; too high a value may"
" demand too many resources (for example, more buffers). Second, the "
"throughput of a pipeline is limited by the throughput of the slowest "
"sequential filter. This is true even for a pipeline with no parallel "
"filters. No matter how fast the other filters are, the slowest sequential"
" filter is the bottleneck. So in general you should try to keep the "
"sequential filters fast, and when possible, shift work to the parallel "
"filters."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Throughput_of_pipeline.rst:21
msgid ""
"The text processing example has relatively poor speedup, because the "
"serial filters are limited by the I/O speed of the system. Indeed, even "
"with files that are on a local disk, you are unlikely to see a speedup "
"much more than 2. To really benefit from a pipeline, the parallel filters"
" need to be doing some heavy lifting compared to the serial filters."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Throughput_of_pipeline.rst:29
msgid ""
"The window size, or sub-problem size for each token, can also limit "
"throughput. Making windows too small may cause overheads to dominate the "
"useful work. Making windows too large may cause them to spill out of "
"cache. A good guideline is to try for a large window size that still fits"
" in cache. You may have to experiment a bit to find a good window size."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Timing.rst:4
msgid "Timing"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Timing.rst:7
msgid ""
"When measuring the performance of parallel programs, it is usually *wall "
"clock* time, not CPU time, that matters. The reason is that better "
"parallelization typically increases aggregate CPU time by employing more "
"CPUs. The goal of parallelizing a program is usually to make it run "
"*faster* in real time."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Timing.rst:14
msgid ""
"The class ``tick_count`` in |full_name| provides a simple interface for "
"measuring wall clock time. A ``tick_count`` value obtained from the "
"static method tick_count::now() represents the current absolute time. "
"Subtracting two ``tick_count`` values yields a relative time in "
"``tick_count::interval_t``, which you can convert to seconds, as in the "
"following example:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Timing.rst:33
msgid ""
"Unlike some timing interfaces, ``tick_count`` is guaranteed to be safe to"
" use across threads. It is valid to subtract ``tick_count`` values that "
"were created by different threads. A ``tick_count`` difference can be "
"converted to seconds."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Timing.rst:39
msgid ""
"The resolution of ``tick_count`` corresponds to the highest resolution "
"timing service on the platform that is valid across threads in the same "
"process. Since the CPU timer registers are *not* valid across threads on "
"some platforms, this means that the resolution of tick_count can not be "
"guaranteed to be consistent across platforms."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Timing.rst:48
msgid ""
"On Linux\\* OS, you may need to add -lrt to the linker command when you "
"use oneapi::tbb::tick_count class. For more information, see "
"`http://fedoraproject.org/wiki/Features/ChangeInImplicitDSOLinking "
"<http://fedoraproject.org/wiki/Features/ChangeInImplicitDSOLinking>`_."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/UpgradeDowngrade.rst:4
msgid "Upgrade/Downgrade"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/UpgradeDowngrade.rst:7
msgid ""
"It is possible to upgrade a reader lock to a writer lock, by using the "
"method ``upgrade_to_writer``. Here is an example."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/UpgradeDowngrade.rst:33
msgid ""
"Note that the vector must sometimes be searched again. This is necessary "
"because ``upgrade_to_writer`` might have to temporarily release the lock "
"before it can upgrade. Otherwise, deadlock might ensue, as discussed in "
"**Lock Pathologies**. Method ``upgrade_to_writer`` returns a ``bool`` "
"that is true if it successfully upgraded the lock without releasing it, "
"and false if the lock was released temporarily. Thus when "
"``upgrade_to_writer`` returns false, the code must rerun the search to "
"check that the key was not inserted by another writer. The example "
"presumes that keys are always added to the end of the vector, and that "
"keys are never removed. Because of these assumptions, it does not have to"
" re-search the entire vector, but only the elements beyond those "
"originally searched. The key point to remember is that when "
"``upgrade_to_writer`` returns false, any assumptions established while "
"holding a reader lock may have been invalidated, and must be rechecked."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/UpgradeDowngrade.rst:49
msgid ""
"For symmetry, there is a corresponding method ``downgrade_to_reader``, "
"though in practice there are few reasons to use it."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Using_Circular_Buffers.rst:4
msgid "Using Circular Buffers"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Using_Circular_Buffers.rst:7
msgid ""
"Circular buffers can sometimes be used to minimize the overhead of "
"allocating and freeing the items passed between pipeline filters. If the "
"first filter to create an item and last filter to consume an item are "
"both ``serial_in_order``, the items can be allocated and freed via a "
"circular buffer of size at least ``ntoken``, where ``ntoken`` is the "
"first parameter to ``parallel_pipeline``. Under these conditions, no "
"checking of whether an item is still in use is necessary."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Using_Circular_Buffers.rst:16
msgid ""
"The reason this works is that at most ``ntoken`` items can be in flight, "
"and items will be freed in the order that they were allocated. Hence by "
"the time the circular buffer wraps around to reallocate an item, the item"
" must have been freed from its previous use in the pipeline. If the first"
" and last filter are *not* ``serial_in_order``, then you have to keep "
"track of which buffers are currently in use, because buffers might not be"
" retired in the same order they were allocated."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Not_to_Use_Queues.rst:4
msgid "When Not to Use Queues"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Not_to_Use_Queues.rst:7
msgid ""
"Queues are widely used in parallel programs to buffer consumers from "
"producers. Before using an explicit queue, however, consider using "
"``parallel_for_each`` ``parallel_pipeline`` instead. These is often more "
"efficient than queues for the following reasons:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Not_to_Use_Queues.rst:13
msgid ""
"A queue is inherently a bottle neck, because it must maintain first-in "
"first-out order."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Not_to_Use_Queues.rst:17
msgid ""
"A thread that is popping a value may have to wait idly until the value is"
" pushed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Not_to_Use_Queues.rst:21
msgid ""
"A queue is a passive data structure. If a thread pushes a value, it could"
" take time until it pops the value, and in the meantime the value (and "
"whatever it references) becomes \"cold\" in cache. Or worse yet, another "
"thread pops the value, and the value (and whatever it references) must be"
" moved to the other processor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Not_to_Use_Queues.rst:28
msgid ""
"In contrast, ``parallel_pipeline`` avoids these bottlenecks. Because its "
"threading is implicit, it optimizes use of worker threads so that they do"
" other work until a value shows up. It also tries to keep items hot in "
"cache."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Task-Based_Programming_Is_Inappropriate.rst:4
msgid "When Task-Based Programming Is Inappropriate"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/When_Task-Based_Programming_Is_Inappropriate.rst:7
msgid ""
"Using the task scheduler is usually the best approach to threading for "
"performance, however there are cases when the task scheduler is not "
"appropriate. The task scheduler is intended for high-performance "
"algorithms composed from non-blocking tasks. It still works if the tasks "
"rarely block. However, if threads block frequently, there is a "
"performance loss when using the task scheduler because while the thread "
"is blocked, it is not working on any tasks. Blocking typically occurs "
"while waiting for I/O or mutexes for long periods. If threads hold "
"mutexes for long periods, your code is not likely to perform well anyway,"
" no matter how many threads it has. If you have blocking tasks, it is "
"best to use full-blown threads for those. The task scheduler is designed "
"so that you can safely mix your own threads with |full_name| tasks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:4
msgid "Which Dynamic Libraries to Use"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:7
msgid ""
"The template ``scalable_allocator<T>`` requires the |full_name| scalable "
"memory allocator library as described in **Scalable Memory Allocator**. "
"It does not require the oneTBB general library, and can be used "
"independently of the rest of oneTBB."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:14
msgid ""
"The templates ``tbb_allocator<T>`` and ``cache_aligned_allocator<T>`` use"
" the scalable allocator library if it is present otherwise it reverts to "
"using ``malloc`` and ``free``. Thus, you can use these templates even in "
"applications that choose to omit the scalable memory allocator library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:21
msgid ""
"The rest of |full_name| can be used with or without the oneTBB scalable "
"memory allocator library."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:31
msgid "Template"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:32
msgid "Requirements"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:33
msgid "Notes"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:34
msgid "\\ ``scalable_allocator<T>``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:35
msgid ""
"|full_name| scalable    memory allocator library. See **Scalable Memory "
"Allocator**."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:37
msgid "\\ ``tbb_allocator<T>``           \\ ``cache_aligned_allocator<T>``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Which_Dynamic_Libraries_to_Use.rst:39
msgid ""
"Uses the scalable allocator library if it is present,    otherwise it "
"reverts to using ``malloc`` and ``free``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:4
msgid "Windows\\* OS C/C++ Dynamic Memory Interface Replacement"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:7
msgid ""
"Release version of the proxy library is ``tbbmalloc_proxy.dll``, debug "
"version is ``tbbmalloc_proxy_debug.dll``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:14
msgid ""
"Standard C library functions: ``malloc``, ``calloc``, ``realloc``, "
"``free``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:21
msgid ""
"Microsoft\\* C run-time library functions: ``_msize``, "
"``_aligned_malloc``, ``_aligned_realloc``, ``_aligned_free``, "
"``_aligned_msize``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:27
msgid ""
"Replacement of memory allocation functions is not supported for Universal"
" Windows Platform applications."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:31
msgid "To do the replacement use one of the following methods:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:34
msgid ""
"Add the following header to a source code of any binary which is loaded "
"during application startup."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:44
msgid ""
"Alternatively, add the following parameters to the linker options for the"
" .exe or .dll file that is loaded during application startup."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:48
msgid "For 32-bit code (note the triple underscore):"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:57
msgid "For 64-bit code (note the double underscore):"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:66
msgid ""
"The OS program loader must be able to find the proxy library and the "
"scalable memory allocator library at program load time. For that you may "
"include the directory containing the libraries in the ``PATH`` "
"environment variable."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:72
msgid ""
"The replacement uses in-memory binary instrumentation of Visual C++\\* "
"runtime libraries. To ensure correctness, it must first recognize a "
"subset of dynamic memory functions in these libraries. If a problem "
"occurs, the replacement is skipped, and the program continues to use the "
"standard memory allocation functions. You can use the "
"``TBB_malloc_replacement_log`` function to check if the replacement has "
"succeeded and to get additional information."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_C_Dynamic_Memory_Interface_Replacement.rst:80
msgid ""
"Set the ``TBB_MALLOC_DISABLE_REPLACEMENT`` environment variable to 1 to "
"disable replacement for a specific program invocation. In this case, the "
"program will use standard dynamic memory allocation functions. Note that "
"the oneTBB memory allocation libraries are still required for the program"
" to start even if their usage is disabled."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:4
msgid "Windows\\*"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:6
msgid ""
"This section uses <*tbb_install_dir*> to indicate the top-level "
"installation directory. The following table describes the subdirectory "
"structure for Windows\\*, relative to <*tbb_install_dir*>."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst
msgid "``include\\oneapi\\tbb.h``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst
msgid "``include\\oneapi\\tbb\\*.h``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:22
msgid "``INCLUDE``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:23
msgid ".lib files"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:24
msgid "``lib\\<arch>\\vc<vcversion>\\<lib><variant><version>.lib``\\"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:25
msgid "``LIB``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:26
msgid ".dll files"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:27
msgid "``redist\\<arch>\\vc<vcversion>\\<lib><variant><version>.dll``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:28
msgid "``PATH``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:29
msgid ".pdb files"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:30
msgid "Same as corresponding ``.dll`` file."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:31
msgid "\\"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:37
msgid "``<lib>`` - ``tbb``, ``tbbmalloc``, ``tbbmalloc_proxy`` or ``tbbbind``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:39
msgid "``<vcversion>``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:41
msgid "``14`` - use for dynamic linkage  with the CRT"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:43
msgid "``14_uwp`` - use for Windows 10 Universal Windows applications"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:45
msgid "``14_uwd`` - use for Universal Windows Drivers"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:47
msgid "``_mt`` - use for static linkage with the CRT"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:51
msgid "``<version>`` - binary version"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:53
msgid ""
"The last column shows which environment variables are used by the "
"Microsoft\\* Visual C++\\* or IntelÂ® C++ Compiler Classic or IntelÂ® "
"oneAPI DPC++/C++ Compiler to find these subdirectories."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:58
msgid ""
"Ensure that the relevant product directories are mentioned by the "
"environment variables; otherwise the compiler might not find the required"
" files."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Windows_OS_ug.rst:64
msgid ""
"Microsoft\\* C/C++ run-time libraries come in static and dynamic forms. "
"Either can be used with oneTBB. Linking to the oneTBB library is always "
"dynamic."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:4
msgid "Working on the Assembly Line: parallel_pipeline"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:7
msgid ""
"*Pipelining* is a common parallel pattern that mimics a traditional "
"manufacturing assembly line. Data flows through a series of pipeline "
"filters and each filter processes the data in some way. Given an incoming"
" stream of data, some of these filters can operate in parallel, and "
"others cannot. For example, in video processing, some operations on "
"frames do not depend on other frames, and so can be done on multiple "
"frames at the same time. On the other hand, some operations on frames "
"require processing prior frames first."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:17
msgid ""
"The |full_name| classes ``parallel_pipeline`` and filter implement the "
"pipeline pattern. A simple text processing example will be used to "
"demonstrate the usage of ``parallel_pipeline`` and filter to perform "
"parallel formatting. The example reads a text file, squares each decimal "
"numeral in the text, and writes the modified text to a new file. Below is"
" a picture of the pipeline."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:27
msgid ""
"Since the body object provided to the filters of the ``parallel_pipline``"
" might be copied, its ``operator()`` should not modify the body. "
"Otherwise the modification might or might not become visible to the "
"thread that invoked ``parallel_pipeline``, depending upon whether "
"``operator()`` is acting on the original or a copy. As a reminder of this"
" nuance, ``parallel_pipeline`` requires that the body object's "
"``operator()`` be declared ``const``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:42
msgid "Read chunk from input file"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:44
msgid "Square numerals in chunk"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:46
msgid "Write chunk to output file"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:51
msgid ""
"Assume that the raw file I/O is sequential. The squaring filter can be "
"done in parallel. That is, if you can serially read ``n`` chunks very "
"quickly, you can transform each of the ``n`` chunks in parallel, as long "
"as they are written in the proper order to the output file. Though the "
"raw I/O is sequential, the formatting of input and output can be moved to"
" the middle filter, and thus be parallel."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:59
msgid ""
"To amortize parallel scheduling overheads, the filters operate on chunks "
"of text. Each input chunk is approximately 4000 characters. Each chunk is"
" represented by an instance of class ``TextSlice``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:106
msgid ""
"Below is the top-level code for building and running the pipeline. "
"``TextSlice`` objects are passed between filters using pointers to avoid "
"the overhead of copying a ``TextSlice``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:128
msgid ""
"The parameter ``ntoken`` to method ``parallel_pipeline`` controls the "
"level of parallelism. Conceptually, tokens flow through the pipeline. In "
"a serial in-order filter, each token must be processed serially in order."
" In a parallel filter, multiple tokens can by processed in parallel by "
"the filter. If the number of tokens were unlimited, there might be a "
"problem where the unordered filter in the middle keeps gaining tokens "
"because the output filter cannot keep up. This situation typically leads "
"to undesirable resource consumption by the middle filter. The parameter "
"to method ``parallel_pipeline`` specifies the maximum number of tokens "
"that can be in flight. Once this limit is reached, the pipeline never "
"creates a new token at the input filter until another token is destroyed "
"at the output filter."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:142
msgid ""
"The second parameter specifies the sequence of filters. Each filter is "
"constructed by function ``make_filter<inputType, "
"outputType>(mode,functor)``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:146
msgid ""
"The *inputType* specifies the type of values input by a filter. For the "
"input filter, the type is ``void``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:150
msgid ""
"The *outputType* specifies the type of values output by a filter. For the"
" output filter, the type is ``void``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:154
msgid ""
"The *mode* specifies whether the filter processes items in parallel, "
"serial in-order, or serial out-of-order."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:158
msgid ""
"The *functor* specifies how to produce an output value from an input "
"value."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:162
msgid ""
"The filters are concatenated with ``operator&``. When concatenating two "
"filters, the *outputType* of the first filter must match the *inputType* "
"of the second filter."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:167
msgid ""
"The filters can be constructed and concatenated ahead of time. An "
"equivalent version of the previous example that does this follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:186
msgid ""
"The input filter must be ``serial_in_order`` in this example because the "
"filter reads chunks from a sequential file and the output filter must "
"write the chunks in the same order. All ``serial_in_order`` filters "
"process items in the same order. Thus if an item arrives at "
"``MyOutputFunc`` out of the order established by ``MyInputFunc``, the "
"pipeline automatically delays invoking ``MyOutputFunc::operator()`` on "
"the item until its predecessors are processed. There is another kind of "
"serial filter, ``serial_out_of_order``, that does not preserve order."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:196
msgid ""
"The middle filter operates on purely local data. Thus any number of "
"invocations of its functor can run concurrently. Hence it is specified as"
" a parallel filter."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:201
msgid ""
"The functors for each filter are explained in detail now. The output "
"functor is the simplest. All it has to do is write a ``TextSlice`` to a "
"file and free the ``TextSlice``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:234
msgid ""
"Method ``operator()`` processes a ``TextSlice``. The parameter ``out`` "
"points to the ``TextSlice`` to be processed. Since it is used for the "
"last filter of the pipeline, it returns ``void``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:239
msgid ""
"The functor for the middle filter is similar, but a bit more complex. It "
"returns a pointer to the ``TextSlice`` that it produces."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:278
msgid ""
"The input functor is the most complicated, because it has to ensure that "
"no numeral crosses a boundary. When it finds what could be a numeral "
"crossing into the next slice, it copies the partial numeral to the next "
"slice. Furthermore, it has to indicate when the end of input is reached. "
"It does this by invoking method ``stop()`` on a special argument of type "
"``flow_control``. This idiom is required for any functor used for the "
"first filter of a pipline."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Working_on_the_Assembly_Line_pipeline.rst:340
msgid ""
"The copy constructor must be defined because the functor is copied when "
"the underlying ``oneapi::tbb::filter_t`` is built from the functor, and "
"again when the pipeline runs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/always_use_wait_for_all.rst:4
msgid "Always Use wait_for_all()"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/always_use_wait_for_all.rst:7
msgid ""
"One of the most common mistakes made in flow graph programming is to "
"forget to call wait_for_all. The function graph::wait_for_all blocks "
"until all tasks spawned by the graph are complete. This is not only "
"useful when you want to wait until the computation is done, but it is "
"necessary to call wait_for_all before destroying the graph, or any of its"
" nodes. For example, the following function will lead to a program "
"failure:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/always_use_wait_for_all.rst:32
msgid ""
"In the function above, the graph g and its node f are destroyed at the "
"end of the function's scope. However, the task spawned to execute f's "
"body is still in flight. When the task completes, it will look for any "
"successors connected to its node, but by then both the graph and the node"
" have been deleted out from underneath it. Placing a g.wait_for_all() at "
"the end of the function prevents the premature destruction of the graph "
"and node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/always_use_wait_for_all.rst:41
msgid ""
"If you use a flow graph and see mysterious behavior, check first to see "
"that you have called wait_for_all."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_A.rst:4
msgid "Appendix A Costs of Time Slicing"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_A.rst:7
msgid ""
"Time slicing enables there to be more logical threads than physical "
"threads. Each logical thread is serviced for a *time slice* by a physical"
" thread. If a thread runs longer than a time slice, as most do, it "
"relinquishes the physical thread until it gets another turn. This "
"appendix details the costs incurred by time slicing."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_A.rst:14
msgid ""
"The most obvious is the time for *context switching* between logical "
"threads. Each context switch requires that the processor save all its "
"registers for the previous logical thread that it was executing, and load"
" its registers for the next logical thread that it runs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_A.rst:20
msgid ""
"A more subtle cost is *cache cooling*. Processors keep recently accessed "
"data in cache memory, which is very fast, but also relatively small "
"compared to main memory. When the processor runs out of cache memory, it "
"has to evict items from cache and put them back into main memory. "
"Typically, it chooses the least recently used items in the cache. (The "
"reality of set-associative caches is a bit more complicated, but this is "
"not a cache primer.) When a logical thread gets its time slice, as it "
"references a piece of data for the first time, this data will be pulled "
"into cache, taking hundreds of cycles. If it is referenced frequently "
"enough to not be evicted, each subsequent reference will find it in "
"cache, and only take a few cycles. Such data is called \"hot in cache\". "
"Time slicing undoes this, because if a thread A finishes its time slice, "
"and subsequently thread B runs on the same physical thread, B will tend "
"to evict data that was hot in cache for A, unless both threads need the "
"data. When thread A gets its next time slice, it will need to reload "
"evicted data, at the cost of hundreds of cycles for each cache miss. Or "
"worse yet, the next time slice for thread A may be on a different "
"physical thread that has a different cache altogether."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_A.rst:40
msgid ""
"Another cost is *lock preemption.* This happens if a thread acquires a "
"lock on a resource, and its time slice runs out before it releases the "
"lock. No matter how short a time the thread intended to hold the lock, it"
" is now going to hold it for at least as long as it takes for its next "
"turn at a time slice to come up. Any other threads waiting on the lock "
"either pointlessly busy-wait, or lose the rest of their time slice. The "
"effect is called *convoying*, because the threads end up \"bumper to "
"bumper\" waiting for the preempted thread in front to resume driving."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_B.rst:4
msgid "Appendix B Mixing With Other Threading Packages"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_B.rst:7
msgid ""
"|full_name| can be mixed with other threading packages. No special effort"
" is required to use any part of oneTBB with other threading packages."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_B.rst:12
msgid ""
"Here is an example that parallelizes an outer loop with OpenMP and an "
"inner loop with oneTBB."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_B.rst:38
msgid ""
"The details of ``InnerBody`` are omitted for brevity. The ``#pragma omp "
"parallel`` causes the OpenMP to create a team of threads, and each thread"
" executes the block statement associated with the pragma. The ``#pragma "
"omp for`` indicates that the compiler should use the previously created "
"thread team to execute the loop in parallel."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/appendix_B.rst:45
msgid "Here is the same example written using POSIX\\* Threads."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoid_dynamic_node_removal.rst:4
msgid "Avoid Dynamic Node Removal"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoid_dynamic_node_removal.rst:7
msgid "These are the basic guidelines regarding nodes and edges:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoid_dynamic_node_removal.rst:10
msgid "Avoid dynamic node removal"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoid_dynamic_node_removal.rst:13
msgid "Adding edges and nodes is supported"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoid_dynamic_node_removal.rst:16
msgid "Removing edges is supported"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoid_dynamic_node_removal.rst:19
msgid ""
"It is possible to add new nodes and edges and to remove old edges from a "
"flow graph as nodes are actively processing messages in the graph. "
"However, removing nodes is discouraged. Destroying a graph or any of its "
"nodes while there are messages being processed in the graph can lead to "
"premature deletion of memory that will be later touched by tasks in the "
"graph causing program failure. Removal of nodes when the graph is not "
"idle may lead to intermittent failures and hard to find failures, so it "
"should be avoided."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoiding_data_races.rst:4
msgid "Avoiding Data Races"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoiding_data_races.rst:7
msgid ""
"The edges in a flow graph make explicit the dependence relationships that"
" you want the library to enforce. Similarly, the concurrency limits on "
"``function_node`` and ``multifunction_node`` objects limit the maximum "
"number of concurrent invocations that the runtime library will allow. "
"These are the limits that are enforced by the library; the library does "
"not automatically protect you from data races. You must explicitly "
"prevent data races by using these mechanisms."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoiding_data_races.rst:16
msgid ""
"For example, the follow code has a data race because there is nothing to "
"prevent concurrent accesses to the global count object referenced by node"
" f:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/avoiding_data_races.rst:53
msgid ""
"If you run the above example, it will likely calculate a global sum that "
"is a bit smaller than the expected solution due to the data race. The "
"data race could be avoided in this simple example by changing the allowed"
" concurrency in ``f`` from unlimited to 1, forcing each value to be "
"processed sequentially by ``f``. You may also note that the "
"``input_node`` also updates a global value, ``src_count``. However, since"
" an ``input_node`` always executes serially, there is no race possible."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:4
msgid "Sending to One or Multiple Successors"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:7
msgid ""
"An important characteristic of the predefined nodes is whether they push "
"their output to a single successor or broadcast to all successors. The "
"following predefined nodes push messages to a single successor:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:12
msgid "buffer_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:13
msgid "queue_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:14
msgid "priority_queue_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:15
msgid "sequencer_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:18
msgid "Other nodes push messages to all successors that will accept them."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:21
msgid ""
"The nodes that push to only a single successor are all buffer nodes. "
"Their purpose is to hold messages temporarily, until they are consumed "
"downstream. Consider the example below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:59
msgid ""
"First, function_nodes by default queue up the messages they receive at "
"their input. To make a priority_queue_node work properly with a "
"function_node, the example above constructs its function_nodes with its "
"buffer policy set to rejecting. So, f1 and f2 do not internally buffer "
"incoming messages, but instead rely on upstream buffering in the "
"priority_queue_node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:67
msgid ""
"In the above example, each message buffered by the priority_queue_node is"
" sent to either f1 or f2, but not both."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:71
msgid ""
"Let's consider the alternative behavior; that is; what if the "
"priority_queue_node broadcasts to all successors. What if some, but not "
"all, nodes accept a message? Should the message be buffered until all "
"nodes accept it, or be only delivered to the accepting subset? If the "
"node continues to buffer the message, should it eventually deliver the "
"messages in the same order to all nodes or in the current priority order "
"at the time the node accepts the next message? For example, assume a "
"priority_queue_node only contains \"9\" when a successor node, f1, "
"accepts \"9\" but another successor node, f2, rejects it. Later a value "
"\"100\" arrives and f2 is available to accept messages. Should f2 receive"
" \"9\" next or \"100\", which has a higher priority? In any case, trying "
"to ensure that all successors receive each message creates a garbage "
"collection problem and complicates reasoning. Therefore, these buffering "
"nodes push each message to only one successor. And, you can use this "
"characteristic to create useful graph structures such as the one shown in"
" the graph above, where each message will be processed in priority order,"
" by either f1 or f2."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:90
msgid ""
"But what if you really do want both f1 and f2 to receive all of the "
"values, and in priority order? You can easily create this behavior by "
"creating one priority_queue_node for each function_node, and pushing each"
" value to both queues through a broadcast_node, as shown below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/broadcast_or_send.rst:131
msgid ""
"So, when connecting a node in your graph to multiple successors, be sure "
"to understand whether the output will broadcast to all of the successors,"
" or just a single successor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancel_a_graph.rst:4
msgid "Cancel a Graph Explicitly"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancel_a_graph.rst:7
msgid ""
"To cancel a graph execution without an exception, you can create the "
"graph using an explicit task_group_context, and then call "
"cancel_group_execution() on that object. This is done in the example "
"below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancel_a_graph.rst:44
msgid ""
"When a graph execution is canceled, any node that has already started to "
"execute will execute to completion, but any node that has not started to "
"execute will not start. So in the example above, f2 will print both the "
"Begin and End message for input 1, but will not receive the input 2."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancel_a_graph.rst:50
msgid ""
"You can also get the task_group_context that a node belongs to from "
"within the node body and use it to cancel the execution of the graph it "
"belongs to:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancel_a_graph.rst:84
msgid ""
"You can get the task_group_context from a node's body even if the graph "
"was not explicitly passed one at construction time."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancelling_nested_parallelism.rst:4
msgid "Canceling Nested Parallelism"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancelling_nested_parallelism.rst:7
msgid ""
"Nested parallelism is canceled if the inner context is bound to the outer"
" context; otherwise it is not."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancelling_nested_parallelism.rst:11
msgid ""
"If the execution of a flow graph is canceled, either explicitly or due to"
" an exception, any tasks started by parallel algorithms or flow graphs "
"nested within the nodes of the canceled flow graph may or may not be "
"canceled."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/cancelling_nested_parallelism.rst:17
msgid ""
"As with all of the library's nested parallelism, you can control "
"cancellation relationships by use of explicit task_group_context objects."
" If you do not provide an explicit task_group_context to a flow graph, it"
" is created with an isolated context by default."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/catching_exceptions.rst:4
msgid "Catching Exceptions Inside the Node that Throws the Exception"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/catching_exceptions.rst:7
msgid ""
"If you catch an exception within the node's body, execution continues "
"normally, as you might expect. If an exception is thrown but is not "
"caught before it propagates beyond the node's body, the execution of all "
"of the graph's nodes are canceled and the exception is rethrown at the "
"call site of graph::wait_for_all(). Take the graph below as an example:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/catching_exceptions.rst:40
msgid ""
"In the code above, the second function_node, f2, throws an exception that"
" is not caught within the body. This will cause the execution of the "
"graph to be canceled and the exception to be rethrown at the call to "
"g.wait_for_all(). Since it is not handled there either, the program will "
"terminate. If desirable, the exception could be caught and handled within"
" the body:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/catching_exceptions.rst:62
msgid ""
"If the exception is caught and handled in the body, then there is no "
"effect on the overall execution of the graph. However, you could choose "
"instead to catch the exception at the call to wait_for_all:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/catching_exceptions.rst:77
msgid ""
"In this case, the execution of the graph is canceled. For our example, "
"this means that the input 1 never reaches f3 and that input 2 never "
"reaches either f2 or f3."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:4
msgid "Communication Between Graphs"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:7
msgid ""
"All graph nodes require a reference to a graph object as one of the "
"arguments to their constructor. It is only safe to construct edges "
"between nodes that are part of the same graph. An edge expresses the "
"topology of your graph to the runtime library. Connecting two nodes in "
"different graphs can make it difficult to reason about whole graph "
"operations, such as calls to graph::wait_for_all and exception handling. "
"To optimize performance, the library may make calls to a node's "
"predecessor or successor at times that are unexpected by the user."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:17
msgid ""
"If two graphs must communicate, do NOT create an edge between them, but "
"instead use explicit calls to try_put. This will prevent the runtime "
"library from making any assumptions about the relationship of the two "
"nodes, and therefore make it easier to reason about events that cross the"
" graph boundaries. However, it may still be difficult to reason about "
"whole graph operations. For example, consider the graphs below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:70
msgid ""
"In the example above, m1.try_put(1) sends a message to node m1, which "
"runs its body and then sends a message to node m2. Next, node m2 runs its"
" body and sends a message to n1 using an explicit try_put. In turn, n1 "
"runs its body and sends a message to n2. The runtime library does not "
"consider m2 to be a predecessor of n1 since no edge exists."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:77
msgid ""
"If you want to wait until all of the tasks spawned by these graphs are "
"done, you need to call the function wait_for_all on both graphs. However,"
" because there is cross-graph communication, the order of the calls is "
"important. In the (incorrect) code segment above, the first call to "
"g.wait_for_all() returns immediately because there are no tasks yet "
"active in g; the only tasks that have been spawned by then belong to g2. "
"The call to g2.wait_for_all returns after both m1 and m2 are done, since "
"they belong to g2; the call does not however wait for n1 and n2, since "
"they belong to g. The end of this code segment is therefore reached "
"before n1 and n2 are done."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:89
msgid "If the calls to wait_for_all are swapped, the code works as expected:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/communicate_with_nodes.rst:102
msgid ""
"While it is not too difficult to reason about how these two very small "
"graphs interact, the interaction of two larger graphs, perhaps with "
"cycles, will be more difficult to understand. Therefore, communication "
"between nodes in different graphs should be done with caution."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_hash_map.rst:4
msgid "concurrent_hash_map"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_hash_map.rst:7
msgid ""
"A ``concurrent_hash_map<Key, T, HashCompare >`` is a hash table that "
"permits concurrent accesses. The table is a map from a key to a type "
"``T``. The traits type HashCompare defines how to hash a key and how to "
"compare two keys."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_hash_map.rst:13
msgid ""
"The following example builds a ``concurrent_hash_map`` where the keys are"
" strings and the corresponding data is the number of times each string "
"occurs in the array ``Data``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_hash_map.rst:86
msgid ""
"A ``concurrent_hash_map`` acts as a container of elements of type "
"``std::pair<const Key,T>``. Typically, when accessing a container "
"element, you are interested in either updating it or reading it. The "
"template class ``concurrent_hash_map`` supports these two purposes "
"respectively with the classes ``accessor`` and ``const_accessor`` that "
"act as smart pointers. An *accessor* represents *update* (*write*) "
"access. As long as it points to an element, all other attempts to look up"
" that key in the table block until the ``accessor`` is done. A "
"``const_accessor`` is similar, except that is represents *read-only* "
"access. Multiple ``const_accessors`` can point to the same element at the"
" same time. This feature can greatly improve concurrency in situations "
"where elements are frequently read and infrequently updated."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_hash_map.rst:100
msgid ""
"The methods ``find`` and ``insert`` take an ``accessor`` or "
"``const_accessor`` as an argument. The choice tells "
"``concurrent_hash_map`` whether you are asking for *update* or *read-"
"only* access. Once the method returns, the access lasts until the "
"``accessor`` or ``const_accessor`` is destroyed. Because having access to"
" an element can block other threads, try to shorten the lifetime of the "
"``accessor`` or ``const_accessor``. To do so, declare it in the innermost"
" block possible. To release access even sooner than the end of the block,"
" use method ``release``. The following example is a rework of the loop "
"body that uses ``release`` instead of depending upon destruction to end "
"thread lifetime:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_hash_map.rst:124
msgid ""
"The method ``remove(key)`` can also operate concurrently. It implicitly "
"requests write access. Therefore before removing the key, it waits on any"
" other extant accesses on ``key``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:4
msgid "concurrent_vector"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:7
msgid ""
"``A concurrent_vector<T>`` is a dynamically growable array of ``T``. It "
"is safe to grow a ``concurrent_vector`` while other threads are also "
"operating on elements of it, or even growing it themselves. For safe "
"concurrent growing, ``concurrent_vector`` has three methods that support "
"common uses of dynamic arrays: ``push_back``, ``grow_by``, and "
"``grow_to_at_least``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:15
msgid ""
"Method ``push_back(x)`` safely appends x to the array. Method "
"``grow_by(n)`` safely appends ``n`` consecutive elements initialized with"
" ``T()``. Both methods return an iterator pointing to the first appended "
"element. Each element is initialized with ``T()``. So for example, the "
"following routine safely appends a C string to a shared vector:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:32
msgid ""
"The related method ``grow_to_at_least(n)``\\ grows a vector to size ``n``"
" if it is shorter. Concurrent calls to the growth methods do not "
"necessarily return in the order that elements are appended to the vector."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:38
msgid ""
"Method ``size()`` returns the number of elements in the vector, which may"
" include elements that are still undergoing concurrent construction by "
"methods ``push_back``, ``grow_by,`` or ``grow_to_at_least``. The example "
"uses std::copy and iterators, not ``strcpy and pointers``, because "
"elements in a ``concurrent_vector`` might not be at consecutive "
"addresses. It is safe to use the iterators while the "
"``concurrent_vector`` is being grown, as long as the iterators never go "
"past the current value of ``end()``. However, the iterator may reference "
"an element undergoing concurrent construction. You must synchronize "
"construction and access."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:50
msgid ""
"A ``concurrent_vector<T>`` never moves an element until the array is "
"cleared, which can be an advantage over the STL std::vector even for "
"single-threaded code. However, ``concurrent_vector`` does have more "
"overhead than std::vector. Use ``concurrent_vector`` only if you really "
"need the ability to dynamically resize it while other accesses are (or "
"might be) in flight, or require that an element never move."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/concurrent_vector_ug.rst:59
msgid ""
"Operations on ``concurrent_vector`` are concurrency safe with respect to "
"*growing*, not for clearing or destroying a vector. Never invoke method "
"``clear()`` if there are other operations in flight on the "
"``concurrent_vector``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:4
msgid "Create a Token-Based System"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:7
msgid ""
"A more flexible solution to limit the number of messages in a flow graph "
"is to use tokens. In a token-based system, a limited number of tokens are"
" available in the graph and a message will not be allowed to enter the "
"graph until it can be paired with an available token. When a message is "
"retired from the graph, its token is released, and can be paired with a "
"new message that will then be allowed to enter."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:15
msgid ""
"The ``oneapi::tbb::parallel_pipeline`` algorithm relies on a token-based "
"system. In the flow graph interface, there is no explicit support for "
"tokens, but ``join_node``s can be used to create an analogous system. A "
"``join_node`` has two template arguments, the tuple that describes the "
"types of its inputs and a buffer policy:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:29
msgid "The buffer policy can be one of the following:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:32
msgid ""
"``queueing``. This type of policy causes inputs to be matched first-in-"
"first-out; that is, the inputs are joined together to form a tuple in the"
" order they are received."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:35
msgid ""
"``tag_matching``. This type of policy joins inputs together that have "
"matching tags."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:37
msgid ""
"``reserving``. This type of policy causes the ``join_node`` to do no "
"internally buffering, but instead to consume inputs only when it can "
"first reserve an input on each port from an upstream source. If it can "
"reserve an input at each port, it gets those inputs and joins those "
"together to form an output tuple."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:44
msgid "A token-based system can be created by using reserving join_nodes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:47
msgid ""
"In the example below, there is an ``input_node`` that generates ``M`` big"
" objects and a ``buffer_node`` that is pre-filled with three tokens. The "
"``token_t`` can be anything, for example it could be ``typedef int "
"token_t;``. The ``input_node`` and ``buffer_node`` are connected to a "
"reserving ``join_node``. The ``input_node`` will only generate an input "
"when one is pulled from it by the reserving ``join_node``, and the "
"reserving ``join_node`` will only pull the input from the ``input_node`` "
"when it knows there is also an item to pull from the ``buffer_node``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:109
msgid ""
"In the above code, you can see that the ``function_node`` returns the "
"token back to the ``buffer_node``. This cycle in the flow graph allows "
"the token to be recycled and paired with another input from the "
"``input_node``. So like in the previous sections, there will be at most "
"four big objects in the graph. There could be three big objects in the "
"``function_node`` and one buffered in the ``input_node``, awaiting a "
"token to be paired with."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:117
msgid ""
"Since there is no specific ``token_t`` defined for the flow graph, you "
"can use any type for a token, including objects or pointers to arrays. "
"Therefore, unlike in the example above, the ``token_t`` doesn't need to "
"be a dummy type; it could for example be a buffer or other object that is"
" essential to the computation. We could, for example, modify the example "
"above to use the big objects themselves as the tokens, removing the need "
"to repeatedly allocate and deallocate them, and essentially create a free"
" list of big objects using a cycle back to the ``buffer_node``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:127
msgid ""
"Also, in our example above, the ``buffer_node`` was prefilled by a fixed "
"number of explicit calls to ``try_put``, but there are other options. For"
" example, an ``input_node`` could be attached to the input of the "
"``buffer_node``, and it could generate the tokens. In addition, our "
"``function_node`` could be replaced by a ``multifunction_node`` that can "
"optionally put 0 or more outputs to each of its output ports. Using a "
"``multifunction_node``, you can choose to recycle or not recycle a token,"
" or even generate more tokens, thereby increasing or decreasing the "
"allowed concurrency in the graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/create_token_based_system.rst:138
msgid ""
"A token based system is therefore very flexible. You are free to declare "
"the token to be of any type and to inject or remove tokens from the "
"system as it is executing, thereby having dynamic control of the allowed "
"concurrency in the system. Since you can pair the token with an input at "
"the source, this approach enables you to limit resource consumption "
"across the entire graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:4
msgid "Agglomeration"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst
msgid "Problem"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:13
msgid ""
"Parallelism is so fine grained that overhead of parallel scheduling or "
"communication swamps the useful work."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst
msgid "Context"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:23
msgid ""
"Many algorithms permit parallelism at a very fine grain, on the order of "
"a few instructions per task. But synchronization between threads usually "
"requires orders of magnitude more cycles. For example, elementwise "
"addition of two arrays can be done fully in parallel, but if each scalar "
"addition is scheduled as a separate task, most of the time will be spent "
"doing synchronization instead of useful addition."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst
msgid "Forces"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:37
msgid ""
"Individual computations can be done in parallel, but are small. For "
"practical use of |full_name|, \"small\" here means less than 10,000 clock"
" cycles."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:42
msgid ""
"The parallelism is for sake of performance and not required for semantic "
"reasons."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst
msgid "Solution"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:52
msgid ""
"Group the computations into blocks. Evaluate computations within a block "
"serially."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:56
msgid ""
"The block size should be chosen to be large enough to amortize parallel "
"overhead. Too large a block size may limit parallelism or load balancing "
"because the number of blocks becomes too small to distribute work evenly "
"across processors."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:62
msgid "The choice of block topology is typically driven by two concerns:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:65
msgid "Minimizing synchronization between blocks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:68
msgid "Minimizing cache traffic between blocks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:71
msgid ""
"If the computations are completely independent, then the blocks will be "
"independent too, and then only cache traffic issues must be considered."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:76
msgid ""
"If the loop is \"small\", on the order of less than 10,000 clock cycles, "
"then it may be impractical to parallelize at all, because the optimal "
"agglomeration might be a single block,"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:87
msgid ""
"TBB loop templates such as ``oneapi::tbb::parallel_for`` that take a "
"*range* argument support automatic agglomeration."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:91
msgid ""
"When agglomerating, think about cache effects. Avoid having cache lines "
"cross between groups if possible."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:95
msgid ""
"There may be boundary to interior ratio effects. For example, if the "
"computations form a 2D grid, and communicate only with nearest neighbors,"
" then the computation per block grows quadratically (with the block's "
"area), but the cross-block communication grows with linearly (with the "
"block's perimeter). The following figure shows four different ways to "
"agglomerate an 8Ã8 grid. If doing such analysis, be careful to consider "
"that information is transferred in cache line units. For a given area, "
"the perimeter may be minimized when the block is square with respect to "
"the underlying grid of cache lines, not square with respect to the "
"logical grid."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:111
msgid "Four different agglomerations of an 8Ã8 grid. |image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:114
msgid ""
"Also consider vectorization. Blocks that contain long contiguous subsets "
"of data may better enable vectorization."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:118
msgid ""
"For recursive computations, most of the work is towards the leaves, so "
"the solution is to treat subtrees as a groups as shown in the following "
"figure."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:127
msgid "Agglomeration of a recursive computation |image1|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:130
msgid ""
"Often such an agglomeration is achieved by recursing serially once some "
"threshold is reached. For example, a recursive sort might solve sub-"
"problems in parallel only if they are above a certain threshold size."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst
msgid "Reference"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:142
msgid ""
"Ian Foster introduced the term \"agglomeration\" in his book Designing "
"and Building Parallel Programs http://www.mcs.anl.gov/~itf/dbpp. There "
"agglomeration is part of a four step **PCAM** design method:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:147
msgid "**P**\\ artitioning - break the program into the smallest tasks possible."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:151
msgid ""
"**C**\\ ommunication â figure out what communication is required between "
"tasks. When using oneTBB, communication is usually cache line transfers. "
"Though they are automatic, understanding which ones happen between tasks "
"helps guide the agglomeration step."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:157
msgid ""
"**A**\\ gglomeration â combine tasks into larger tasks. His book has an "
"extensive list of considerations that is worth reading."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Agglomeration.rst:161
msgid ""
"**M**\\ apping â map tasks onto processors. The oneTBB task scheduler "
"does this step for you."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:4
msgid "Design Patterns"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:7
msgid ""
"This section provides some common parallel programming patterns and how "
"to implement them in |full_name|."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:11
msgid "The description of each pattern has the following format:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:14
msgid "**Problem** â describes the problem to be solved."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:17
msgid "**Context** â describes contexts in which the problem arises."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:20
msgid "**Forces** - considerations that drive use of the pattern."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:23
msgid "**Solution** - describes how to implement the pattern."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:26
msgid "**Example** â presents an example implementation."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:29
msgid ""
"Variations and examples are sometimes discussed. The code examples are "
"intended to emphasize key points and are not full-fledged code. Examples "
"may omit obvious const overloads of non-const methods."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:34
msgid ""
"Much of the nomenclature and examples are adapted from Web pages created "
"by Eun-Gyu and Marc Snir, and the Berkeley parallel patterns wiki. See "
"links in the **General References** section."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Design_Patterns.rst:39
msgid ""
"For brevity, some of the code examples use C++11 lambda expressions. It "
"is straightforward, albeit sometimes tedious, to translate such lambda "
"expressions into equivalent C++03 code."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:4
msgid "Divide and Conquer"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:13
msgid "Parallelize a divide and conquer algorithm."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:22
msgid ""
"Divide and conquer is widely used in serial algorithms. Common examples "
"are quicksort and mergesort."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:32
msgid ""
"Problem can be transformed into subproblems that can be solved "
"independently."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:36
msgid ""
"Splitting problem or merging solutions is relatively cheap compared to "
"cost of solving the subproblems."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:46
msgid ""
"There are several ways to implement divide and conquer in |full_name|. "
"The best choice depends upon circumstances."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:50
msgid ""
"If division always yields the same number of subproblems, use recursion "
"and ``oneapi::tbb::parallel_invoke``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:54
msgid ""
"If the number of subproblems varies, use recursion and "
"``oneapi::tbb::task_group``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:64
msgid ""
"Quicksort is a classic divide-and-conquer algorithm. It divides a sorting"
" problem into two subsorts. A simple serial version looks like [1]_."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:82
msgid ""
"The number of subsorts is fixed at two, so "
"``oneapi::tbb::parallel_invoke`` provides a simple way to parallelize it."
" The parallel code is shown below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:101
msgid ""
"Eventually the subsorts become small enough that serial execution is more"
" efficient. The following variation, does sorts of less than 500 elements"
" using the earlier serial code."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:121
msgid "The change is an instance of the Agglomeration pattern."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:124
msgid ""
"The next example considers a problem where there are a variable number of"
" subproblems. The problem involves a tree-like description of a "
"mechanical assembly. There are two kinds of nodes:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:129
msgid "Leaf nodes represent individual parts."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:132
msgid "Internal nodes represent groups of parts."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:135
msgid ""
"The problem is to find all nodes that collide with a target node. The "
"following code shows a serial solution that walks the tree. It records in"
" ``Hits`` any nodes that collide with ``Target``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:158
msgid "A parallel version is shown below."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:191
msgid ""
"The recursive walk is parallelized using class ``task_group`` to do "
"recursive calls in parallel."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:195
msgid ""
"There is another significant change because of the parallelism that is "
"introduced. Because it would be unsafe to update ``Hits`` concurrently, "
"the parallel walk uses variable ``LocalHits`` to accumulate results. "
"Because it is of type ``enumerable_thread_specific``, each thread "
"accumulates its own private result. The results are spliced together into"
" Hits after the walk completes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:204
msgid "The results will *not* be in the same order as the original serial code."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:208
msgid ""
"If parallel overhead is high, use the agglomeration pattern. For example,"
" use the serial walk for subtrees under a certain threshold."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Divide_and_Conquer.rst:212
msgid ""
"Production quality quicksort implementations typically use more "
"sophisticated pivot selection, explicit stacks instead of recursion, and "
"some other sorting algorithm for small subsorts. The simple algorithm is "
"used here to focus on exposition of the parallel pattern."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:4
msgid "Elementwise"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:13
msgid ""
"Initiate similar independent computations across items in a data set, and"
" wait until all complete."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:23
msgid ""
"Many serial algorithms sweep over a set of items and do an independent "
"computation on each item. However, if some kind of summary information is"
" collected, use the Reduction pattern instead."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:34
msgid "No information is carried or merged between the computations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:43
msgid ""
"If the number of items is known in advance, use "
"``oneapi::tbb::parallel_for``. If not, consider using "
"``oneapi::tbb::parallel_for_each``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:48
msgid ""
"Use agglomeration if the individual computations are small relative to "
"scheduler overheads."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:52
msgid ""
"If the pattern is followed by a reduction on the same data, consider "
"doing the element-wise operation as part of the reduction, so that the "
"combination of the two patterns is accomplished in a single sweep instead"
" of two sweeps. Doing so may improve performance by reducing traffic "
"through the memory hierarchy."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:65
msgid ""
"Convolution is often used in signal processing. The convolution of a "
"filter ``c`` and signal ``x`` is computed as:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:69
msgid "|image0| Serial code for this computation might look like:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:85
msgid ""
"For simplicity, the fragment assumes that ``x`` is a pointer into an "
"array padded with zeros such that ``x[k]``\\ returns zero when ``k<0`` or"
" ``kâ¥xlen``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:90
msgid ""
"The inner loop does not fit the elementwise pattern, because each "
"iteration depends on the previous iteration. However, the outer loop fits"
" the elementwise pattern. It is straightforward to render it using "
"``oneapi::tbb::parallel_for`` as shown:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Elementwise.rst:107
msgid ""
"``oneapi::tbb::parallel_for`` does automatic agglomeration by implicitly "
"using ``oneapi::tbb::auto_partitioner`` in its underlying implementation."
" If there is reason to agglomerate explicitly, use the overload of "
"``oneapi::tbb::parallel_for`` that takes an explicit range argument. The "
"following shows the example transformed to use the overload."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:4
msgid "Fenced Data Transfer"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:13
msgid ""
"Write a message to memory and have another processor read it on hardware "
"that does not have a sequentially consistent memory model."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:23
msgid ""
"The problem normally arises only when unsynchronized threads concurrently"
" act on a memory location, or are using reads and writes to create "
"synchronization. High level synchronization constructs normally include "
"mechanisms that prevent unwanted reordering."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:29
msgid ""
"Modern hardware and compilers can reorder memory operations in a way that"
" preserves the order of a thread's operation from its viewpoint, but not "
"as observed by other threads. A serial common idiom is to write a message"
" and mark it as ready to ready as shown in the following code:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:56
msgid "Two key assumptions of the code are:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:59
msgid "``Ready`` does not become true until ``Message`` is written."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:62
msgid "``Message`` is not read until ``Ready`` becomes true."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:65
msgid ""
"These assumptions are trivially true on uniprocessor hardware. However, "
"they may break on multiprocessor hardware. Reordering by the hardware or "
"compiler can cause the sender's writes to appear out of order to the "
"receiver (thus breaking condition a) or the receiver's reads to appear "
"out of order (thus breaking condition b)."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:78
msgid "Creating synchronization via raw reads and writes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:87
msgid ""
"Change the flag from ``bool`` to ``std::atomic<bool>`` for the flag that "
"indicates when the message is ready. Here is the previous example with "
"modifications."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:112
msgid ""
"A write to a ``std::atomic`` value has *release* semantics, which means "
"that all of its prior writes will be seen before the releasing write. A "
"read from ``std::atomic`` value has *acquire* semantics, which means that"
" all of its subsequent reads will happen after the acquiring read. The "
"implementation of ``std::atomic`` ensures that both the compiler and the "
"hardware observe these ordering constraints."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst
msgid "Variations"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:127
msgid ""
"Higher level synchronization constructs normally include the necessary "
"*acquire* and *release* fences. For example, mutexes are normally "
"implemented such that acquisition of a lock has *acquire* semantics and "
"release of a lock has *release* semantics. Thus a thread that acquires a "
"lock on a mutex always sees any memory writes done by another thread "
"before it released a lock on that mutex."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst
msgid "Non Solutions"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:141
msgid ""
"Mistaken solutions are so often proposed that it is worth understanding "
"why they are wrong."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:145
msgid ""
"One common mistake is to assume that declaring the flag with the "
"``volatile`` keyword solves the problem. Though the ``volatile`` keyword "
"forces a write to happen immediately, it generally has no effect on the "
"visible ordering of that write with respect to other memory operations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:152
msgid ""
"Another mistake is to assume that conditionally executed code cannot "
"happen before the condition is tested. However, the compiler or hardware "
"may speculatively hoist the conditional code above the condition."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Fenced_Data_Transfer.rst:158
msgid ""
"Similarly, it is a mistake to assume that a processor cannot read the "
"target of a pointer before reading the pointer. A modern processor does "
"not read individual values from main memory. It reads cache lines. The "
"target of a pointer may be in a cache line that has already been read "
"before the pointer was read, thus giving the appearance that the "
"processor presciently read the pointer target."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:4
msgid "GUI Thread"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:12
msgid ""
"A user interface thread must remain responsive to user requests, and must"
" not get bogged down in long computations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:22
msgid ""
"Graphical user interfaces often have a dedicated thread (\"GUI thread\") "
"for servicing user interactions. The thread must remain responsive to "
"user requests even while the application has long computations running. "
"For example, the user might want to press a \"cancel\" button to stop the"
" long running computation. If the GUI thread takes part in the long "
"running computation, it will not be able to respond to user requests."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:37
msgid "The GUI thread services an event loop."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:40
msgid ""
"The GUI thread needs to offload work onto other threads without waiting "
"for the work to complete."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:44
msgid ""
"The GUI thread must be responsive to the event loop and not become "
"dedicated to doing the offloaded work."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst
msgid "Related"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:54
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:4
msgid "Non-Preemptive Priorities"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:55
#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:4
msgid "Local Serializer"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:64
msgid ""
"The GUI thread offloads the work by firing off a task to do it using "
"method ``task_arena::enqueue`` of a ``task_arena`` instance. When "
"finished, the task posts an event to the GUI thread to indicate that the "
"work is done. The semantics of ``enqueue`` cause the task to eventually "
"run on a worker thread distinct from the calling thread."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:70
msgid ""
"The following figure sketches the communication paths. Items in black are"
" executed by the GUI thread; items in blue are executed by another "
"thread."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:81
msgid ""
"The example is for the Microsoft Windows\\* operating systems, though "
"similar principles apply to any GUI using an event loop idiom. For each "
"event, the GUI thread calls a user-defined function ``WndProc`` to "
"process an event."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:135
msgid "The GUI thread processes long computations as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:138
msgid ""
"The GUI thread calls ``LongRunningWork``, which hands off the work to a "
"worker thread and immediately returns."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:142
msgid ""
"The GUI thread continues servicing the event loop. If it has to repaint "
"the window, it uses the value of\\ ``CurrentResult``, which is the most "
"recent ``Foo`` that it has seen."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:147
msgid ""
"When a worker finishes the long computation, it pushes the result into "
"ResultQueue, and sends a message WM_POP_FOO to the GUI thread."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:151
msgid ""
"The GUI thread services a ``WM_POP_FOO`` message by popping an item from "
"ResultQueue into CurrentResult. The ``try_pop`` always succeeds because "
"there is exactly one ``WM_POP_FOO`` message for each item in "
"``ResultQueue``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:157
msgid ""
"Routine ``LaunchLongRunningWork`` creates a function task and launches it"
" using method ``task_arena::enqueue``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:182
msgid ""
"It is essential to use method ``task_arena::enqueue`` here. Even though, "
"an explicit ``task_arena`` instance is created, the method ``enqueue`` "
"ensures that the function task eventually executes when resources permit,"
" even if no thread explicitly waits on the task. In contrast, "
"``oneapi::tbb::task_group::run`` may postpone execution of the function "
"task until it is explicitly waited upon with the "
"``oneapi::tbb::task_group::wait``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:188
msgid ""
"The example uses a ``concurrent_queue`` for workers to communicate "
"results back to the GUI thread. Since only the most recent result matters"
" in the example, and alternative would be to use a shared variable "
"protected by a mutex. However, doing so would block the worker while the "
"GUI thread was holding a lock on the mutex, and vice versa. Using "
"``concurrent_queue`` provides a simple robust solution."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/GUI_Thread.rst:195
msgid ""
"If two long computations are in flight, there is a chance that the first "
"computation completes after the second one. If displaying the result of "
"the most recently requested computation is important, then associate a "
"request serial number with the computation. The GUI thread can pop from "
"``ResultQueue`` into a temporary variable, check the serial number, and "
"update ``CurrentResult`` only if doing so advances the serial number."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:4
msgid "General References"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:7
msgid ""
"This section lists general references. References specific to a pattern "
"are listed at the end of the topic for the pattern."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:10
msgid "Gamma, R. Helm, R. Johnson, J. Vlissides. Design Patterns (1995)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:11
msgid ""
"`Berkeley Pattern Language for Parallel Programming "
"<https://patterns.eecs.berkeley.edu/?page_id=98>`_"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:12
msgid ""
"Mattson, B. Sanders, B. Massingill. Patterns for Parallel Programming "
"(2005)"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:13
msgid ""
"`ParaPLoP 2009 "
"<https://web.archive.org/web/20111118224546/http://www.upcrc.illinois.edu/workshops/paraplop09/program.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:14
msgid ""
"`ParaPLoP 2010 "
"<https://web.archive.org/web/20111118222933/http://www.upcrc.illinois.edu/workshops/paraplop10/program.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/General_References.rst:15
msgid ""
"Eun-Gyu Kim and Marc Snir, `Parallel Programming Patterns "
"<http://snir.cs.illinois.edu/PPP.html>`_"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:13
msgid ""
"Consider an interactive program. To maximize concurrency and "
"responsiveness, operations requested by the user can be implemented as "
"tasks. The order of operations can be important. For example, suppose the"
" program presents editable text to the user. There might be operations to"
" select text and delete selected text. Reversing the order of \"select\" "
"and \"delete\" operations on the same buffer would be bad. However, "
"commuting operations on different buffers might be okay. Hence the goal "
"is to establish serial ordering of tasks associated with a given object, "
"but not constrain ordering of tasks between different objects."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:31
msgid ""
"Operations associated with a certain object must be performed in serial "
"order."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:35
msgid ""
"Serializing with a lock would be wasteful because threads would be "
"waiting at the lock when they could be doing useful work elsewhere."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:46
msgid ""
"Sequence the work items using a FIFO (first-in first-out structure). "
"Always keep an item in flight if possible. If no item is in flight when a"
" work item appears, put the item in flight. Otherwise, push the item onto"
" the FIFO. When the current item in flight completes, pop another item "
"from the FIFO and put it in flight."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:53
msgid ""
"The logic can be implemented without mutexes, by using "
"``concurrent_queue`` for the FIFO and ``atomic<int>`` to count the number"
" of items waiting and in flight. The example explains the accounting in "
"detail."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:65
msgid ""
"The following example builds on the Non-Preemptive Priorities example to "
"implement local serialization in addition to priorities. It implements "
"three priority levels and local serializers. The user interface for it "
"follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:85
msgid ""
"Template function ``EnqueueWork`` causes functor ``f`` to run when the "
"three constraints in the following table are met."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:95
msgid "Constraint"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:96
msgid "Resolved by class..."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:97
msgid "Any prior work for the ``Serializer`` has completed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:98
msgid "\\ ``Serializer``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:99
msgid "A thread is available."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:100
msgid "\\ ``RunWorkItem``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:101
msgid "No higher priority work is ready to run."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:102
msgid "\\ ``ReadyPileType``"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:107
msgid ""
"Constraints on a given functor are resolved from top to bottom in the "
"table. The first constraint does not exist when s is NULL. The "
"implementation of ``EnqueueWork`` packages the functor in a "
"``SerializedWorkItem`` and routes it to the class that enforces the first"
" relevant constraint between pieces of work."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:127
msgid ""
"A ``SerializedWorkItem`` is derived from a ``WorkItem``, which serves as "
"a way to pass around a prioritized piece of work without knowing further "
"details of the work."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:164
msgid ""
"Base class ``WorkItem`` is the same as class WorkItem in the example for "
"Non-Preemptive Priorities. The notion of serial constraints is completely"
" hidden from the base class, thus permitting the framework to extend "
"other kinds of constraints or lack of constraints. Class "
"``SerializedWorkItem`` is essentially ``ConcreteWorkItem`` from the "
"example for Non-Preemptive Priorities, extended with a ``Serializer`` "
"aspect."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:173
msgid ""
"Virtual method ``run()`` is invoked when it becomes time to run the "
"functor. It performs three steps:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:177
msgid "Run the functor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:180
msgid "Destroy the functor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:183
msgid ""
"Notify the ``Serializer`` that the functor completed, and thus "
"unconstraining the next waiting functor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:187
msgid ""
"Step 3 is the difference from the operation of ConcreteWorkItem::run. "
"Step 2 could be done after step 3 in some contexts to increase "
"concurrency slightly. However, the presented order is recommended because"
" if step 2 takes non-trivial time, it likely has side effects that should"
" complete before the next functor runs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:194
msgid "Class ``Serializer`` implements the core of the Local Serializer pattern:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:222
msgid "The class maintains two members:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:225
msgid "A queue of WorkItem waiting for prior work to complete."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:228
msgid "A count of queued or in-flight work."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:231
msgid ""
"Mutexes are avoided by using ``concurrent_queue<WorkItem*>`` and "
"``atomic<int>`` along with careful ordering of operations. The "
"transitions of count are the key understanding how class ``Serializer`` "
"works."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:237
msgid ""
"If method ``add`` increments ``count`` from 0 to 1, this indicates that "
"no other work is in flight and thus the work should be moved to the "
"``ReadyPile``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:242
msgid ""
"If method ``noteCompletion`` decrements count and it is *not* from 1 to "
"0, then the queue is non-empty and another item in the queue should be "
"moved to ``ReadyPile``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:247
msgid ""
"Class ``ReadyPile`` is explained in the example for Non-Preemptive "
"Priorities."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:251
msgid ""
"If priorities are not necessary, there are two variations on method "
"``moveOneItemToReadyPile``, with different implications."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:255
msgid ""
"Method ``moveOneItemToReadyPile`` could directly invoke\\ "
"``item->run()``. This approach has relatively low overhead and high "
"thread locality for a given ``Serializer``. But it is unfair. If the "
"``Serializer`` has a continual stream of tasks, the thread operating on "
"it will keep servicing those tasks to the exclusion of others."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:263
msgid ""
"Method ``moveOneItemToReadyPile`` could invoke ``task::enqueue`` to "
"enqueue a task that invokes ``item->run()``. Doing so introduces higher "
"overhead and less locality than the first approach, but avoids "
"starvation."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:269
msgid ""
"The conflict between fairness and maximum locality is fundamental. The "
"best resolution depends upon circumstance."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Local_Serializer.rst:273
msgid ""
"The pattern generalizes to constraints on work items more general than "
"those maintained by class Serializer. A generalized ``Serializer::add`` "
"determines if a work item is unconstrained, and if so, runs it "
"immediately. A generalized ``Serializer::noteCompletion`` runs all "
"previously constrained items that have become unconstrained by the "
"completion of the current work item. The term \"run\" means to run work "
"immediately, or if there are more constraints, forwarding the work to the"
" next constraint resolver."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:13
msgid "Choose the next work item to do, based on priorities."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:22
msgid ""
"The scheduler in |full_name| chooses tasks using rules based on "
"scalability concerns. The rules are based on the order in which tasks "
"were spawned or enqueued, and are oblivious to the contents of tasks. "
"However, sometimes it is best to choose work based on some kind of "
"priority relationship."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:35
msgid ""
"Given multiple work items, there is a rule for which item should be done "
"next that is *not* the default oneTBB rule."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:39
msgid ""
"Preemptive priorities are not necessary. If a higher priority item "
"appears, it is not necessary to immediately stop lower priority items in "
"flight. If preemptive priorities are necessary, then non-preemptive "
"tasking is inappropriate. Use threads instead."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:51
msgid ""
"Put the work in a shared work pile. Decouple tasks from specific work, so"
" that task execution chooses the actual piece of work to be selected from"
" the pile."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:62
msgid ""
"The following example implements three priority levels. The user "
"interface for it and top-level implementation follow:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:83
msgid ""
"The caller provides a priority ``p`` and a functor ``f`` to routine "
"``EnqueueWork``. The functor may be the result of a lambda expression. "
"``EnqueueWork`` packages ``f`` as a ``WorkItem`` and adds it to global "
"object ``ReadyPile``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:88
msgid ""
"Class ``WorkItem`` provides a uniform interface for running functors of "
"unknown type:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:118
msgid ""
"Class ``ReadyPile`` contains the core pattern. It maintains a collection "
"of work and fires off tasks through the ``oneapi::tbb::task_group::run`` "
"interface and then choose a work from the collection:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:150
msgid ""
"The task added by ``add(item)`` does *not* necessarily execute that item."
" The task itself executes ``runNextWorkItem()``, which may find a higher "
"priority item. There is one task for each item, but the mapping resolves "
"when the task actually executes, not when it is created."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:155
msgid "Here are the details of class ``RunWorkItem``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:166
msgid ""
"``RunWorkItem`` objects are fungible. They enable the oneTBB scheduler to"
" choose when to do a work item, not which work item to do."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:170
msgid ""
"Other priority schemes can be implemented by changing the internals for "
"``ReadyPileType``. A priority queue could be used to implement very fine "
"grained priorities."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Non-Preemptive_Priorities.rst:174
msgid ""
"The scalability of the pattern is limited by the scalability of "
"``ReadyPileType``. Ideally scalable concurrent containers should be used "
"for it."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:4
msgid "Odd-Even Communication"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:13
msgid ""
"Operations on data cannot be done entirely independently, but data can be"
" partitioned into two subsets such that all operations on a subset can "
"run in parallel."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:24
msgid ""
"Solvers for partial differential equations can often be modified to "
"follow this pattern. For example, for a 2D grid with only nearest-"
"neighbor communication, it may be possible to treat the grid as a "
"checkerboard, and alternate between updating red squares and black "
"squares."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:31
msgid ""
"Another context is staggered grid (\"leap frog\") Finite Difference Time "
"Domain (FDTD solvers, which naturally fit the pattern."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:41
msgid "Dependencies between items form a bipartite graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:50
msgid ""
"Alternate between updating one subset and then the other subset. Apply "
"the elementwise pattern to each subset."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Odd-Even_Communication.rst:59
msgid ""
"Eun-Gyu Kim and Mark Snir, \"Odd-Even Communication Group\", "
"http://snir.cs.illinois.edu/patterns/oddeven.pdf"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:4
msgid "Reduction"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:13
msgid "Perform an associative reduction operation across a data set."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:22
msgid ""
"Many serial algorithms sweep over a set of items to collect summary "
"information."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:32
msgid ""
"The summary can be expressed as an associative operation over the data "
"set, or at least is close enough to associative that reassociation does "
"not matter."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:43
msgid ""
"Two solutions exist in |full_name|. The choice on which to use depends "
"upon several considerations:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:47
msgid "Is the operation commutative as well as associative?"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:50
msgid ""
"Are instances of the reduction type expensive to construct and destroy. "
"For example, a floating point number is inexpensive to construct. A "
"sparse floating-point matrix might be very expensive to construct."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:56
msgid ""
"Use ``oneapi::tbb::parallel_reduce`` when the objects are inexpensive to "
"construct. It works even if the reduction operation is not commutative."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:61
msgid ""
"Use ``oneapi::tbb::parallel_for`` and ``oneapi::tbb::combinable`` if the "
"reduction operation is commutative and instances of the type are "
"expensive."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:65
msgid ""
"If the operation is not precisely associative but a precisely "
"deterministic result is required, use recursive reduction and parallelize"
" it using ``oneapi::tbb::parallel_invoke``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:76
msgid ""
"The examples presented here illustrate the various solutions and some "
"tradeoffs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:80
msgid ""
"The first example uses ``oneapi::tbb::parallel_reduce`` to do a + "
"reduction over sequence of type ``T``. The sequence is defined by a half-"
"open interval [first,last)."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:104
msgid ""
"The third and fourth arguments to this form of ``parallel_reduce`` are a "
"built in form of the agglomeration pattern. If there is an elementwise "
"action to be performed before the reduction, incorporating it into the "
"third argument (reduction of a subrange) may improve performance because "
"of better locality of reference. Note that the block size for "
"agglomeration is not explicitly specified; ``parallel_reduce`` defines "
"blocks automatically with the help of implicitly used "
"``oneapi::tbb::auto_partitioner``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:114
msgid ""
"The second example assumes the + is commutative on ``T``. It is a good "
"solution when ``T`` objects are expensive to construct."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:133
msgid ""
"Sometimes it is desirable to destructively use the partial results to "
"generate the final result. For example, if the partial results are lists,"
" they can be spliced together to form the final result. In that case use "
"class ``oneapi::tbb::enumerable_thread_specific`` instead of "
"``combinable``. The ``ParallelFindCollisions`` example in "
":ref:`Divide_and_Conquer` demonstrates the technique."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:141
msgid ""
"Floating-point addition and multiplication are almost associative. "
"Reassociation can cause changes because of rounding effects. The "
"techniques shown so far reassociate terms non-deterministically. Fully "
"deterministic parallel reduction for a not quite associative operation "
"requires using deterministic reassociation. The code below demonstrates "
"this in the form of a template that does a + reduction over a sequence of"
" values of type ``T``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:171
msgid ""
"The outer if-else is an instance of the agglomeration pattern for "
"recursive computations. The reduction graph, though not a strict binary "
"tree, is fully deterministic. Thus the result will always be the same for"
" a given input sequence, assuming all threads do identical floating-point"
" rounding."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:178
msgid ""
"``oneapi::tbb::parallel_deterministic_reduce`` is a simpler and more "
"efficient way to get reproducible non-associative reduction. It is very "
"similar to ``oneapi::tbb::parallel_reduce`` but, unlike the latter, "
"builds a deterministic reduction graph. With it, the ``RepeatableReduce``"
" sample can be almost identical to ``AssociativeReduce``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:206
msgid ""
"Besides the function name change, note the grain size of 1000 specified "
"for ``oneapi::tbb::blocked_range``. It defines the desired block size for"
" agglomeration; automatic block size selection is not used due to non-"
"determinism."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:212
msgid ""
"The final example shows how a problem that typically is not viewed as a "
"reduction can be parallelized by viewing it as a reduction. The problem "
"is retrieving floating-point exception flags for a computation across a "
"data set. The serial code might look something like:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:231
msgid ""
"The code can be parallelized by computing chunks of the loop separately, "
"and merging floating-point flags from each chunk. To do this with "
"``tbb:parallel_reduce``, first define a \"body\" type, as shown below."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reduction.rst:269
msgid "Then invoke it as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:4
msgid "Reference Counting"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:13
msgid "Destroy an object when it will no longer be used."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:22
msgid ""
"Often it is desirable to destroy an object when it is known that it will "
"not be used in the future. Reference counting is a common serial solution"
" that extends to parallel programming if done carefully."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:33
msgid ""
"If there are cycles of references, basic reference counting is "
"insufficient unless the cycle is explicitly broken."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:37
msgid "Atomic counting is relatively expensive in hardware."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:46
msgid ""
"Thread-safe reference counting is like serial reference counting, except "
"that the increment/decrement is done atomically, and the decrement and "
"test \"count is zero?\" must act as a single atomic operation. The "
"following example uses ``std::atomic<int>`` to achieve this."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:75
msgid ""
"It is incorrect to use a separate read for testing if the count is zero. "
"The following code would be an incorrect implementation of method "
"``remove_ref``\\ () because two threads might both execute the decrement,"
" and then both read ``my_count`` as zero. Hence two callers would both be"
" told incorrectly that they had removed the last reference."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:90
msgid ""
"The decrement may need to have a *release* fence so that any pending "
"writes complete before the object is deleted."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:94
msgid ""
"There is no simple way to atomically copy a pointer and increment its "
"reference count, because there will be a timing hole between the copying "
"and the increment where the reference count is too low, and thus another "
"thread might decrement the count to zero and delete the object. Two ways "
"to address the problem are \"hazard pointers\" and \"pass the buck\". See"
" the references below for details."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:108
msgid ""
"Atomic increment/decrement can be more than an order of magnitude more "
"expensive than ordinary increment/decrement. The serial optimization of "
"eliminating redundant increment/decrement operations becomes more "
"important with atomic reference counts."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:114
msgid ""
"Weighted reference counting can be used to reduce costs if the pointers "
"are unshared but the referent is shared. Associate a *weight* with each "
"pointer. The reference count is the sum of the weights. A pointer ``x`` "
"can be copied as a pointer ``x'`` without updating the reference count by"
" splitting the original weight between ``x`` and ``x'``. If the weight of"
" ``x`` is too low to split, then first add a constant W to the reference "
"count and weight of ``x``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:129
msgid ""
"D. Bacon and V.T. Rajan, \"Concurrent Cycle Collection in Reference "
"Counted Systems\" in Proc. European Conf. on Object-Oriented Programming "
"(June 2001). Describes a garbage collector based on reference counting "
"that does collect cycles."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:135
msgid ""
"M. Michael, \"Hazard Pointers: Safe Memory Reclamation for Lock-Free "
"Objects\" in IEEE Transactions on Parallel and Distributed Systems (June "
"2004). Describes the \"hazard pointer\" technique."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Reference_Counting.rst:140
msgid ""
"M. Herlihy, V. Luchangco, and M. Moir, \"The Repeat Offender Problem: A "
"Mechanism for Supporting Dynamic-Sized, Lock-Free Data Structures\" in "
"Proceedings of the 16th International Symposium on Distributed Computing "
"(Oct. 2002). Describes the \"pass the buck\" technique."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:4
msgid "Wavefront"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:13
msgid ""
"Perform computations on items in a data set, where the computation on an "
"item uses results from computations on predecessor items."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:23
msgid "The dependences between computations form an acyclic graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:32
msgid "Dependence constraints between items form an acyclic graph."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:35
msgid ""
"The number of immediate predecessors in the graph is known in advance, or"
" can be determined some time before the last predecessor completes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:46
msgid ""
"The solution is a parallel variant of topological sorting, using "
"``oneapi::tbb::parallel_for_each`` to process items. Associate an atomic "
"counter with each item. Initialize each counter to the number of "
"predecessors. Invoke ``oneapi::tbb::parallel_for_each`` to process the "
"items that have no predessors (have counts of zero). After an item is "
"processed, decrement the counters of its successors. If a successor's "
"counter reaches zero, add that successor to the "
"``oneapi::tbb::parallel_for_each`` via a \"feeder\"."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:56
msgid ""
"If the number of predecessors for an item cannot be determined in "
"advance, treat the information \"know number of predecessors\" as an "
"additional predecessor. When the number of predecessors becomes known, "
"treat this conceptual predecessor as completed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:62
msgid ""
"If the overhead of counting individual items is excessive, aggregate "
"items into blocks, and do the wavefront over the blocks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:72
msgid ""
"Below is a serial kernel for the longest common subsequence algorithm. "
"The parameters are strings ``x`` and ``y`` with respective lengths "
"``xlen`` and ``ylen``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:92
msgid ""
"The kernel sets ``F[i][j]`` to the length of the longest common "
"subsequence shared by ``x[0..i-1]`` and ``y[0..j-1]``. It assumes that "
"F[0][0..ylen] and ``F[0..xlen][0]`` have already been initialized to "
"zero."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:98
msgid ""
"The following figure shows the data dependences for calculating "
"``F[i][j]``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:106
msgid "Data dependences for longest common substring calculation. |image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:110
msgid ""
"The following figure shows the gray diagonal dependence is the transitive"
" closure of other dependencies. Thus for parallelization purposes it is a"
" redundant dependence that can be ignored."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:119
msgid "Diagonal dependence is redundant. |image1|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:123
msgid ""
"It is generally good to remove redundant dependences from consideration, "
"because the atomic counting incurs a cost for each dependence considered."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:128
msgid ""
"Another consideration is grain size. Scheduling each ``F[i][j]`` element "
"calculation separately is prohibitively expensive. A good solution is to "
"aggregate the elements into contiguous blocks, and process the contents "
"of a block serially. The blocks have the same dependence pattern, but at "
"a block scale. Hence scheduling overheads can be amortized over blocks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:136
msgid ""
"The parallel code follows. Each block consists of ``NÃN`` elements. Each "
"block has an associated atomic counter. Array ``Count`` organizes these "
"counters for easy lookup. The code initializes the counters and then "
"rolls a wavefront using ``parallel_for_each``, starting with the block at"
" the origin since it has no predecessors."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/design_patterns/Wavefront.rst:189
msgid ""
"Eun-Gyu Kim and Mark Snir, \"Wavefront Pattern\", "
"http://snir.cs.illinois.edu/patterns/wavefront.pdf"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/destroy_graphs_outside_main_thread.rst:4
msgid "Destroying Graphs That Run Outside the Main Thread"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/destroy_graphs_outside_main_thread.rst:6
msgid ""
"Make sure to enqueue a task to wait for and destroy graphs that run "
"outside the main thread."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/destroy_graphs_outside_main_thread.rst:8
msgid ""
"You may not always want to block the main application thread by calling "
"wait_for_all(). However, it is safest to call wait_for_all on a graph "
"before destroying it. A common solution is to enqueue a task to build and"
" wait for the graph to complete. For example, assume you really do not "
"want to call a wait_for_all in the example from "
":ref:`always_use_wait_for_all`, Instead you can enqueue a task that "
"creates the graph and waits for it:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/destroy_graphs_outside_main_thread.rst:39
msgid ""
"In the code snippet above, the enqueued task executes at some point, but "
"it's not clear when. If you need to use the results of the enqueued task,"
" or even ensure that it completes before the program ends, you will need "
"to use some mechanism to signal from the enqueued task that the graph is "
"complete."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst:4
msgid "Estimating Flow Graph Performance"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst:7
msgid ""
"The performance or scalability of a flow graph is not easy to predict. "
"However there are a few key points that can guide you in estimating the "
"limits on performance and speedup of some graphs."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst
msgid "The Critical Path Limits the Scalability in a Dependence Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst:19
msgid ""
"A critical path is the most time consuming path from a node with no "
"predecessors to a node with no successors. In a dependence graph, the "
"execution of the nodes along a path cannot be overlapped since they have "
"a strict ordering. Therefore, for a dependence graph, the critical path "
"limits scalability."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst:26
msgid ""
"More formally, let T be the total time consumed by all of the nodes in "
"your graph if executed sequentially. Then let C be the time consumed "
"along the path that takes the most time. The nodes along this path cannot"
" be overlapped even in a parallel execution. Therefore, even if all other"
" paths are executed in parallel with C, the wall clock time for the "
"parallel execution is at least C, and the maximum possible speedup "
"(ignoring microarchitectural and memory effects) is T/C."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst
msgid "There is Overhead in Spawning a Node's Body as a Task"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/estimate_flow_graph_performance.rst:42
msgid ""
"The bodies of ``input_nodes``, ``function_nodes``, ``continue_nodes`` and"
" ``multifunction_nodes`` execute within spawned tasks by default. This "
"means that you need to take into account the overhead of task scheduling "
"when estimating the time it takes for a node to execute its body. All of "
"the rules of thumb for determining the appropriate granularity of tasks "
"therefore also apply to node bodies as well. If you have many fine-"
"grained nodes in your flow graph, the impact of these overheads can "
"noticeably impact your performance. However, depending on the graph "
"structure, you can reduce such overheads by using lightweight policy with"
" these nodes."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:4
msgid "parallel_for"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:7
msgid ""
"Suppose you want to apply a function ``Foo`` to each element of an array,"
" and it is safe to process each element concurrently. Here is the "
"sequential code to do this:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:21
msgid ""
"The iteration space here is of type ``size_t``, and goes from ``0`` to "
"``n-1``. The template function ``oneapi::tbb::parallel_for`` breaks this "
"iteration space into chunks, and runs each chunk on a separate thread. "
"The first step in parallelizing this loop is to convert the loop body "
"into a form that operates on a chunk. The form is an STL-style function "
"object, called the *body* object, in which ``operator()`` processes a "
"chunk. The following code declares the body object."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:49
msgid ""
"The ``using`` directive in the example enables you to use the library "
"identifiers without having to write out the namespace prefix "
"``oneapi::tbb`` before each identifier. The rest of the examples assume "
"that such a ``using`` directive is present."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:55
msgid ""
"Note the argument to ``operator()``. A ``blocked_range<T>`` is a template"
" class provided by the library. It describes a one-dimensional iteration "
"space over type ``T``. Class ``parallel_for`` works with other kinds of "
"iteration spaces too. The library provides ``blocked_range2d`` for two-"
"dimensional spaces. You can define your own spaces as explained in "
":ref:`Advanced_Topic_Other_Kinds_of_Iteration_Spaces`."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:63
msgid ""
"An instance of ``ApplyFoo`` needs member fields that remember all the "
"local variables that were defined outside the original loop but used "
"inside it. Usually, the constructor for the body object will initialize "
"these fields, though ``parallel_for`` does not care how the body object "
"is created. Template function ``parallel_for`` requires that the body "
"object have a copy constructor, which is invoked to create a separate "
"copy (or copies) for each worker thread. It also invokes the destructor "
"to destroy these copies. In most cases, the implicitly generated copy "
"constructor and destructor work correctly. If they do not, it is almost "
"always the case (as usual in C++) that you must define *both* to be "
"consistent."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:76
msgid ""
"Because the body object might be copied, its ``operator()`` should not "
"modify the body. Otherwise the modification might or might not become "
"visible to the thread that invoked ``parallel_for``, depending upon "
"whether ``operator()`` is acting on the original or a copy. As a reminder"
" of this nuance, ``parallel_for`` requires that the body object's "
"``operator()`` be declared ``const``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:84
msgid ""
"The example ``operator()`` loads ``my_a`` into a local variable ``a``. "
"Though not necessary, there are two reasons for doing this in the "
"example:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:89
msgid "**Style**. It makes the loop body look more like the original."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:92
msgid ""
"**Performance**. Sometimes putting frequently accessed values into local "
"variables helps the compiler optimize the loop better, because local "
"variables are often easier for the compiler to track."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:97
msgid ""
"Once you have the loop body written as a body object, invoke the template"
" function ``parallel_for``, as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_for_os.rst:112
msgid ""
"The ``blocked_range`` constructed here represents the entire iteration "
"space from 0 to n-1, which ``parallel_for`` divides into subspaces for "
"each processor. The general form of the constructor is "
"``blocked_range<T>(begin,end,grainsize)``. The ``T`` specifies the value "
"type. The arguments ``begin`` and ``end`` specify the iteration space "
"STL-style as a half-open interval [``begin``,\\ ``end``). The argument "
"*grainsize* is explained in the :ref:`Controlling_Chunking` section. The "
"example uses the default grainsize of 1 because by default "
"``parallel_for`` applies a heuristic that works well with the default "
"grainsize."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:4
msgid "parallel_reduce"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:7
msgid "A loop can do a reduction, as in this summation:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:21
msgid ""
"If the iterations are independent, you can parallelize this loop using "
"the template class ``parallel_reduce`` as follows:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:35
msgid ""
"The class ``SumFoo`` specifies details of the reduction, such as how to "
"accumulate subsums and combine them. Here is the definition of class "
"``SumFoo``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:69
msgid ""
"Note the differences with class ``ApplyFoo`` from parallel_for. First, "
"``operator()`` is *not* ``const``. This is because it must update "
"SumFoo::my_sum. Second, ``SumFoo`` has a *splitting constructor* and a "
"method ``join`` that must be present for ``parallel_reduce`` to work. The"
" splitting constructor takes as arguments a reference to the original "
"object, and a dummy argument of type ``split``, which is defined by the "
"library. The dummy argument distinguishes the splitting constructor from "
"a copy constructor."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:80
msgid ""
"In the example, the definition of ``operator()`` uses local temporary "
"variables (``a``, ``sum``, ``end``) for scalar values accessed inside the"
" loop. This technique can improve performance by making it obvious to the"
" compiler that the values can be held in registers instead of memory. If "
"the values are too large to fit in registers, or have their address taken"
" in a way the compiler cannot track, the technique might not help. With a"
" typical optimizing compiler, using local temporaries for only written "
"variables (such as ``sum`` in the example) can suffice, because then the "
"compiler can deduce that the loop does not write to any of the other "
"locations, and hoist the other reads to outside the loop."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:93
msgid ""
"When a worker thread is available, as decided by the task scheduler, "
"``parallel_reduce`` invokes the splitting constructor to create a subtask"
" for the worker. When the subtask completes, ``parallel_reduce`` uses "
"method ``join`` to accumulate the result of the subtask. The graph at the"
" top of the following figure shows the split-join sequence that happens "
"when a worker is available:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:105
msgid "Graph of the Split-join Sequence |image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:109
msgid ""
"An arrows in the above figure indicate order in time. The splitting "
"constructor might run concurrently while object ``x`` is being used for "
"the first half of the reduction. Therefore, all actions of the splitting "
"constructor that creates y must be made thread safe with respect to "
"``x``. So if the splitting constructor needs to increment a reference "
"count shared with other objects, it should use an atomic increment."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:117
msgid ""
"If a worker is not available, the second half of the iteration is reduced"
" using the same body object that reduced the first half. That is the "
"reduction of the second half starts where reduction of the first half "
"finished."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:124
msgid ""
"Since split/join are not used if workers are unavailable, "
"``parallel_reduce`` does not necessarily do recursive splitting."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:129
msgid ""
"Since the same body might be used to accumulate multiple subranges, it is"
" critical that ``operator()`` not discard earlier accumulations. The code"
" below shows an incorrect definition of ``SumFoo::operator()``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:154
msgid ""
"With the mistake, the body returns a partial sum for the last subrange "
"instead of all subranges to which ``parallel_reduce`` applies it."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:158
msgid ""
"The rules for partitioners and grain sizes for ``parallel_reduce`` are "
"the same as for ``parallel_for``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:162
msgid ""
"``parallel_reduce`` generalizes to any associative operation. In general,"
" the splitting constructor does two things:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:166
msgid "Copy read-only information necessary to run the loop body."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:169
msgid ""
"Initialize the reduction variable(s) to the identity element of the "
"operation(s)."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:173
msgid ""
"The join method should do the corresponding merge(s). You can do more "
"than one reduction at the same time: you can gather the min and max with "
"a single ``parallel_reduce``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/parallel_reduce.rst:179
msgid ""
"The reduction operation can be non-commutative. The example still works "
"if floating-point addition is replaced by string concatenation."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/title.rst:4
msgid "|short_name| Developer Guide"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/title.rst:6
msgid "|full_name|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_concurrency_limits.rst:4
msgid "Use Concurrency Limits"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_concurrency_limits.rst:7
msgid ""
"To control the number of instances of a single node, you can use the "
"concurrency limit on the node. To cause it to reject messages after it "
"reaches its concurrency limit, you construct it as a \"rejecting\" node."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_concurrency_limits.rst:12
msgid ""
"A function node is constructed with one or more template arguments. The "
"third argument controls the buffer policy used by the node, and is by "
"default queueing. With a queueing policy, a ``function_node`` that has "
"reached its concurrency limit still accepts incoming messages, but "
"buffers them internally. If the policy is set to rejecting the node will "
"instead reject the incoming messages."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_concurrency_limits.rst:29
msgid ""
"For example, you can control the number of big objects in flight in a "
"graph by placing a rejecting function_node downstream of an "
"``input_node``, as is done below:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_concurrency_limits.rst:69
msgid ""
"The ``function_node`` will operate on at most three big objects "
"concurrently. The node's concurrency threshold that limits the node to "
"three concurrent invocations. When the ``function_node`` is running three"
" instances concurrently, it will start rejecting incoming messages from "
"the ``input_node``, causing the ``input_node`` to buffer its last created"
" object and temporarily stop invoking its body object. Whenever the "
"``function_node`` drops below its concurrency limit, it will pull new "
"messages from the ``input_node``. At most four big objects will exist "
"simultaneously, three in the ``function_node`` and one buffered in the "
"``input_node``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_graph_reset.rst:4
msgid "Use graph::reset() to Reset a Canceled Graph"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_graph_reset.rst:7
msgid ""
"When a graph execution is canceled either because of an unhandled "
"exception or because its task_group_context is canceled explicitly, the "
"graph and its nodes may be left in an indeterminate state. For example, "
"in the code samples shown in :ref:`cancel_a_graph` the input 2 may be "
"left in a buffer. But even beyond remnants in the buffers, there are "
"other optimizations performed during the execution of a flow graph that "
"can leave its nodes and edges in an indeterminate state. If you want to "
"re-execute or restart a graph, you first need to reset the graph:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:4
msgid "Using input_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:7
msgid "By default, an ``input_node`` is constructed in the inactive state:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:16
msgid ""
"To activate an inactive ``input_node``, you call the node's function "
"activate:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:28
msgid ""
"All ``input_node`` objects are constructed in the inactive state and "
"usually activated after the entire flow graph is constructed."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:32
msgid ""
"For example, you can use the code in :ref:`Data_Flow_Graph`. In that "
"implementation, the ``input_node`` is constructed in the inactive state "
"and activated after all other edges are made:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:49
msgid ""
"In this example, if the ``input_node`` was toggled to the active state at"
" the beginning, it might send a message to squarer immediately after the "
"edge to squarer is connected. Later, when the edge to cuber is connected,"
" cuber will receive all future messages, but may have already missed "
"some."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:55
msgid ""
"In general it is safest to create your ``input_node`` objects in the "
"inactive state and then activate them after the whole graph is "
"constructed. However, this approach serializes graph construction and "
"graph execution."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:61
msgid ""
"Some graphs can be constructed safely with ``input_node``s active, "
"allowing the overlap of construction and execution. If your graph is a "
"directed acyclic graph (DAG), and each ``input_node`` has only one "
"successor, you can activate your ``input_node``s just after their "
"construction if you construct the edges in reverse topological order; "
"that is, make the edges at the largest depth in the tree first, and work "
"back to the shallowest edges. For example, if src is an ``input_node`` "
"and ``func1`` and ``func2`` are both function nodes, the following graph "
"would not drop messages, even though src is activated just after its "
"construction:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_input_node.rst:105
msgid ""
"The above code is safe because the edge from ``func1`` to ``func2`` is "
"made before the edge from src to ``func1``. If the edge from src to func1"
" were made first, ``func1`` might generate a message before ``func2`` is "
"attached to it; that message would be dropped. Also, src has only a "
"single successor. If src had more than one successor, the successor that "
"is attached first might receive messages that do not reach the successors"
" that are attached after it."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_limiter_node.rst:4
msgid "Using limiter_node"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_limiter_node.rst:7
msgid ""
"One way to limit resource consumption is to use a limiter_node to set a "
"limit on the number of messages that can flow through a given point in "
"your graph. The constructor for a limiter node takes two arguments:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_limiter_node.rst:18
msgid ""
"The first argument is a reference to the graph it belongs to. The second "
"argument sets the maximum number of items that should be allowed to pass "
"through before the node starts rejecting incoming messages."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_limiter_node.rst:23
msgid ""
"A limiter_node maintains an internal count of the messages that it has "
"allowed to pass. When a message leaves the controlled part of the graph, "
"a message can be sent to the decrement port on the ``limiter_node`` to "
"decrement the count, allowing additional messages to pass through. In the"
" example below, an ``input_node`` will generate ``M`` big objects. But "
"the user wants to allow at most three big objects to reach the "
"``function_node`` at a time, and to prevent the ``input_node`` from "
"generating all ``M`` big objects at once."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_limiter_node.rst:75
msgid ""
"The example above prevents the ``input_node`` from generating all ``M`` "
"big objects at once. The ``limiter_node`` has a threshold of 3, and will "
"therefore start rejecting incoming messages after its internal count "
"reaches 3. When the ``input_node`` sees its message rejected, it stops "
"calling its body object and temporarily buffers the last generated value."
" The ``function_node`` has its output, a ``continue_msg``, sent to the "
"decrement port of the ``limiter_node``. So, after it completes executing,"
" the ``limiter_node`` internal count is decremented. When the internal "
"count drops below the threshold, messages begin flowing from the "
"``input_node`` again. So in this example, at most four big objects exist "
"at a time, the three that have passed through the ``limiter_node`` and "
"the one that is buffered in the ``input_node``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_make_edge.rst:4
msgid "Use make_edge and remove_edge"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_make_edge.rst:7
msgid "These are the basic guidelines for creating and removing edges:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_make_edge.rst:10
msgid "use make_edge and remove_edge"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_make_edge.rst:13
msgid "Avoid using register_successor and register_predecessor"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_make_edge.rst:16
msgid "Avoid using remove_successor and remove_predecessor"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_make_edge.rst:19
msgid ""
"As a convention, to communicate the topology, use only functions "
"flow::make_edge and flow::remove_edge. The runtime library uses node "
"functions, such as sender<T>::register_successor, to create these edges, "
"but those functions should not be called directly. The runtime library "
"calls these node functions directly to implement optimizations on the "
"topology at runtime."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_algorithms.rst:4
msgid "Use Nested Algorithms to Increase Scalability"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_algorithms.rst:7
msgid ""
"One powerful way to increase the scalability of a flow graph is to nest "
"other parallel algorithms inside of node bodies. Doing so, you can use a "
"flow graph as a coordination language, expressing the most coarse-grained"
" parallelism at the level of the graph, with finer grained parallelism "
"nested within."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_algorithms.rst:14
msgid ""
"In the example below, five nodes are created: an ``input_node``, "
"``matrix_source``, that reads a sequence of matrices from a file, two "
"``function_nodes``, ``n1`` and ``n2``, that receive these matrices and "
"generate two new matrices by applying a function to each element, and two"
" final ``function_nodes``, ``n1_sink`` and ``n2_sink``, that process "
"these resulting matrices. The ``matrix_source`` is connected to both "
"``n1`` and ``n2``. The node ``n1`` is connected to ``n1_sink``, and "
"``n2`` is connected to ``n2_sink``. In the lambda expressions for ``n1`` "
"and ``n2``, a ``parallel_for`` is used to apply the functions to the "
"elements of the matrix in parallel. The functions ``read_next_matrix``, "
"``f1``, ``f2``, ``consume_f1`` and ``consume_f2`` are not provided below."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_flow_graphs.rst:4
msgid "Use Nested Flow Graphs"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_flow_graphs.rst:7
msgid ""
"In addition to nesting algorithms within a flow graph node, it is also "
"possible to nest flow graphs. For example, below there is a graph ``g`` "
"with two nodes, ``a`` and ``b``. When node ``a`` receives a message, it "
"constructs and executes an inner dependence graph. When node ``b`` "
"receives a message, it constructs and executes an inner data flow graph:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_flow_graphs.rst:65
msgid ""
"If the nested graph remains unchanged in structure between invocations of"
" the node, it is redundant to construct it each time. Reconstructing the "
"graph only adds overhead to the execution. You can modify the example "
"above, for example, to have node ``b`` reuse a graph that is persistent "
"across its invocations:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/use_nested_flow_graphs.rst:125
msgid ""
"It is only necessary to call ``h.wait_for_all()`` at the end of each "
"invocation of ``b``'s body in our modified code, if you wish for this "
"``b``'s body to block until the inner graph is done. In the first "
"implementation of ``b``, it was necessary to call ``h.wait_for_all`` at "
"the end of each invocation since the graph was destroyed at the end of "
"the scope. So it would be valid in the body of ``b`` above to call "
"``m1.try_put(i)`` and then return without waiting for ``h`` to become "
"idle."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:4
msgid "Work Isolation"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:10
msgid ""
"In |full_name|, a thread waiting for a group of tasks to complete might "
"execute other available tasks. In particular, when a parallel construct "
"calls another parallel construct, a thread can obtain a task from the "
"outer-level construct while waiting for completion of the inner-level "
"one."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:17
msgid ""
"In the following example with two ``parallel_for`` calls, the call to the"
" second (nested) parallel loop blocks execution of the first (outer) loop"
" iteration:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:32
msgid ""
"The blocked thread is allowed to take tasks belonging to the first "
"parallel loop. As a result, two or more iterations of the outer loop "
"might be simultaneously assigned to the same thread. In other words, in "
"oneTBB execution of functions constituting a parallel construct is "
"*unsequenced* even within a single thread. In most cases, this behavior "
"is harmless or even beneficial because it does not restrict parallelism "
"available for the thread."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:41
msgid ""
"However, in some cases such unsequenced execution may result in errors. "
"For example, a thread-local variable might unexpectedly change its value "
"after a nested parallel construct:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:60
msgid ""
"In other scenarios, the described behavior might lead to deadlocks and "
"other issues. In these cases, a stronger guarantee of execution being "
"sequenced within a thread is desired. For that, oneTBB provides ways to "
"*isolate* execution of a parallel construct, for its tasks to not "
"interfere with other simultaneously running tasks."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:67
msgid ""
"One of these ways is to execute the inner level loop in a separate "
"``task_arena``:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:88
msgid ""
"However, using a separate arena for work isolation is not always "
"convenient, and might have noticeable overheads. To address these "
"shortcomings, oneTBB provides ``this_task_arena::isolate`` function which"
" runs a user-provided functor in isolation by restricting the calling "
"thread to process only tasks scheduled in the scope of the functor (also "
"called the isolation region)."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:96
msgid ""
"When entered a task waiting call or a blocking parallel construct inside "
"an isolated region, a thread can only execute tasks spawned within the "
"region and their child tasks spawned by other threads. The thread is "
"prohibited from executing any outer level tasks or tasks belonging to "
"other isolated regions."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:103
msgid ""
"The isolation region imposes restrictions only upon the thread that "
"called it. Other threads running in the same task arena have no "
"restrictions on task selection unless isolated by a distinct call to "
"``this_task_arena::isolate``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/work_isolation.rst:109
msgid ""
"The following example demonstrates the use of "
"``this_task_arena::isolate`` to ensure that a thread-local variable is "
"not changed unexpectedly during the call to a nested parallel construct."
msgstr ""

