# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2021, Intel Corporation
# This file is distributed under the same license as the oneTBB package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: oneTBB\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-23 14:08+0300\n"
"PO-Revision-Date: 2022-02-23 15:55+0300\n"
"Last-Translator: Evgeny <schmidte@list.ru>\n"
"Language-Team: \n"
"Language: os\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Generator: Poedit 3.0.1\n"

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:4
msgid "Bandwidth and Cache Affinity"
msgstr "Пропускная способность и аффинити кэша"

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:7
msgid "For a sufficiently simple function ``Foo``, the examples might not show good speedup when written as parallel loops. The cause could be insufficient system bandwidth between the processors and memory. In that case, you may have to rethink your algorithm to take better advantage of cache. Restructuring to better utilize the cache usually benefits the parallel program as well as the serial program."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:15
msgid "An alternative to restructuring that works in some cases is ``affinity_partitioner.`` It not only automatically chooses the grainsize, but also optimizes for cache affinity and tries to distribute the data uniformly among threads. Using ``affinity_partitioner`` can significantly improve performance when:"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:22
msgid "The computation does a few operations per data access."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:25
msgid "The data acted upon by the loop fits in cache."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:28
msgid "The loop, or a similar loop, is re-executed over the same data."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:31
msgid "There are more than two hardware threads available (and especially if the number of threads is not a power of two). If only two threads are available, the default scheduling in |full_name| usually provides sufficient cache affinity."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:37
msgid "The following code shows how to use ``affinity_partitioner``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:43
msgid ""
"#include \"oneapi/tbb.h\"\n"
"\n"
"\n"
"void ParallelApplyFoo( float a[], size_t n ) {\n"
"    static affinity_partitioner ap;\n"
"    parallel_for(blocked_range<size_t>(0,n), ApplyFoo(a), ap);\n"
"}\n"
"\n"
"\n"
"void TimeStepFoo( float a[], size_t n, int steps ) {\n"
"    for( int t=0; t<steps; ++t )\n"
"        ParallelApplyFoo( a, n );\n"
"}"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:58
msgid "In the example, the ``affinity_partitioner`` object ``ap`` lives between loop iterations. It remembers where iterations of the loop ran, so that each iteration can be handed to the same thread that executed it before. The example code gets the lifetime of the partitioner right by declaring the ``affinity_partitioner`` as a local static object. Another approach would be to declare it at a scope outside the iterative loop in ``TimeStepFoo``, and hand it down the call chain to ``parallel_for``."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:67
msgid "If the data does not fit across the system’s caches, there may be little benefit. The following figure shows the situations."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:75
msgid "Benefit of Affinity Determined by Relative Size of Data Set and Cache |image0|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:101
msgid "image0"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:101
msgid ".. image:: main/tbb_userguide/Images/image007.jpg"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:79
msgid "The next figure shows how parallel speedup might vary with the size of a data set. The computation for the example is ``A[i]+=B[i]`` for ``i`` in the range [0,N). It was chosen for dramatic effect. You are unlikely to see quite this much variation in your code. The graph shows not much improvement at the extremes. For small N, parallel scheduling overhead dominates, resulting in little speedup. For large N, the data set is too large to be carried in cache between loop invocations. The peak in the middle is the sweet spot for affinity. Hence ``affinity_partitioner`` should be considered a tool, not a cure-all, when there is a low ratio of computations to memory accesses."
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:95
msgid "Improvement from Affinity Dependent on Array Size |image1|"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:104
msgid "image1"
msgstr ""

#: ../../oneTBB/doc/main/tbb_userguide/Bandwidth_and_Cache_Affinity_os.rst:104
msgid ".. image:: main/tbb_userguide/Images/image008.jpg"
msgstr ""
